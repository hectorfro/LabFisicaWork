\documentclass[aspectratio=1610]{beamer}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage[spanish,es-tabla]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{xcolor}
%\usepackage{enumitem}
\usetheme{Madrid} 
\useinnertheme{circles}
\usecolortheme{default} 

\setbeamercolor{structure}{fg=green!60!black} % Color principal a un tono de verde
\setbeamercolor{title}{fg=green!60!black} % Establece el color del título al verde
\setbeamercolor{subtitle}{fg=green!50!black} % Establece el color del subtítulo al verde

\usefonttheme[onlymath]{serif} % Cambiar a fuente serif para matemáticas
%\usefonttheme{serif}

\title{\bf Regresión Lineal}
\subtitle{Consideraciones para el análisis de datos en los cursos de laboratorio de Física}
\author{Héctor F. Hernández G.}
\date{\today}

% Definir el color verde personalizado
\definecolor{customgreen}{RGB}{0,128,0}

% Configurar los colores de Beamer para el tema Warsaw
\setbeamercolor{title in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{author in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{date in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{section in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{subsection in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{frametitle}{bg=customgreen, fg=white}
\setbeamercolor{block title}{bg=customgreen, fg=white}
\setbeamercolor{block body}{bg=customgreen!10, fg=black}
\setbeamercolor{item}{fg=customgreen}


\begin{document}

\begin{frame}
    \begin{minipage}[t]{0.0\linewidth}
        \vspace{0.0cm} % Ajustar el espacio vertical si es necesario
        \includegraphics[width=2.5cm]{figuras/logouis} 
    \end{minipage}
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \vspace{0.2cm}  \hspace{0.0cm} % Espacio superior
        {\huge \color{green!60!black} \inserttitle\par} % Título
        \vspace{0.4cm} \hspace{0.0cm}  % Espacio entre el título y el subtítulo
        {\large \color{green!50!black} \insertsubtitle\par} % Subtítulo en verde
        \vspace{0.6cm} \hspace{0.0cm} % Espacio entre el título y la imagen
        \includegraphics[width=0.6\textwidth]{figuras/fig01} % Imagen
    \end{minipage}
\end{frame}
% \frame{\titlepage}

\begin{frame}
    \frametitle{Regresión Lineal}
    \begin{enumerate}
        \item El problema de la regresión lineal
        \item La regresión lineal simple
        \item Método de mínimos cuadrados
        \item Coeficiente de regresión
	\item Coeficiente de correlación lineal
	\item El contraste de regresión
	\item Inferencias acerca de los parámetros
	 \item Inferencias acerca de la predicción
	 \item Los supuestos del modelo de regresión lineal
	\item Un ejemplo en donde no se cumplen los supuestos
    \end{enumerate}
\end{frame}


\begin{frame}
    \frametitle{1. El problema de la regresión lineal}
    \begin{itemize}
     \item La regresión es una técnica estadística para investigar y modelar relaciones entre variables.
\item Las relaciones estadísticas difieren de las funcionales porque no son perfectas; las observaciones no caen directamente sobre una curva.
\item Se supone una relación entre una respuesta cuantitativa $y$ y $k$ predictores $x_1, x_2, \ldots, x_k$ de la forma general:
$$
y=f(x)+\varepsilon 
$$
donde $f$ es una función desconocida de $x_1, \ldots, x_k$ y  $\varepsilon$ es un término de error aleatorio independiente de $x$ con media cero.
\item  La función  $f$ representa la información sistemática que $x$ proporciona sobre $y$.
\item El método paramétrico más utilizado asume que $f$ es lineal en $x$:
$$
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k 
$$
\item Para ajustar el modelo lineal, se estiman los parámetros $\beta_0, \beta_1, \ldots, \beta_k$ de manera que:
$$
y \approx \beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k 
$$
    \end{itemize}
\end{frame}


\begin{frame}{2. La regresión lineal simple}
\begin{itemize}
\item Modelo:
$$
y_i=\beta_0+\beta_1 x_i+\varepsilon_i
$$
donde $y_i$ es la respuesta, $x_i$ es el regresor, $\beta_0$ es la intersección, $\beta_1$ es la pendiente, y $\varepsilon_i$ es el término de error aleatorio.
 \end{itemize}
 
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
 
\begin{itemize}
\item Características del Modelo:

1. $y_i$ es una variable aleatoria compuesta por $\beta_0+\beta_1 x_i$

2. La función de regresión:
$$
E\left(y_i \mid x_i\right)=\beta_0+\beta_1 x_i
$$
relaciona las medias de las distribuciones de $y_i$ para cada $x_i$.

 \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
\vspace{0.5cm} 
3. $\varepsilon_i$ introduce variabilidad adicional a $y_i$ con varianza constante $\sigma^2$.

4. El modelo asume varianza constante para $y_i$:
$$
\sigma^2\left\{y_i\right\}=\sigma^2
$$
5. Los términos de error $\varepsilon_i$ no están correlacionados entre sí, lo que implica que las respuestas $y_i$ tampoco lo están.

        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{}
Con técnicas de regresión de una variable $y$ (respuesta) sobre una variable $x$ (regresor), se busca una función que sea una buena aproximación de una nube de puntos,  mediante una curva del tipo: 
$$
\hat{y}= f(x)
$$
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figuras/fig03}
    \end{figure}

\end{frame}

\begin{frame}{}
Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
$$
y_i=\beta_0+\beta_1 x_i+\varepsilon_i
$$

En donde $\beta_0$ es la ordenada en el origen (el valor que toma $y$ cuando $x$ vale 0 ), $\beta_1$ es la pendiente de la recta (e indica cómo cambia $y$ al incrementar $x$ en una unidad) y $\varepsilon$ una variable que incluye un conjunto grande de factores, cada uno de los cuales influye en la respuesta sólo en pequeña magnitud, a la que llamaremos error. 
    \begin{figure}
        \centering
        \includegraphics[width=0.3\textwidth]{figuras/fig04}
    \end{figure}

Por lo tanto, $x$ e $y$ son variables aleatorias, por lo que no se puede establecer una relación lineal exacta entre ellas.
\end{frame}

\begin{frame}{ }
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
\begin{itemize}
\item Ejemplo:  Ley de enfriamiento de Steinhart-Hart. 
 \vspace{0.5cm}
 
Para describir con precisión la relación entre la resistencia $R$  de un termistor y su temperatura $T$ se utiliza la ecuación de Steinhart-Hart:
$$
\frac{1}{T}=A+B \ln (R)+C[\ln (R)]^3
$$
donde:

- $T$ es la temperatura en grados Kelvin.

- $R$ es la resistencia en ohmios.

- $A, B, C$ son constantes específicas del termistor.

En un rango pequeño de temperaturas, la relación entre $R$ y $T$ puede aproximarse a:
$$
R \approx R_0+\alpha\left(T-T_0\right)
$$
 \end{itemize} 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
es decir, a una linea recta donde: $R_0$ es la resistencia a una temperatura de referencia $T_0$ y $\alpha$ es el coeficiente de temperatura, que indica la tasa de cambio de la resistencia con la temperatura.
\vspace{0.5cm}

{\bf Experimento:}  Medición de la Resistencia de un Termistor NTC

Descripción del Experimento:
\begin{itemize}
\item  Objetivo: Determinar la relación lineal aproximada entre la resistencia y la temperatura de un termistor NTC en un rango limitado de temperaturas.
\item Variables: $y$, resistencia del material (en ohmios) y $x$ la temperatura  (en grados Kelvin)
\end{itemize}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{ }
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
Procedimiento:  
\begin{itemize}
\item Calentar el Termistor: Coloca el termistor en un baño de agua caliente para llevarlo a una temperatura significativamente más alta que la ambiente.
\item  Medir la Resistencia y Temperatura: Retira el termistor del agua caliente y mide su resistencia a intervalos regulares mientras se enfría. Simultáneamente, mide la temperatura del termistor.
\item Registrar los Datos: Anota las mediciones de resistencia $R$ y temperatura $T$

 \end{itemize} 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
       \vspace{0.8cm} 
            \centering
            \includegraphics[width=\textwidth]{figuras/fig02} % Imagen de ejemplo
        \end{column}
    \end{columns}
\end{frame}






\begin{frame}{3. Método de mínimos cuadrados}

El método de los mínimos cuadrados permite  estimar $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. 
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. 

A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador.  El método de mínimos cuadrados consiste en minimizar suma de los cuadrados de los errores:
$$
\sum_{i=1}^n e_i^2=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
$$
Es decir, la suma de los cuadrados de las diferencias entre los valores reales observados $\left(y_i\right)$ y los valores estimados $\left(\hat{y}_i\right)$.

\end{frame}


\begin{frame}{}
Minimizar suma de los cuadrados de los errores se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.

Con este método, las expresiones que se obtiene para $b$ y $m$ son las siguientes:
$$
m=\frac{S_{xy}}{S_x^2} \quad b=\bar{y}-m \bar{x} \,,
$$

En donde $\bar{x}$ e $\bar{y}$ denotan las medias muestrales de $x$ y $y$ (respectivamente), 
$$
\bar{x}=\frac{\sum_{i=1}^n x_i}{n}, \, \bar{y}=\frac{\sum_{i=1}^n y_i}{n}\,,
$$

$S_x^2$ es la varianza muestral de $x$ y $S_{xy}$ es la covarianza muestral entre $x$ y $y$. Estos parámetros se calculan como:
$$
S_x^2=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}{n}, \, S_y^2=\frac{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}{n}, \, S_{xy}=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{n} .
$$

La cantidad $m$ se denomina coeficiente de regresión de $y$ sobre $x$, lo denotamos por $m_{y/x}$.

\end{frame}

\begin{frame}{}
En nuestro ejemplo, los estadísticos descriptivos anteriores para las ($n=21$) variables temperatura y tiempo del enfriamiento son los siguientes:
\vspace{0.5cm} 
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
$$
\begin{aligned}
& \bar{x}=125.0119 , \quad \bar{y}=2156.81\\
& S_x^2=12896.0685, \quad S_y^2=196582.6099 \\
& S_{xy}=-48017.6465 \\
& m= -3.7234, \quad   b=  2622.2834
\end{aligned}
$$

La recta de regresión ajustada es la siguiente:
$$
\hat{y}=2622.2834 -3.7234 x \,,
$$
donde $\hat{y}$ es la resistencia y $x$ la temperatura. 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
            \centering
            \includegraphics[width=\textwidth]{figuras/fig02b} % Imagen de ejemplo
            \vspace{0.5cm} % Espacio vertical
        \end{column}
    \end{columns}

Hay varias propiedades que se cumplen para los mínimos cuadrados:
\end{frame}


\begin{frame}{}
\begin{enumerate}
\item La suma de los valores observados $y_i$ es igual a la suma de los valores ajustados $\hat{y}_i$, o bien
$$
\sum_{i=1}^n y_i=\sum_{i=1}^n \hat{y}_i
$$ 

\item  La suma de los residuos que contiene un intercepto $\beta_0$ es siempre cero
$$
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n e_i=0
$$
\item La recta siempre pasa por el centroide, el punto $(\bar{x}, \bar{y})$ de los datos.
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n  x_i \,, \quad  \bar{y} = \frac{1}{n}\sum_{i=1}^n  y_i= m\bar{x} +b \,. 
$$

\item  La suma de los residuos ponderada por el valor de la variable regresora  es cero
$$
\sum_{i=1}^n x_i e_i=0
$$

\item La suma de los residuos ponderada por el valor ajustado  siempre es cero
$$
\sum_{i=1}^n \hat{y}_i e_i=0
$$

\end{enumerate}
\end{frame}



\begin{frame}{4. Coeficiente de regresión}
El coeficiente de regresión nos da información sobre el comportamiento de la variable $y$ frente a la variable $x$, de manera que:
\begin{enumerate}
\item Si $m_{y / x}=0$, para cualquier valor de $x$ la variable $y$ es constante.
\item Si $m_{y / x}>0$, esto nos indica que al aumentar el valor de $x$, también aumenta el valor de $y$.
\item Si $m_{y / x}<0$, esto nos indica que al aumentar el valor de $x$, el valor de $y$ disminuye.

\end{enumerate}


En el ajuste de regresión lineal para la resistencia y la temperatura resultó en 
$$
\hat{y}=2622.2834 -3.7234 x \,,
$$

El coeficiente de regresión que se obtuvo fue  $m_{y/x}=-3.7234 < 0$ y esto indica que al aumentar $x$ disminuye $y$.
\end{frame}


\begin{frame}{5. Coeficiente de correlación lineal}

El coeficiente de correlación lineal entre $x$ e $y$ viene dado por:
$$
r=\frac{S_{xy}}{S_x S_y}
$$
y es una medida estadística que cuantifica la intensidad de la relación lineal entre dos variables. Su cuadrado se denomina coeficiente de determinación $r^2$.

Propiedades del coeficiente de correlación:
\begin{enumerate}
\item  No tiene dimensión, y siempre toma valores en [-1,1].
\item Si las variables son independientes, entonces $r=0$, el inverso no tiene por qué ser cierto.
\item Si existe una relación lineal exacta entre $x$ e $y$, entonces $r=1$  (relación directa) ó $r=-1$ (relación inversa).
\item Si $r>0$,  indica una relación directa entre las variables (si aumenta $x$,  aumenta $y$ ).
\item Si $r<0$, indica una relación directa entre las variables (si aumenta $x$,  disminuye $y$).
\end{enumerate}
\end{frame}

\begin{frame}{}
Para nuestro ejemplo el valor de $r$ es
$$
r=\frac{S_{xy}}{S_x S_y} = \frac{-48017.6465}{ \sqrt{12896.0685} \sqrt{196582.6099}}= -0.9537
$$
El menos indica que existe una relación inversa entre las variables. Además su valor es próximo a $1$ indicando una dependencia lineal muy fuerte.
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
        \vspace{0.2cm} 
El coeficiente de determinación al cuadrado es $r^2=0.9095$.

La relación entre los coeficientes de regresión y de correlación:
$$
m_{y/ x}=r \frac{S_y}{S_x}=-3.7234\,,
$$
$$
m_{x / y}=r \frac{S_x}{S_y}=-0.2443 
$$
Los dos coeficientes de regresión y el coeficiente de correlación tienen  el mismo signo: a medida que $x$ aumenta  $y$ tiende a disminuir.

            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
            \centering
            \includegraphics[width=0.9\textwidth]{figuras/fig05} % Imagen de ejemplo
            \vspace{0.5cm} % Espacio vertical
        \end{column}
    \end{columns}

\end{frame}

\begin{frame}{}
\begin{itemize}
\item Descomposición de la variabilidad: 
\begin{enumerate}
\item Variabilidad Explicada (Sum of Squares for Regression, SSR): Representa la parte de la variabilidad de $y$ que se explica por el modelo de regresión.
$$
\mathrm{SSR}=\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2
$$

\item Variabilidad Residual (Sum of Squares for Error, SSE): Representa la parte de la variabilidad de $y$  que no se puede explicar por el modelo de regresión.
$$
\mathrm{SSE}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 
$$
\item Variabilidad Total (Total Sum of Squares, SST): Representa la variabilidad total de la variable dependiente $y$ respecto a su media.
$$
\mathrm{SST}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2
$$
\end{enumerate}

\item La variabilidad total se puede descomponer en la variabilidad explicada por el modelo y la variabilidad residual
$$
\mathrm{SST}= \mathrm{SSR} + \mathrm{SSE} \,\, \Rightarrow \,\,
\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 =
\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 + 
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
$$

\end{itemize}
\end{frame}

\begin{frame}{}
\begin{itemize}
\item Coeficiente de determinación $r^2$: El coeficiente de determinación  es una medida que cuantifica la proporción de la variabilidad total de $y$ que es explicada por el modelo de regresión. Se calcula como:
$$
r^2=\frac{\sum\left(\hat{y}_i-\bar{y}\right)^2}{\sum\left(y_i-\bar{y}\right)^2}=\frac{\mathrm{SSR}}{\mathrm{SST}}
$$
\end{itemize}

\begin{itemize}
\item Para el modelo de temperaturas y resistencias:
$$
\mathrm{SSR}= 1787904.8827\,, \,\, \mathrm{SSE}= 177921.2163\,,
\mathrm{SST}= 1965826.099
$$

$$
r^2=\frac{\mathrm{SSR}}{\mathrm{SST}} = \frac{1787904.8827}{1965826.099}= 0.9095 
$$


\end{itemize}
\end{frame}

\begin{frame}{6. El contraste de regresión}

Pruebas estadísticas  para evaluar la significancia de los coeficientes en un modelo de regresión. Ayudan a determinar si las relaciones observadas entre las variables independientes (predictores) y la variable dependiente (respuesta) son estadísticamente significativas. 

\begin{enumerate}
\item  Hipótesis en el Contraste de Regresión:
\begin{itemize}
\item  Hipótesis Nula $\left(H_0\right)$: Indica que el coeficiente de regresión no tiene un efecto significativo en la variable dependiente. Matemáticamente es $\beta_i=0$, donde $\beta_i$ es el coeficiente de regresión de una variable independiente específica.

\item  Hipótesis Alternativa $\left(H_a\right)$ Sugiere que el coeficiente de regresión tiene un efecto significativo en la variable dependiente. Esto se expresa como $\beta_i  \neq 0$.
\end{itemize}

\item Estadístico t (t-Estadístico)
Para cada coeficiente de regresión $\beta_i$, se calcula un t-estadístico para evaluar su significancia.
Este se define como:
$$
t_i=\frac{\hat{\beta}_i}{\mathrm{SE}\left(\hat{\beta}_i\right)}
$$
donde $\hat{\beta}_i$ es el estimador del coeficiente de regresión y $\mathrm{SE}\left(\hat{\beta}_i\right)$ es el error estándar del coeficiente de regresión.

\end{enumerate}
\end{frame}

\begin{frame}{}
\begin{enumerate}
\setcounter{enumi}{2} 
\item Valor $\mathrm{p}$ (p-Value): el valor $\mathrm{p}$ asociado con el t-estadístico se utiliza para tomar decisiones respecto a las hipótesis. Si el valor $p$ es menor que un nivel de significancia predefinido (generalmente 0.05), se rechaza la hipótesis nula, indicando que el coeficiente es significativamente diferente de cero.

\item Contraste Global del Modelo: además de evaluar cada coeficiente individualmente, se puede realizar un contraste global del modelo para verificar si al menos una de las variables independientes tiene un efecto significativo. 

Esto se hace usando el estadístico F (F-statistic)

$$
\mathrm{F}=\frac{\mathrm{MSR}}{\mathrm{MSE}}= \dfrac{\dfrac{\mathrm{SSR}}{ k-1}}{\dfrac{\mathrm{SSE}}{ n-k}}
$$
donde $k$  es el número de coeficientes estimados en el modelo de regresión y $n$  el número de observaciones.
\end{enumerate}
\end{frame}

\begin{frame}{}
Para el ejemplo en estudio podemos calcular los  t-estadístico, primero calculamos  el error estándar residual y  los errores estándar de la pendiente $\Delta m$ y el término independiente $\Delta b$ de la regresión lineal.

$$
\mathrm{SE}\left(m\right)^2=\Delta m=\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \,, \,\, 
\mathrm{SE}\left(b\right)^2=\Delta b=\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right] 
$$
donde 
$$
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2} = 9364.2745 
$$

$$
\Delta m = \sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}}= 0.2695
$$
$$
\Delta b = \sqrt{\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right]  }   = 39.7582
$$

$$
t_m=\frac{m}{\Delta m}= -13.8177\,, \,\, t_b=\frac{b}{\Delta b}= 65.9557
$$

\end{frame}

\begin{frame}{}
\begin{itemize}
\item Estimaciones de los  intervalos de confianza  para los parámetros:
\end{itemize}
Los errores estándar pueden utilizarse para calcular intervalos de confianza. El intervalo se define en términos de límites inferior y superior calculados a partir de la muestra de datos. Un intervalo de confianza del 95\% tiene la siguiente propiedad: si tomamos muestras repetidas y construimos el intervalo de confianza para cada muestra, el 95\% de los intervalos contendrán el verdadero valor desconocido del parámetro. 
   \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
El intervalo de confianza del 95 \% para $m$ sería        
$$
m \pm 2 \cdot \mathrm{SE}\left(m\right)
$$
Tenemos una probabilidad del 95 \% de que el intervalo
$$
\left[m-2 \cdot \mathrm{SE}\left(m\right), m+2 \cdot \mathrm{SE}\left(m\right)\right]
$$
contendrá el verdadero valor de $m$. Igual para  $b$
$$
m \in  [-4.2624 \ ,\ -3.1845]
$$
$$
b \in  [2542.7669 \ , \ 2701.7999]
$$
%            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
Para el cálculo de estadístico F (F-statistic)
$$
\mathrm{F}=\frac{\mathrm{MSR}}{\mathrm{MSE}}= \dfrac{\dfrac{\mathrm{SSR}}{ 2-1}}{\dfrac{\mathrm{SSE}}{ 21-2}}
$$

Resulta
$$
\mathrm{F}=\frac{1787904.8827}{9364.2745}= 190.9283
$$
        \end{column}
    \end{columns}
\end{frame}



\begin{frame}
    \frametitle{Gráficos}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figuras/fig20a}
        \caption{Descripción de la imagen.}
    \end{figure}
\end{frame}

\end{document}
