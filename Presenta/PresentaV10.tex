\documentclass[aspectratio=1610]{beamer}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,amsfonts}
\usepackage[spanish,es-tabla]{babel}
\usepackage[ansinew]{inputenc}
\usepackage{xcolor}
%\usepackage{enumitem}
\usetheme{Madrid} 
\useinnertheme{circles}
\usecolortheme{default} 

\setbeamercolor{structure}{fg=green!60!black} % Color principal a un tono de verde
\setbeamercolor{title}{fg=green!60!black} % Establece el color del título al verde
\setbeamercolor{subtitle}{fg=green!50!black} % Establece el color del subtítulo al verde

\usefonttheme[onlymath]{serif} % Cambiar a fuente serif para matemáticas
%\usefonttheme{serif}

\title{\bf Regresión Lineal}
\subtitle{Consideraciones para el análisis de datos en los cursos de laboratorio de Física}
\author{Héctor F. Hernández G.}
\date{\today}

% Definir el color verde personalizado
\definecolor{customgreen}{RGB}{0,128,0}

% Configurar los colores de Beamer para el tema Warsaw
\setbeamercolor{title in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{author in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{date in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{section in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{subsection in head/foot}{bg=customgreen, fg=white}
\setbeamercolor{frametitle}{bg=customgreen, fg=white}
\setbeamercolor{block title}{bg=customgreen, fg=white}
\setbeamercolor{block body}{bg=customgreen!10, fg=black}
\setbeamercolor{item}{fg=customgreen}


\begin{document}

\begin{frame}
    \begin{minipage}[t]{0.0\linewidth}
        \vspace{0.0cm} % Ajustar el espacio vertical si es necesario
        \includegraphics[width=2.5cm]{figuras/logouis} 
    \end{minipage}
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \vspace{0.2cm}  \hspace{0.0cm} % Espacio superior
        {\huge \color{green!60!black} \inserttitle\par} % Título
        \vspace{0.4cm} \hspace{0.0cm}  % Espacio entre el título y el subtítulo
        {\large \color{green!50!black} \insertsubtitle\par} % Subtítulo en verde
        \vspace{0.6cm} \hspace{0.0cm} % Espacio entre el título y la imagen
        \includegraphics[width=0.6\textwidth]{figuras/fig01} % Imagen
    \end{minipage}
\end{frame}
% \frame{\titlepage}

\begin{frame}
    \frametitle{Regresión Lineal}
    \begin{enumerate}
        \item El problema de la regresión
        \item La regresión lineal simple
        \item Método de mínimos cuadrados
        \item Coeficiente de regresión
	\item Coeficiente de correlación lineal
	\item Inferencias acerca de los parámetros
	 \item Condiciones y supuestos para modelo de regresión lineal
	\item Un ejemplo en donde no se cumplen los supuestos
    \end{enumerate}
\end{frame}


\begin{frame}
    \frametitle{1. El problema de la regresión}
    \begin{itemize}
     \item La regresión es una técnica estadística para investigar y modelar relaciones entre variables.
\item Las relaciones estadísticas difieren de las funcionales porque no son perfectas; las observaciones no caen directamente sobre una curva.
\item Se supone una relación entre una respuesta cuantitativa $y$ y $k$ predictores $x_1, x_2, \ldots, x_k$ de la forma general:
$$
y=f(x)+\varepsilon 
$$
donde $f$ es una función desconocida de $x_1, \ldots, x_k$ y  $\varepsilon$ es un término de error aleatorio independiente de $x$ con media cero.
\item  La función  $f$ representa la información sistemática que $x$ proporciona sobre $y$.
\item El método paramétrico más utilizado asume que $f$ es lineal en $x$:
$$
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k 
$$
\item Para ajustar el modelo lineal, se estiman los parámetros $\beta_0, \beta_1, \ldots, \beta_k$ de manera que:
$$
y \approx \beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k 
$$
    \end{itemize}
\end{frame}

\begin{frame}{}
Con técnicas de regresión de una variable $y$ (respuesta), sobre una variable $x$ (regresor), se busca una función que sea una buena aproximación de una nube de puntos,  mediante una curva del tipo: 
$$
\hat{y}= f(x)
$$
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{figuras/fig03}
    \end{figure}

\end{frame}


\begin{frame}{2. La regresión lineal simple}
\begin{itemize}
\item Modelo:
$$
y_i=\beta_0+\beta_1 x_i+\varepsilon_i
$$
donde $y_i$ es la respuesta, $x_i$ es el regresor, $\beta_0$ es la intersección, $\beta_1$ es la pendiente, y $\varepsilon_i$ es el término de error aleatorio.
 \end{itemize}
 
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
 
\begin{itemize}
\item Características del Modelo:
 \end{itemize}
1. $y_i$ es una variable aleatoria compuesta por $\beta_0+\beta_1 x_i$

2. La intersección $\beta_0$ y la pendiente $\beta_1$ son constantes desconocidas y $\varepsilon$ es un componente de error aleatorio. 

 3. Se supone que los errores tienen media cero y varianza desconocida $\sigma^2$.
        \end{column}
        \begin{column}{0.5\textwidth}
\vspace{0.6cm} 
4. Normalmente se asume que los errores no están correlacionados. 

5. El modelo asume varianza constante para $y_i$:
$$
\sigma^2\left\{y_i\right\}=\sigma^2
$$
6. Los términos de error $\varepsilon_i$ no están correlacionados entre sí, lo que implica que las respuestas $y_i$ tampoco lo están.

        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{}
Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
$$
y_i=\beta_0+\beta_1 x_i+\varepsilon_i
$$

En donde $\beta_0$ es la ordenada en el origen (el valor que toma $y$ cuando $x$ vale 0 ), $\beta_1$ es la pendiente de la recta (e indica cómo cambia $y$ al incrementar $x$ en una unidad) y $\varepsilon$ una variable que incluye un conjunto grande de factores, cada uno de los cuales influye en la respuesta sólo en pequeña magnitud, a la que llamaremos error. 
    \begin{figure}
        \centering
        \includegraphics[width=0.3\textwidth]{figuras/fig04}
    \end{figure}

Por lo tanto, $x$ e $y$ son variables aleatorias, por lo que no se puede establecer una relación lineal exacta entre ellas.
\end{frame}

\begin{frame}{ }
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
\begin{itemize}
\item {\bf Ejemplo}:  Ley de enfriamiento de Steinhart-Hart. 
 \vspace{0.5cm}
 
Para describir con precisión la relación entre la resistencia $R$  de un termistor y su temperatura $T$ se utiliza la ecuación de Steinhart-Hart:
$$
\frac{1}{T}=A+B \ln (R)+C[\ln (R)]^3
$$
donde:

- $T$ es la temperatura.

- $R$ es la resistencia en ohmios.

- $A, B, C$ son constantes específicas del termistor.

 \end{itemize} 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
En un rango pequeño de temperaturas, la relación entre $R$ y $T$ puede aproximarse a:
$$
R \approx R_0+\alpha\left(T-T_0\right)
$$        
donde: $R_0$ es la resistencia a una temperatura de referencia $T_0$ y $\alpha$ es el coeficiente de temperatura, que indica la tasa de cambio de la resistencia con la temperatura.
\vspace{0.2cm}

{\bf Experimento:}  Medición de la resistencia de un termistor NTC

Descripción del Experimento:
\begin{itemize}
\item  Objetivo: Determinar la relación lineal aproximada entre la resistencia y la temperatura de un termistor NTC en un rango limitado de temperaturas.
\item Variables: $y$, resistencia del material y $x$ la temperatura.  
\end{itemize}
        \end{column}
    \end{columns}
\end{frame}


\begin{frame}{ }
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}
Procedimiento:  
\begin{itemize}
\item Calentar el termistor: coloca el termistor en un baño de agua caliente para llevarlo a una temperatura significativamente más alta que la ambiente.
\item  Medir la resistencia y temperatura: retirar el termistor del agua caliente y medir su resistencia a intervalos regulares mientras se enfría. Simultáneamente, medir la temperatura del termistor.
\item Registrar los Datos: anotar las mediciones de resistencia $R$ y temperatura $T$

 \end{itemize} 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
       \vspace{0.8cm} 
            \centering
            \includegraphics[width=\textwidth]{figuras/fig02} % Imagen de ejemplo
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{3. Método de mínimos cuadrados}

El método de los mínimos cuadrados permite  estimar $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. 
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. 

A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador.  El método de mínimos cuadrados consiste en minimizar suma de los cuadrados de los errores:
$$
\sum_{i=1}^n e_i^2=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
$$
Es decir, la suma de los cuadrados de las diferencias entre los valores reales observados $\left(y_i\right)$ y los valores estimados $\left(\hat{y}_i\right)$.

\end{frame}


\begin{frame}{}
Minimizar suma de los cuadrados de los errores se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.

Con este método, las expresiones que se obtiene para $b$ y $m$ son las siguientes:
$$
m=\frac{S_{xy}}{S_x^2} \,,  \quad b=\bar{y}-m \bar{x} \,,
$$

En donde $\bar{x}$ e $\bar{y}$ denotan las medias muestrales de $x$ y $y$ (respectivamente), 
$$
\bar{x}=\frac{\sum_{i=1}^n x_i}{n}, \, \quad \bar{y}=\frac{\sum_{i=1}^n y_i}{n}\,,
$$

$S_x^2$ es la varianza muestral de $x$ y $S_{xy}$ es la covarianza muestral entre $x$ y $y$. Estos parámetros se calculan como:
$$
S_x^2=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}{n}, \, S_y^2=\frac{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}{n}, \, S_{xy}=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{n} .
$$

La cantidad $m$ se denomina coeficiente de regresión de $y$ sobre $x$, lo denotamos por $m_{y/x}$.

\end{frame}

\begin{frame}{}
En nuestro ejemplo, los estadísticos descriptivos anteriores para las ($n=21$) variables temperatura y tiempo del enfriamiento son los siguientes:
\vspace{0.5cm} 
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
$$
\begin{aligned}
& \bar{x}=125.0119 , \quad \bar{y}=2156.81\\
& S_x^2=12896.0685, \quad S_y^2=196582.6099 \\
& S_{xy}=-48017.6465 \\
& m= -3.7234, \quad   b=  2622.2834
\end{aligned}
$$

La recta de regresión ajustada es la siguiente:
$$
\hat{y}=2622.2834 -3.7234 x \,,
$$
donde $\hat{y}$ es la resistencia y $x$ la temperatura. 
            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
            \centering
            \includegraphics[width=\textwidth]{figuras/fig02b} % Imagen de ejemplo
            \vspace{0.5cm} % Espacio vertical
        \end{column}
    \end{columns}

Hay varias propiedades que se cumplen para los mínimos cuadrados:
\end{frame}


\begin{frame}{}
\begin{enumerate}
\item La suma de los valores observados $y_i$ es igual a la suma de los valores ajustados $\hat{y}_i$, o bien
$$
\sum_{i=1}^n y_i=\sum_{i=1}^n \hat{y}_i
$$ 

\item  La suma de los residuos que contiene un intercepto $\beta_0$ es siempre cero
$$
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n e_i=0
$$
\item La recta siempre pasa por el centroide, el punto $(\bar{x}, \bar{y})$ de los datos.
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n  x_i \,, \quad  \bar{y} = \frac{1}{n}\sum_{i=1}^n  y_i= m\bar{x} +b \,. 
$$

\item  La suma de los residuos ponderada por el valor de la variable regresora  es cero
$$
\sum_{i=1}^n x_i e_i=0
$$

\item La suma de los residuos ponderada por el valor ajustado  siempre es cero
$$
\sum_{i=1}^n \hat{y}_i e_i=0
$$

\end{enumerate}
\end{frame}



\begin{frame}{4. Coeficiente de regresión}
El coeficiente de regresión nos da información sobre el comportamiento de la variable $y$ frente a la variable $x$, de manera que:
\begin{enumerate}
\item Si $m_{y / x}=0$, para cualquier valor de $x$ la variable $y$ es constante.
\item Si $m_{y / x}>0$, esto nos indica que al aumentar el valor de $x$, también aumenta el valor de $y$.
\item Si $m_{y / x}<0$, esto nos indica que al aumentar el valor de $x$, el valor de $y$ disminuye.

\end{enumerate}


En el ajuste de regresión lineal para la resistencia y la temperatura resultó en 
$$
\hat{y}=2622.2834 -3.7234 x \,,
$$

El coeficiente de regresión que se obtuvo fue  $m_{y/x}=-3.7234 < 0$ y esto indica que al aumentar $x$ disminuye $y$.
\end{frame}


\begin{frame}{5. Coeficiente de correlación lineal}

El coeficiente de correlación lineal entre $x$ e $y$ viene dado por:
$$
r=\frac{S_{xy}}{S_x S_y}
$$
y es una medida estadística que cuantifica la intensidad de la relación lineal entre dos variables. Su cuadrado se denomina coeficiente de determinación $r^2$.

Propiedades del coeficiente de correlación:
\begin{enumerate}
\item  No tiene dimensión, y siempre toma valores en [-1,1].
\item Si las variables son independientes, entonces $r=0$, el inverso no tiene por qué ser cierto.
\item Si existe una relación lineal exacta entre $x$ e $y$, entonces $r=1$  (relación directa) ó $r=-1$ (relación inversa).
\item Si $r>0$,  indica una relación directa entre las variables (si aumenta $x$,  aumenta $y$ ).
\item Si $r<0$, indica una relación directa entre las variables (si aumenta $x$,  disminuye $y$).
\end{enumerate}
\end{frame}

\begin{frame}{}
Para nuestro ejemplo el valor de $r$ es
$$
r=\frac{S_{xy}}{S_x S_y} = \frac{-48017.6465}{ \sqrt{12896.0685} \sqrt{196582.6099}}= -0.9537
$$
El menos indica que existe una relación inversa entre las variables. Además su valor es próximo a $1$ indicando una dependencia lineal muy fuerte.
    \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
        \vspace{0.2cm} 
El coeficiente de determinación al cuadrado es $r^2=0.9095$.

La relación entre los coeficientes de regresión y de correlación:
$$
m_{y/ x}=r \frac{S_y}{S_x}=-3.7234\,,
$$
$$
m_{x / y}=r \frac{S_x}{S_y}=-0.2443 
$$
Los dos coeficientes de regresión y el coeficiente de correlación tienen  el mismo signo: a medida que $x$ aumenta  $y$ tiende a disminuir.

            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
            \centering
            \includegraphics[width=0.9\textwidth]{figuras/fig05} % Imagen de ejemplo
            \vspace{0.5cm} % Espacio vertical
        \end{column}
    \end{columns}

\end{frame}

\begin{frame}{}
\begin{itemize}
\item Descomposición de la variabilidad: 
\begin{enumerate}
\item Variabilidad Explicada (Sum of Squares for Regression, SSR): Representa la parte de la variabilidad de $y$ que se explica por el modelo de regresión.
$$
\mathrm{SSR}=\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2
$$

\item Variabilidad Residual (Sum of Squares for Error, SSE): Representa la parte de la variabilidad de $y$  que no se puede explicar por el modelo de regresión.
$$
\mathrm{SSE}=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 
$$
\item Variabilidad Total (Total Sum of Squares, SST): Representa la variabilidad total de la variable dependiente $y$ respecto a su media.
$$
\mathrm{SST}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2
$$
\end{enumerate}

\item La variabilidad total se puede descomponer en la variabilidad explicada por el modelo y la variabilidad residual
$$
\mathrm{SST}= \mathrm{SSR} + \mathrm{SSE} \,\, \Rightarrow \,\,
\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 =
\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 + 
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
$$

\end{itemize}
\end{frame}

\begin{frame}{}
\begin{itemize}
\item Coeficiente de determinación $r^2$: El coeficiente de determinación  es una medida que cuantifica la proporción de la variabilidad total de $y$ que es explicada por el modelo de regresión. Se calcula como:
$$
r^2=\frac{\sum\left(\hat{y}_i-\bar{y}\right)^2}{\sum\left(y_i-\bar{y}\right)^2}=\frac{\mathrm{SSR}}{\mathrm{SST}}
$$
\end{itemize}

\begin{itemize}
\item Para el modelo de temperaturas y resistencias:
$$
\mathrm{SSR}= 1787904.8827\,, \,\, \mathrm{SSE}= 177921.2163\,,
\mathrm{SST}= 1965826.099
$$

$$
r^2=\frac{\mathrm{SSR}}{\mathrm{SST}} = \frac{1787904.8827}{1965826.099}= 0.9095 
$$


\end{itemize}
\end{frame}

\begin{frame}{6. Inferencias acerca de los parámetros}

Existen pruebas estadísticas  para evaluar la significancia de los coeficientes en un modelo de regresión. Ayudan a determinar si las relaciones observadas entre las variables independientes (predictores) y la variable dependiente (respuesta) son estadísticamente significativas. 

\begin{enumerate}
\item  Hipótesis en el Contraste de Regresión:
\begin{itemize}
\item  Hipótesis Nula $\left(H_0\right)$: Indica que el coeficiente de regresión no tiene un efecto significativo en la variable dependiente. Matemáticamente es $\beta_i=0$, donde $\beta_i$ es el coeficiente de regresión de una variable independiente específica.

\item  Hipótesis Alternativa $\left(H_a\right)$ Sugiere que el coeficiente de regresión tiene un efecto significativo en la variable dependiente. Esto se expresa como $\beta_i  \neq 0$.
\end{itemize}

\item Estadístico t (t-Estadístico)
Para cada coeficiente de regresión $\beta_i$, se calcula un t-estadístico para evaluar su significancia.
Este se define como:
$$
t_i=\frac{\hat{\beta}_i}{\mathrm{SE}\left(\hat{\beta}_i\right)}
$$
donde $\hat{\beta}_i$ es el estimador del coeficiente de regresión y $\mathrm{SE}\left(\hat{\beta}_i\right)$ es el error estándar del coeficiente de regresión.

\end{enumerate}
\end{frame}

\begin{frame}{}
\begin{enumerate}
\setcounter{enumi}{2} 
\item Valor $\mathrm{p}$ (p-Value): el valor $\mathrm{p}$ asociado con el t-estadístico se utiliza para tomar decisiones respecto a las hipótesis. Si el valor $p$ es menor que un nivel de significancia predefinido (generalmente 0.05), se rechaza la hipótesis nula, indicando que el coeficiente es significativamente diferente de cero.

\item Contraste Global del Modelo: además de evaluar cada coeficiente individualmente, se puede realizar un contraste global del modelo para verificar si al menos una de las variables independientes tiene un efecto significativo. 
Esto se hace usando el estadístico F (F-statistic)

$$
\mathrm{F}=\frac{\mathrm{MSR}}{\mathrm{MSE}}= 
\dfrac{\dfrac{\mathrm{SSR}}{ k-1}}{\dfrac{\mathrm{SSE}}{n-k}} = 
\dfrac{\dfrac{\mathrm{SSR}}{ k-1}}{\hat{\sigma}}
$$
donde $k$  es el número de coeficientes estimados en el modelo de regresión y $n$  el número de observaciones. $\mathrm{MSE} = \hat{\sigma}$ =Error Mean Square o Residual Mean Square. 
\end{enumerate}
\end{frame}

\begin{frame}{}
Para el ejemplo en estudio podemos calcular los  t-estadístico, primero calculamos  el error estándar residual y  los errores estándar de la pendiente $\Delta m$ y el término independiente $\Delta b$ de la regresión lineal.

$$
\mathrm{SE}\left(m\right)^2=\Delta m=\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \,, \,\, 
\mathrm{SE}\left(b\right)^2=\Delta b=\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right] 
$$
donde 
$$
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2} = 9364.2745 
$$

$$
\Delta m = \sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}}= 0.2695
$$
$$
\Delta b = \sqrt{\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right]  }   = 39.7582
$$

$$
t_m=\frac{m}{\Delta m}= -13.8177\,, \,\, t_b=\frac{b}{\Delta b}= 65.9557
$$

\end{frame}

\begin{frame}{}
\begin{itemize}
\item  El nivel de confianza refleja la probabilidad de que el intervalo de confianza contenga el valor verdadero del parámetro de la población. Se construye a través de un  valor crítico $t_{\alpha / 2 , n-2}$ o $t_{\text {critical}}$ que es un punto específico en la distribución ($t$-Student). Un nivel de confianza del $95 \%$ corresponde a $\alpha=0.05$, porque $1-0.95=0.05$. Dado que el nivel de confianza es bilateral, se divide en dos colas de la distribución $t$, lo que significa que cada cola tiene un área de $\alpha / 2$.

\item  Fórmula del intervalo de confianza:
$$
\hat{\beta}_j \pm t_{\alpha / 2, n-2} \cdot \operatorname{SE}\left(\hat{\beta}_j\right)
$$
Aquí, $\hat{\beta}_j$ es el estimador del parámetro (intersección o pendiente), y $\operatorname{SE}\left(\hat{\beta}_j\right)$ es el error estándar del estimador.

\end{itemize}
\end{frame}


\begin{frame}{}
\begin{itemize}
\item Estimaciones de los  intervalos de confianza  para los parámetros:
\end{itemize}
En el contexto del ejemplo para tener un nivel de confianza del $95 \%, \alpha=0.05$. Como los grados de libertad son 19, el valor crítico $t_{\alpha / 2 \text {,dof }}$ para $\alpha / 2=0.025$ y 19 grados de libertad es de 2.093. (prueba t de Student)
 \vspace{0.4cm} 
   \begin{columns}[T] % Alineación superior de las columnas
        \begin{column}{0.5\textwidth}      
Para $\hat{\beta}_1=m=-3.7234$ y  $\mathrm{SE}\left(m\right)=0.2695$
$$
\begin{aligned}
& \mathrm{IC}= \hat{\beta}_1 \pm t_{\alpha / 2, n-2} \cdot \mathrm{SE}(\hat{\beta}_1) \\
& \mathrm{IC}= -3.7234 \pm 2.093 \cdot 0.2695 \\
& \mathrm{IC}= -3.7234 \pm 0.5640\\
& \mathrm{IC}= [-4.2874 \,\, , \,\, -3.1594]
\end{aligned}
$$

%            \vspace{0.5cm} % Espacio vertical
        \end{column}
        \begin{column}{0.5\textwidth}
%        \vspace{1.0cm} 
Para $\hat{\beta}_0=b=2622.2834$ y  $\mathrm{SE}\left(b\right)=39.7582$
$$
\begin{aligned}
& \mathrm{IC}=\hat{\beta}_0 \pm t_{\alpha / 2, n-2} \cdot \mathrm{SE}(\hat{\beta}_0) \\
& \mathrm{IC}= 2622.2834 \pm 2.093 \cdot 39.7582 \\
& \mathrm{IC}= 2622.2834\pm 83.2140 \\
& \mathrm{IC}= [2539.0694 \,\, , \,\,  2705.4975]
\end{aligned}
$$
        \end{column}
    \end{columns}
\vspace{0.5cm} 
Estos intervalos indican que con un $95 \%$ de confianza, el verdadero valor de la pendiente $m$  y el intercepto $b$ se encuentra dentro de este rango.    
    
\end{frame}



\begin{frame}
\begin{itemize}
\item La significancia global del modelo de regresión
\end{itemize}
Para el cálculo de estadístico F (F-statistic)
$$
\mathrm{F}=\frac{\mathrm{MSR}}{\mathrm{MSE}}= \dfrac{\dfrac{\mathrm{SSR}}{ 2-1}}{\dfrac{\mathrm{SSE}}{ 21-2}}
$$

Resulta
$$
\mathrm{F}=\frac{1787904.8827}{9364.2745}= 190.9283
$$

Este valor indica que el modelo es estadísticamente significativo, lo que significa que la relación entre las variables independientes y la variable dependiente es poco probable que se deba al azar.

\begin{itemize}
\item statsmodels: es un módulo de Python que proporciona clases y funciones para la estimación de  modelos estadísticos diferentes, así como para la realización de pruebas estadísticas, y la exploración de datos estadísticos. Para cada estimador se dispone de una extensa lista de estadísticas de resultados. Los resultados se comprueban con paquetes estadísticos existentes para garantizar que son correctos. La documentación en línea se encuentra en statsmodels.org.
\end{itemize}
\end{frame}

\begin{frame}{Resumen}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figuras/fig20a}
        \caption{Descripción de la imagen.}
    \end{figure}
\end{frame}

\begin{frame}{Condiciones y supuestos para modelo de regresión lineal}

Hemos visto cómo aproximar el modelo de regresión lineal simple
$$
y=\beta_0+\beta_1 x +\varepsilon,
$$
por la recta
$$
\hat{y}=b+m x .
$$

Para garantizar que una aproximación usando una regresión lineal simple es válida, deben cumplirse varias condiciones y supuestos. Estas condiciones aseguran que el modelo lineal es una buena representación de la relación entre las variables y que las inferencias realizadas a partir del modelo son fiables. 

\begin{enumerate}
\item {\bf Linealidad:} La relación entre la variable dependiente $y$ y la variable independiente 
$x$ debe ser lineal. Esto significa que los puntos de datos deben seguir un patrón que se asemeja a una línea recta.
\item {\bf Independencia:}  Los errores o residuos $e_i$ deben ser independientes entre sí. Esto significa que el error de una observación no debe influir en el error de otra observación.
\end{enumerate}
\end{frame}

\begin{frame}
\begin{enumerate}
\setcounter{enumi}{2} 
\item {\bf Homoscedasticidad:} La varianza de los errores $e_i=\left(\hat{y}_i-y_i\right)$ debe ser constante en todos los niveles de la variable independiente $x$. Esto implica que la dispersión de los puntos alrededor de la línea de regresión debe ser aproximadamente la misma para todos los valores de $x$.
\item {\bf Normalidad de los Errores:}   Los errores $e_i$  deben seguir una distribución normal con media cero y varianza constante. Esto es especialmente importante para la validación de las pruebas de hipótesis y los intervalos de confianza.
\end{enumerate}

\begin{itemize}
\item Verificación de Condiciones
\end{itemize}
\begin{description}
\item[-]  Linealidad: Diagramas de Dispersión  (Scatter Plots).
\item[-]   Homoscedasticidad: Gráficos de Residuos (Residual Plots).
\item[-]  Independencia: Prueba de Durbin-Watson.
\item[-]  Normalidad: Histograma de Residuos o Gráfico Q-Q (Quantile-Quantile Plot).
\end{description}

\end{frame}

\begin{frame}
\begin{itemize}
\item La prueba de Durbin-Watson
\end{itemize}

Es una prueba estadística utilizada para detectar la presencia de autocorrelación en los residuos (errores) de un modelo de regresión. La autocorrelación ocurre cuando los residuos no son independientes entre sí, lo que puede invalidar muchas de las inferencias que se pueden hacer a partir del modelo.

La ecuación para el estadístico DW es:
$$
\mathrm{DW}=\frac{\sum_{i=2}^n\left(e_i-e_{i-1}\right)^2}{\sum_{i=1}^n e_i^2}
$$
donde $e_i$ son los residuos del modelo.

Interpretación del Estadístico de Durbin-Watson
\begin{itemize}
\item   $\mathrm{DW} \approx 2$ : No hay autocorrelación.
\item   $\mathrm{DW} <2 $  Autocorrelación positiva.
\item   $\mathrm{DW}  >2 $: Autocorrelación negativa.
\end{itemize}
\end{frame}

\begin{frame}
Para los datos considerados en el ejemplo: Medición de la resistencia de un termistor
    \begin{figure}
        \centering
        \includegraphics[width=0.34\textwidth]{figuras/fig05a}
        \includegraphics[width=0.34\textwidth]{figuras/fig06} \\ 
         \includegraphics[width=0.32\textwidth]{figuras/fig07}
        \includegraphics[width=0.34\textwidth]{figuras/fig08}      
        %\caption{Estadístico de Durbin-Watson: 2.027}
    \end{figure}
Estadístico de Durbin-Watson: 2.027
\end{frame}

\begin{frame}
\begin{itemize}
\item Algunos casos en los que no se cumplen los supuestos
\end{itemize}
    \begin{figure}
        \centering
        \includegraphics[width=0.34\textwidth]{figuras/fig09}
        \includegraphics[width=0.34\textwidth]{figuras/fig10} \\ 
         \includegraphics[width=0.32\textwidth]{figuras/fig11a}
        \includegraphics[width=0.34\textwidth]{figuras/fig11}      
        %\caption{Estadístico de Durbin-Watson: 2.027}
    \end{figure}
\end{frame}

\begin{frame}
En el ejemplo mostrado pareciera que a simple vista el ajuste lineal es adecuado pero el resto de criterios falla
    \begin{figure}
        \centering
        \includegraphics[width=0.34\textwidth]{figuras/fig12}
        \includegraphics[width=0.34\textwidth]{figuras/fig13} \\ 
         \includegraphics[width=0.32\textwidth]{figuras/fig14}
        \includegraphics[width=0.34\textwidth]{figuras/fig15}      
        %\caption{Estadístico de Durbin-Watson: 2.027}
    \end{figure}
\end{frame}

\begin{frame}{Resumen}
    \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{figuras/fig16}
        \caption{Descripción de la imagen.}
    \end{figure}
\end{frame}


\end{document}
