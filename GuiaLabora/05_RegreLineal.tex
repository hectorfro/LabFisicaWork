\chapter{Regresión lineal}

\section{Introducción}

Este capítulo lo dedicamos al  análisis de regresión, que es una técnica estadística utilizada para investigar y modelar la relación entre variables. Los métodos de análisis de regresión tienen una gran cantidad de aplicaciones en ingeniería, ciencias físicas y químicas, economía, ciencias de la vida y las ciencias sociales. Otro campo de aplicación es la ciencia y análisis de datos, esto hace que el espectro de problemas donde se aplica el análisis de regresión sea muy amplio.

El análisis de regresión fue desarrollado por primera vez por Sir Francis Galton\footnote{Francis Galton 1822-1911. \url{https://en.wikipedia.org/wiki/Francis_Galton}} a finales del siglo XIX. Galton redescubrió de forma independiente el concepto de correlación y demostró su aplicación en el estudio de la herencia, la antropología y la psicología. Galton desarrolló una descripción matemática de la tendencia regresiva, precursora de los modelos de regresión actuales y el término regresión sigue utilizándose para describir las relaciones estadísticas entre variables.

\section{Regresión y la construcción de modelos}
El análisis de regresión es una técnica estadística que permite investigar y construir modelos que establecen relaciones entre variables.

Las relaciones estadísticas son diferentes de las  relaciones funcionales porque las relaciones estadísticas no son perfectas, es decir, las observaciones de una relación estadística no caen directamente sobre una curva de relación. 

Supongamos que observamos una respuesta cuantitativa $y$ para  $k$ ``predictores'' diferentes: $x_1, x_2, \ldots, x_k$. Podemos suponer también que existe alguna relación entre $y$ y $x=\left(x_1, x_2, \ldots, x_k\right)$, que puede escribirse de la forma muy general
\begin{equation}
y=f(x)+\varepsilon \,.
\label{funcion}
\end{equation}
donde $f$ es alguna función fija pero desconocida de $x_1, \ldots, x_k$, y $\varepsilon$ es un término de error aleatorio, que es independiente de $x$ y tiene media cero. En esta formulación, $f$ representa la información sistemática que $x$ proporciona sobre $y$.

Existen dos razones principales para querer estimar $f$: la predicción y la inferencia. 

\paragraph{Predicción:} Aquí hablamos en el proceso de usar un modelo estadístico o de aprendizaje automático para predecir valores futuros o no observados basados en datos existentes. El objetivo principal de la predicción es obtener valores precisos de la variable de interés y la precisión de las predicciones es el criterio más importante. En este caso, es común encontrarse en situaciones donde se dispone fácilmente de un conjunto de entradas $x$, pero no es posible obtener de manera sencilla la salida $y$. En este escenario, dado que el término de error es igual a cero, podemos predecir $y$ utilizando
$$
\hat{y} = \hat{f}(x)\,,
$$
donde $\hat{f}$ representa nuestra estimación de $f$, y $\hat{y}$ es la predicción resultante para $y$. En este contexto, podemos considerar a $\hat{f}$ como una caja negra, en el sentido de que no nos preocupa la forma exacta de $\hat{f}$, siempre y cuando genere predicciones precisas para $y$.  En general, $\hat{f}$ no será una estimación perfecta de $f$, y esta imprecisión introducirá algún error. Este error puede ser reducible si logramos mejorar la precisión de $\hat{f}$ utilizando una técnica de aprendizaje estadístico más adecuada para estimar $f$. Si pudiésemos construir una predicción perfecta de $f$, de manera que $\hat{y} = f(x)$, esta predicción aún tendría error porque en realidad $y$ es una función de $\varepsilon$, que es de carácter aleatorio. Esto se conoce como error irreducible, ya que por muy bien que estimemos $f$, no podemos reducir el error introducido por $\varepsilon$. 

Los métodos más utilizados para la predicción son la regresión lineal, los árboles de decisión, las redes neuronales, métodos que podríamos usas si queremos predecir precios en la bolsa de valores o el valor de un determinado producto bajo determinadas condiciones.


\paragraph{Inferencia:} Aquí el objetivo no es necesariamente hacer predicciones para $y$ al estimar $f$, sino conocer $f$ de manera exacta. Se refiere al proceso de usar un modelo estadístico para entender la relación entre las variables y sacar conclusiones sobre la población de la cual se extrajo la muestra. El objetivo principal de la inferencia es realizar afirmaciones sobre los parámetros del modelo y sobre la naturaleza de las relaciones entre las variables. En este contexto, podríamos preguntarnos si existe una relación entre $y$ y cada predictor que tome la forma de una ecuación lineal, o si se trata de una relación más compleja. Históricamente, la mayoría de los métodos para estimar $f$ han adoptado una forma lineal. En algunas situaciones, este supuesto es razonable o incluso deseable. Sin embargo, a menudo la relación verdadera es más complicada, en cuyo caso un modelo lineal puede no proporcionar una representación precisa de la relación entre las variables de entrada y de salida. 

Los métodos más utilizados en la inferencia son las pruebas de hipótesis, estimación de los intervalos de confianza, el análisis de varianza (ANOVA), la regresión lineal múltiple. Estas técnicas pueden usarse para un estudio estadístico para inferir si existe una relación entre el consumo de café y el riesgo de padecer diabetes. 

Es muy probable encontrar situaciones que se encuadren en el ámbito de la predicción, en el de la inferencia, o en una combinación de ambos. Por ejemplo, los modelos lineales permiten una inferencia relativamente sencilla e interpretable, pero pueden no producir predicciones tan precisas como otros enfoques. Por el contrario, algunos de los enfoques no lineales pueden proporcionar predicciones bastante precisas para $y$, pero a costa de un modelo menos interpretable, haciendo la inferencia más difícil.


Uno de los métodos más utilizados para estimar $f$ es el ``método paramétrico'', donde, en primer lugar, se hace una suposición sobre la forma funcional de $f$. Por ejemplo, una suposición muy simple es que $f$ es lineal en $x$:
\begin{equation}
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,, 
\label{modlineal}
\end{equation}
En este caso, el problema de estimar $f$ se simplifica enormemente, ya que en lugar de tener que estimar una función $k$-dimensional completamente arbitraria $f(x)$, solo hay que estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_k$. Y en segundo lugar, una vez seleccionado un modelo, necesitamos un procedimiento que utilice los datos para ajustar o entrenar el modelo. En el caso del modelo lineal (\ref{modlineal}), necesitamos estimar los parámetros \( \beta_0, \beta_1, \ldots, \beta_k \). Es decir, queremos encontrar valores de estos parámetros tales que
$$
y \approx \beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,.
$$
El enfoque más habitual para ajustar el modelo lineal (\ref{modlineal}) es el de los mínimos cuadrados ordinarios, por ser uno de los métodos más comunes para ajustar el modelo (\ref{modlineal}). Sin embargo, los mínimos cuadrados son una de las muchas formas posibles de ajustar el modelo lineal; existen otras alternativas.






Veamos el siguiente ejemplo de regresión poblacional, supongamos que en una universidad se ha hecho diferentes estudios sobre la relación que puede haber entre el tiempo de estudio (en horas) y las calificaciones obtenidas por los estudiantes en el  examen final de un curso de matemáticas aplicadas. El curso en cuestión tiene 25 estudiantes y las 25 observaciones se representan en el gráfico de dispersión mostrado en el lado izquierdo de la figura \ref{regres1}. 
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.5in,width=2.5in]{figuras/fig18}  \quad 
\includegraphics[height=2.6in,width=2.6in]{figuras/fig19}  
\caption{Izquierda: gráfico de dispersión para las horas de estudio y las notas obtenidas. Derecha: la  generación de observaciones en la regresión lineal.}
\label{regres1}
\end{center}
\end{figure}

Esta representación sugiere  que existe una relación entre el tiempo de estudio y las notas obtenidas, a medida que aumenta el número de horas aumentan las notas. Además, se puede apreciar que los puntos caen aproximadamente a lo largo de una línea recta, se dice que existe una dispersión de los puntos. Se puede pensar que existe una relación que nos indica la tendencia de la forma
\begin{equation}
y=\beta_0+\beta_1  x \,,
\label{reglin1}
\end{equation}
donde $y$ representa las notas obtenidas, $x$ la dedicación en estudio por número de horas,  $\beta_0$ es la intersección con el eje y $\beta_1$ es la pendiente de la recta. 

Como se puede ver, los puntos de datos no caen exactamente sobre una línea recta, esta dispersión de puntos en torno a la línea representa una variación en el número de cajas repartidas y el tiempo de entrega y es de naturaleza aleatoria. Veremos que estas  relaciones estadísticas pueden ser muy útiles, aunque no tengan la exactitud de una relación funcional. Por lo tanto, la ecuación (\ref{reglin1}) debe modificarse para tener esta característica en cuenta. 
 
Llamemos el error  $\varepsilon$ a la diferencia entre el valor observado de $y$ y la recta $\left(\beta_0+\beta_1 x\right)$. Es conveniente pensar en $\varepsilon$ como un error estadístico; es decir, es una variable aleatoria que explica el fracaso del modelo para ajustarse exactamente a los datos. El error puede estar compuesto por los efectos de otras variables al contabilizar las horas de estudio, errores de medición, etc. Así, un modelo más plausible para los datos de las notas obtenidas es
\begin{equation}
y=\beta_0+\beta_1 x+\varepsilon
\label{reglin2}
\end{equation}

A la ecuación (\ref{reglin2}) se le denomina Modelo de Regresión Lineal (MRL), donde normalmente a  $x$ se le denomina la variable independiente y a $y$ la variable dependiente. Esta denominación puede causar  confusión con el concepto de independencia estadística, por lo que suele denominarse a $x$ como la variable ``predictora'' o ``regresora'' y a la variable $y$ como la variable de respuesta. Dado que la ecuación (\ref{reglin2}) sólo incluye una variable regresora, al modelo se le denomina Modelo de Regresión Lineal Simple (MRLS)




Podemos detenernos un momento para analizar el MRL, supongamos que fijamos el valor de la variable regresora $x$ y observamos el valor de la respuesta $y$. Al fijar $x$ la cantidad aleatoria $\varepsilon$ determinará las propiedades de $y$. Supongamos que la media y la varianza de $\varepsilon$ son $0$ y $\sigma^2$, respectivamente. 

La respuesta media a cualquier valor de la variable regresora se puede escribir de la siguiente manera:
\begin{equation}
\mu_{y \mid x} \equiv E(y \mid x)=E\left(\beta_0+\beta_1 x+\varepsilon\right)=\beta_0+\beta_1 x \,,
\end{equation}
que es la misma relación (\ref{reglin1}). La varianza de $y$ dado cualquier valor de $x$ es la siguiente:
\begin{equation}
\sigma_{y \mid x}^2 \equiv \operatorname{Var}(y \mid x)=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right)=\sigma^2 \,.
\end{equation}

Por lo tanto, el verdadero modelo de regresión $\mu_{y \mid x}=\beta_0+\beta_1 x$ es una línea de valores medios, es decir, la altura de la línea de regresión en cualquier valor de $x$ es sólo el valor esperado de $y$ para ese $x$. La pendiente, $\beta_1$ puede interpretarse como el cambio en la media de $y$ para un cambio unitario en $x$. Además, la variabilidad de $y$ en un valor concreto de $x$ viene determinada por la varianza del componente de error del modelo, $\sigma^2$. Esto implica que existe una distribución de los valores de $y$ en cada $x$ y que la varianza de esta distribución es la misma en cada $x$.

Por ejemplo, si suponemos que el verdadero modelo de regresión que relaciona el tiempo de estudio con las notas es $\mu_{y \mid x}=3.5+2.0 x $, y si suponemos también que la varianza es $\sigma^2=2$ entonces el resultado de estas suposiciones se puede ver en la parte derecha de la  figura \ref{regres1}. Aquí hemos utilizado una distribución normal para describir la variación aleatoria de $\varepsilon$. Puesto que $y$ es la suma de una constante $\beta_0+\beta_1 x$ (la media) y una variable aleatoria $\varepsilon$ de distribución normal, $y$ es una variable aleatoria de distribución normal. Para el caso en que  $x=10$ horas, la nota obtenida $y$ tiene una distribución normal con media $3.5+2(10)=23.5$ minutos y varianza 2. La varianza $\sigma^2$ determina la cantidad de variabilidad o ruido en las observaciones $y$ sobre el tiempo de estudio. Cuando $\sigma^2$ es pequeño, los valores observados de las notas obtenidas caerán cerca de la recta, y cuando $\sigma^2$ es grande, los valores observados de las notas observadas  pueden desviarse considerablemente de la recta.

Notemos que pudimos haber hecho un estudio similar pero con ocho estudiantes del curso,  que elegiríamos de manera aleatoria, para entonces proceder a recopilar los datos sobre el tiempo de estudio y las calificaciones de estos estudiantes. En este caso estaríamos  haciendo lo que se llama un estudio de regresión muestral. Si aplicáramos un  método de regresión, como mínimos cuadrados, podríamos obtener un resultado del tipo  $\hat{y}=3.7+1.8 x$, donde los estimadores obtenidos de la muestra (estimadores muestrales) los llamaremos $m=1.8$ y $b=3.7$.  Para evaluar la precisión de estos  estimadores muestrales, podríamos calcular los errores estándar de $m$ y $b$. Esto nos dará una idea de cuánto pueden variar nuestros estimadores muestrales con respecto a los verdaderos parámetros poblacionales. Por ejemplo, si calculamos el error estándar de $m$ y obtenemos $\Delta m=0.1$, podemos reportar el estimador muestral con su error estándar:
$$
m=1.8 \pm 0.1 \,.
$$
Esto significa que el verdadero valor de $m$ probablemente se encuentra en el intervalo $[1.7,1.9]$.

De alguna manera, los modelos de regresión pueden considerarse como modelos empíricos. Si la relación entre las variables es muy compleja se puede utilizar regresiones lineales a trozos para aproximar la relación verdadera entre las variables $y$ y $x$. En este caso las ecuaciones de regresión sólo son válidas en la región de las variables regresoras contenidas en los datos observados. 

De manera general, la variable de respuesta $y$ puede estar relacionada con un número $k$ regresores, $x_1, x_2, . . . , x_k$, de forma que
\begin{equation}
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k+\varepsilon
\label{reglin3}
\end{equation}

La ecuación (\ref{reglin3}) se denomina Modelo de Regresión Lineal Múltiple, y el adjetivo lineal se emplea para indicar que el modelo es lineal en los parámetros $\beta_0, \beta_1, \ldots, \beta_k$, no porque $y$ sea una función lineal de las $x$. Esto significa que muchos modelos en los que $y$ se relaciona con las $x$ de forma no lineal pueden seguir tratándose como modelos de regresión lineal siempre que la ecuación sea lineal en los $\beta$ 's.

Recordemos que el fin principal en el análisis de regresión es estimar los parámetros desconocidos del modelo de regresión. Este proceso también se denomina ajuste del modelo a los datos. Existen varias técnicas de estimación de parámetros y una de estas técnicas es el método de los mínimos cuadrados. En el ejemplo que hemos mencionado  de las horas de estudio y las notas obtenidas por los estudiantes nos llevaría a una ecuación como la que se muestra a continuación
$$
\hat{y}=3.687+ 1.821 x \,,
$$
donde $\hat{y}$  es el valor ajustado o estimado de las notas correspondiente a un valor del número de horas $x$. Esta ecuación ajustada es la que se  representa en la parte izquierda de la figura \ref{regres1}. Note que un estudiante que estudia cero horas $x=0$ obtendría una nota de $3.7$ de 100. 


Una fase importante de un análisis de regresión es la comprobación de la adecuación del modelo. En esta etapa, se evalúa la idoneidad del modelo y la calidad del ajuste. Este análisis determina la utilidad del modelo de regresión y puede indicar si el modelo es razonable o si necesita ser modificado. Por lo tanto, el análisis de regresión es un proceso iterativo en el que los datos guían la construcción del modelo, se ajusta el modelo a los datos y se evalúa la calidad del ajuste para decidir si se adopta o se modifica.

Es importante señalar que un modelo de regresión no implica una relación causa-efecto entre las variables. Una fuerte relación empírica entre dos o más variables no prueba que exista una relación causal entre los regresores y la respuesta. Para establecer causalidad, la relación debe basarse en algo más allá de los datos de la muestra, como consideraciones teóricas. El análisis de regresión puede ayudar a confirmar una relación causa-efecto, pero no debe ser la única base para tal afirmación.

Finalmente, el análisis de regresión es parte de un enfoque más amplio de análisis de datos para la resolución de problemas. La ecuación de regresión en sí puede no ser el objetivo principal del estudio; a menudo, es más importante entender el sistema que genera los datos.

En el trabajo de laboratorio es importante considerar la etapa que tiene que ver con la recolección de los datos,  todo análisis de regresión es tan bueno como los datos en los que se basa. 

Los problemas de estimación de parámetros pueden resolverse mediante métodos de regresión con el objetivo de predecir la variable de respuesta. Sin embargo, al extrapolar utilizando un modelo de regresión para predecir eventos futuros, se corren riesgos significativos debido a los errores inherentes del modelo. Aun cuando la forma del modelo sea correcta, una estimación incorrecta de los parámetros puede llevar a predicciones deficientes o incorrectas. Es esencial tener en cuenta tanto la precisión del modelo como la exactitud en la estimación de los parámetros para minimizar estos riesgos y mejorar la fiabilidad de las predicciones.

Los modelos de regresión pueden utilizarse con fines de control, donde es crucial que las variables estén relacionadas de forma causal. Sin embargo, para predicción, no es estrictamente necesario que exista una relación causa-efecto; basta con que las relaciones presentes en los datos originales utilizados para construir la ecuación de regresión sigan siendo válidas. Por ejemplo, el consumo diario de electricidad durante el mes de agosto en una ciudad puede ser un buen predictor de la temperatura máxima diaria en agosto. No obstante, cualquier intento de reducir la temperatura máxima disminuyendo el consumo de electricidad estaría claramente condenado al fracaso, ya que la relación entre estas variables no es causal.

\section{Los algoritmos en los modelos de regresión}

Desde el punto de vista computacional, la construcción de un modelo de regresión es un proceso iterativo. Los datos de entrada incluyen tanto los conocimientos teóricos del proceso que se está estudiando como los datos disponibles para especificar un modelo de regresión inicial. Las representaciones gráficas de los datos son muy útiles para esta especificación inicial. A continuación, se estiman los parámetros del modelo, normalmente utilizando el método de mínimos cuadrados. Luego, debe evaluarse la adecuación del modelo, lo que implica identificar posibles errores de especificación en la forma del modelo, la omisión de variables importantes, la inclusión de variables innecesarias o la presencia de datos inusuales o inadecuados. Si el modelo resulta ser inadecuado, es necesario realizar ajustes y volver a estimar los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, se debe validar el modelo para asegurarse de que producirá resultados aceptables para la aplicación final.

El análisis de regresión requiere un uso inteligente y hábil del ordenador. Un buen programa de cálculo de regresión es una herramienta esencial en el proceso de creación de modelos. Sin embargo, la aplicación rutinaria de programas de regresión estándar no suele conducir a resultados satisfactorios. El ordenador no sustituye al pensamiento creativo sobre el problema. Debemos aprender a interpretar lo que nos dice el ordenador y a incorporar esa información en modelos posteriores. Entre la gran variedad de software estadístico disponible, se puede mencionar el SAS (Statistical Analysis System), un paquete de software comercial para análisis de datos que ofrece una amplia gama de procedimientos estadísticos, desde pruebas de hipótesis básicas hasta análisis multivariantes complejos. También está R, un lenguaje de código abierto y gratuito desarrollado por una comunidad global de colaboradores, conocido por su flexibilidad, extensibilidad e innovación.


\section{Regresión lineal simple}

Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
\begin{equation}
y=\beta_0+\beta_1 x+\varepsilon \,.
\label{regrelineal}
\end{equation}

La intersección $\beta_0$  y la pendiente $\beta_1$  son constantes desconocidas, mientras que $\varepsilon$ es un componente de error aleatorio. Suponemos que los errores tienen media cero y varianza desconocida $\sigma^2$. También asumimos que los errores no están correlacionados, lo que significa que el valor de un error no depende del valor de ningún otro error. Es importante considerar el regresor $x$
como controlado por el analista de datos o experimentador y medido con un error despreciable, mientras que la respuesta $y$ es una variable aleatoria. Esto implica que existe una distribución de probabilidad para $y$ 
en cada valor posible de $x$.

Recordemos que la media de esta distribución es:
\begin{equation}
E(y \mid x)=\beta_0+\beta_1 x\,.
\label{medireglin}
\end{equation}

Mientras que la varianza es
\begin{equation}
\operatorname{Var}(y \mid x)=\sigma^2=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right) \,.
\label{varireglin}
\end{equation}

La media de $y$ es una función lineal de $x$, aunque la varianza de $y$ no depende del valor de $x$. Además, como los errores no están correlacionados, las respuestas tampoco lo están. Los coeficientes de regresión $\beta_0$ y $\beta_1$ tienen una interpretación sencilla y a menudo útil. La pendiente $\beta_1$ indica el cambio en la media de la distribución de $y$ producido por un cambio unitario en $x$. Si el rango de datos de $x$ incluye $x = 0$, entonces  $\beta_0$ representa la media de la distribución de la respuesta $y$ cuando $x = 0$. Si el rango de $x$ no incluye cero, entonces $\beta_0$ carece de una interpretación práctica.





\subsection{Método de los mínimos cuadrados}

El método de los mínimos cuadrados es uno de los métodos estadísticos más usados para determinar la recta que mejor represente la tendencia de un conjunto de puntos experimentales. Como mencionamos anteriormente, los parámetros $\beta_0$ y $\beta_1$ son desconocidos y deben estimarse utilizando datos muestrales. Supongamos que tenemos $n$ pares de datos, digamos $\left(y_1, x_1\right),\left(y_2, x_2\right), \ldots,\left(y_n, x_n\right)$. Estos datos pueden proceder de un experimento controlado diseñado específicamente para recopilar los datos, de un estudio observacional o de registros históricos existentes (un estudio retrospectivo).

Para estimar $\beta_0$ y $\beta_1$ utilizaremos el método de los mínimos cuadrados, esto es, estimamos $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. A partir de (\ref{regrelineal}) podemos escribir
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{regrelineal}) puede verse como un modelo de regresión poblacional, mientras que la ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. Así, el criterio de mínimos cuadrados es
\begin{equation}
S\left(\beta_0, \beta_1\right)=\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 x_i\right)^2
\end{equation}

Si la dispersión de los puntos experimentales es debida solo a los errores casuales en las mediciones, la mejor recta será aquella para la cual la suma de los cuadrados de las distancias $\left(y_i-y_0 \right)$ sea un mínimo. Es por esto que, a este método se le llama método de los mínimos cuadrados.


A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador. Como se ha indicado anteriormente, los valores de esas magnitudes tendrán sus correspondientes errores que determinaremos más adelante. 

La desviación de un valor cualquiera $y_i$ determinado experimentalmente con respecto a su valor $y_0$ en la recta, será:
\begin{equation}
\Delta y_i=y_i-y_0=y_i-\left(b+m x_i\right)
\label{deltay}
\end{equation}

Ahora se puede enunciar el principio básico de este método, el cual dice que la mejor recta que puede ser trazada entre esos puntos, es aquella para la cual la suma de los cuadrados de las desviaciones $\Delta y_i$ de los datos experimentales, con respecto a esa recta, es mínima.
$$
\sum_{i=1}^n\left(\Delta y_i\right)^2=\sum_{i=1}^n \left[y_i-b-m x_i \right]^2
$$
donde $n$ es el número de pares de valores de $y$ y $x$.

Ya que la condición exigida es la de minimizar la suma anterior, entonces los parámetros $m$ y $b$ deben ajustarse para cumplir con esta condición. Ello se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.
$$
\begin{aligned}
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial b}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial b} = 
\sum_{i=1}^n -2(y_i-b-m x_i)=0  \\
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial m}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial m} =
\sum_{i=1}^n -2x_i(y_i-b-m x_i)=0
&
\end{aligned}
$$

Por lo tanto, se debe resolver el sistema 
$$
\begin{aligned}
&  \sum_{i=1}^n y_i - \sum_{i=1}^nb - \sum_{i=1}^n m x_i = 0  &\,\, \Rightarrow \,\,&
 nb + m \sum_{i=1}^n  x_i = \sum_{i=1}^n y_i  \\
&  \sum_{i=1}^n y_ix_i - \sum_{i=1}^n bx_i -\sum_{i=1}^n  m x_i^2 =0 & \,\, \Rightarrow \,\,&
b \sum_{i=1}^n x_i  + m \sum_{i=1}^n   x_i^2 = \sum_{i=1}^n y_ix_i
\end{aligned}
$$
para $m$ y $b$. 

Utilizando la regla de Cramer
$$
\begin{aligned}
\Delta= 
{\left|\begin{array}{cc}
n & \sum x_i \\
\sum x_i & \sum x_i^2
\end{array}\right|} = n \sum x_i^2-\left(\sum x_i\right)^2 \,,
\end{aligned}
$$
resulta:
$$
m=\frac{\left|\begin{array}{cc}
n & \sum y_i \\
\sum x_i & \sum x_i y_i
\end{array}\right|}{\Delta}=\frac{n\sum x_i y_i-\sum x_i \sum y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} \quad \text{y} \quad
 b=\frac{\left|\begin{array}{cc}
\sum y_i & \sum x_i \\
\sum x_i y_i & \sum x_i^2 
\end{array}\right|}{\Delta}=\frac{\sum x_i^2 \sum y_i-\sum x_i \sum x_i y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} 
$$

Por lo tanto:
\begin{eqnarray}
m &=&\dfrac{ n\sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{n \sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^n x_i\right)^2} = 
\dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} 
\label{eme2} \\
b &=&\dfrac{\sum\limits_{i=1}^n x_i^2 \sum\limits_{i=1}^n y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n x_i y_i}{n\sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^n x_i\right)^2} =  \bar{y}-m \bar{x} \,,
\label{b2}
\end{eqnarray}
donde $\bar{y} \equiv \frac{1}{n} \sum_{i=1}^n y_i$ y $\bar{x} \equiv \frac{1}{n} \sum_{i=1}^n x_i$ son las medias simples.

De esta manera obtenemos la recta $\hat{y}=mx+b$ que mejor de aproxima a los puntos. Nótese que los términos $\sum x_i^2$ y $\left(\sum x_i\right)^2$ no son lo mismo. Idéntica observación se debe hacer para los términos $\sum x_i y_i $ y $\sum x_i \sum y_i$.

Podríamos construir una tabla como la mostrada en \ref{tablamincua}, para así ordenar la información y facilitar los cálculos. Las cuatro sumas en la última línea, son los valores necesarios para calcular $m$ y $b$. Los valores de $m$ y $b$ que se obtengan por el método de los mínimos cuadrados, deberían ser muy próximos a los obtenidos directamente utilizando el método gráfico. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline $y_i [\,\,]$ & $x_i [\,\,]$ & $x_i^2 [\,\,]$& $x_i y_i [\,\,]$ \\
\hline \hline $y_1$ & $x_1$ & $x_1^2$ & $x_1 y_1$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline $y_n$ & $x_n$ & $x_n^2$ & $x_n y_n$ \\ \hline
\hline $\sum y_i$ &$\sum x_i $& $\sum x_i^2$ & $\sum x_i y_i$ \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar el uso del  método de mínimos cuadrados}
\label{tablamincua}
\end{table}

Pero en la actualidad es casi de rutina utilizar alguna herramienta computacional  que permite hacer los cálculos necesarios y los gráficos para el ajuste de la recta. Aunque el uso de este método no nos obliga a hacer el gráfico de la recta, por razones pedagógicas, es conveniente hacerlo para así observar más claramente las desviaciones de los puntos experimentales con respecto a la recta calculada. 

Una vez obtenido los valores de $m$ y ${b}$, es necesario calcular sus errores correspondientes $\Delta m$ y $\Delta b$. Esto lo podemos hacer calculando las desviaciones estándar  de la pendiente y la ordenada al origen, calculadas a partir de la distribución de diferencias $\Delta y_i$, ecuación (\ref{deltay}),  respecto de la mejor línea de ajuste.

Sea $\hat{y}_i=b+m x_i$ la predicción de $y$ basada en el valor $i$ de $x$. Entonces $e_i=y_i-\hat{y}_i$ representa el $i$ residuo - la diferencia entre el $i$ valor de respuesta observado y el $i$ valor de respuesta predicho por nuestro modelo lineal. Definimos la suma residual de cuadrados (RSS) como
$$
\mathrm{RSS}=e_1^2+e_2^2+\cdots+e_n^2
$$
o equivalentemente como
$$
\mathrm{RSS}=\left(y_1-b-m x_1\right)^2+\left(y_2-b-m x_2\right)^2+\cdots+\left(y_n-b-m x_n\right)^2 .
$$
\begin{table}[!t]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline$x_i [\,\,]$ & $y_i [\,\,]$ & $y_i-m x_i-b$ & $(y_i-m x_i-b)^2$ \\
\hline \hline$x_1$ & $y_1$ & $y_1-m x_1-b$ & $\left(y_1-m x_1-b\right)^2$ \\
\hline$x_2$ & $y_2$ & $y_2-m x_2-b$ & $\left(y_2-m x_2-b\right)^2$ \\
\hline$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline$x_n$ & $y_n$ & $y_n-m x_n-b$ & $\left(y_n-m x_n-b\right)^2$ \\ \hline
\hline & & & $\sum\limits_{i=1}^n \qquad \qquad \quad$ \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar los cálculos de $S_y$.}
\label{tablamincua2}
\end{table}

La diferencia entre el valor observado $y_i$ y el correspondiente valor ajustado $\hat{y}_i$ es un residuo. Matemáticamente, el residuo $i$ es
$$
e_i=y_i-\hat{y}_i=y_i-\left(b+m x_i\right), \quad i=1,2, \ldots, n
$$

Los residuos desempeñan un papel importante en la investigación de la adecuación del modelo y en la detección de desviaciones de los supuestos subyacentes. 

Sea SSE (Sum of Squared Errors) la suma de los cuadrados de los residuos.
$$
\mathrm{SSE} = \sum\limits_{i=1}^n e_i^2 = \sum\limits_{i=1}^n (y_i-\left(b+m x_i\right) )^2
$$

En el contexto del análisis de regresión por mínimos cuadrados, se define la cantidad $S_y$ como la desviación estándar de los errores, también conocida como el error estándar residual. Esta cantidad representa la raíz cuadrada de la suma de los cuadrados de los errores dividida por $n-2$,  matemáticamente es:
$$
S_y = \sqrt{\frac{\mathrm{SSE}}{n - 2}} 
$$
$S_y$ es una medida de la dispersión de los datos alrededor de la recta de regresión ajustada y se utilizada para calcular los errores estándar de la pendiente $\Delta m$
 y el término independiente $\Delta b$ de la regresión lineal.

\begin{eqnarray}
\Delta m &= \sqrt{\dfrac{n}{n \sum\limits_{i=1}^n x_i^2 - \left(\sum\limits_{i=1}^n x_i\right)^2}} \cdot S_y 
\label{Delm}\\ \nonumber  \\
\Delta b &= \sqrt{\dfrac{\sum\limits_{i=1}^n x_i^2}{n \sum\limits_{i=1}^n x_i^2 - \left(\sum\limits_{i=1}^n x_i\right)^2}} \cdot S_y
\label{Delb}
\end{eqnarray}


Finalmente para calcular $S_y$ se puede utilizar como ayuda la tabla \ref{tablamincua2}.


%example
\begin{example}
\label{ejemvelocidadtiempo}
En un experimento sobre cinemática un grupo de estudiantes mide los tiempos con los que se desplaza  un móvil en un riel de aire. Los tiempos son medidos en segundos usando un cronómetro de sensibilidad $\Delta t=0,01$ s. Otro  instrumento  mide las velocidades con las que se desplaza el móvil, este instrumento tiene una sensibilidad de $\Delta v= 0,01$ m/s.  La tabla de datos y la respectiva gráfica se muestran la figura \ref{datosvt}.
\begin{figure}[h]
\centering
\begin{minipage}{0.54\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline $t_i \pm 0,01 (\mathrm{s})$ & $v_i \pm 0,01 (\mathrm{m/s})$  \\\hline \hline 
    \hline $0,00$ & 1.00 \\
    \hline $0,10$ & 1.64 \\
    \hline $0,20$ & 1.51 \\
    \hline $0,30$ & 2.03 \\
    \hline $0,40$ & 2.75 \\
    \hline $0,50$ & 3.59 \\
    \hline $0,60$ & 4.87 \\
    \hline $0,70$ & 5.23 \\
    \hline $0,80$ & 5.44 \\
    \hline $1,00$ & 6.37 \\
    \hline
    \end{tabular}
    \caption{Mediciones de la velocidad de un cuerpo en función del tiempo y la gráfica de dispersión respectiva.}
    \label{datosvt}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
    \centering
    \includegraphics[height=2.6in,width=2.6in]{figuras/fig12}
\end{minipage}
\end{figure}

Como queremos  efectuar los cálculos manualmente es recomendable calcular las siguientes sumas:
$$
\begin{array}{c|c|c|c|c}
n&\sum_{i=1}^n t_i & \sum_{i=1}^n v_i   & \sum_{i=1}^n t_i^2  & \sum_{i=1}^n t_i v_i \\ \hline\hline 
10 & 4.60 & 34.43 & 3.04 & 21.275
\end{array}
$$
$$
\Delta=n\sum\limits_{i=1}^n t_i^2-\left(\sum\limits_{i=1}^n t_i\right)^2= 
10(3.04) - \left(4.60 \right)^2= 9.24 \,\, \mathrm{s}^2
$$
$$
b=\dfrac{\sum\limits_{i=1}^n t_i^2 \sum\limits_{i=1}^n v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n t_i v_i}{\Delta} =
\frac{(3.04)  (34.43) -(4.60) (21.275)}{9.24} = 0.7362 \,\, \mathrm{m/s}
$$
$$
m =\dfrac{n \sum\limits_{i=1}^n t_i v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n v_i}{\Delta} =\frac{10(21.275)  -(4.60) ( 34.43)}{9.24}= 5.8844\,\, \mathrm{m/s^2}
$$

Procedemos a calcular la suma de los cuadrados de los residuos
$$
\begin{array}{c|c}
n& \sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2  \\ \hline\hline 
10 & 1.244  
\end{array} \,\,\Rightarrow \,\,  S_y=\left[\frac{\sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2}{n-2}\right]^{\frac{1}{2}}=
\left(\frac{1.244}{8}\right)^{\frac{1}{2}}= 0.394 \,.
$$
y los errores 
$$
\begin{aligned}
\Delta m & =\left[\frac{n}{\Delta}\right]^{\frac{1}{2}} S_y = \left[\frac{10}{9.24}\right]^{\frac{1}{2}} ( 0.394 )=0.401  \,, \\
\Delta b & =\left[\frac{\sum\limits_{i=1}^n\left(t_i\right)^2}{\Delta}\right]^{\frac{1}{2}} S_y= 
\left[\frac{3.04}{9.24}\right]^{\frac{1}{2}} ( 0.394 )= 0.226 \,. 
\end{aligned}
$$

Por lo tanto nuestro resultado será, por los redondeos hechos en los cálculos
$$
v=mt + b= 5.91 t + 0.724 \,\, \mathrm{m/s}
$$
donde lo correcto sería escribir: $m=5.9 \pm 0.4$ y $b=0.7 \pm 0.2$.


En Python podemos hacer todos los cálculos anteriores, incluida la figura  \ref{datosvt}.
\begin{lstlisting}[language=Python]    
import numpy as np
import matplotlib.pyplot as plt
\end{lstlisting}
Los datos:
\begin{lstlisting}[language=Python]    
yn=  array([1.000, 1.64, 1.51, 2.03, 2.75, 3.59, 4.87, 5.23, 5.44, 6.37])
xn = array([0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 1.00])
\end{lstlisting}

La gráfica de la figura \ref{datosvt} se obtiene a partir de las siguientes lineas de código:
\begin{lstlisting}[language=Python] 
plt.scatter(xn, yn, marker='*')
plt.title(r'Velocidad vs tiempo', fontsize=20)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
\end{lstlisting}

Para usar las ecuaciones (\ref{eme2})-(\ref{b2}) hagamos primero los siguientes cálculos intermedios
\begin{lstlisting}[language=Python] 
#Se obtiene el valor de n (numero de datos)
n=len(xn)
#Las sumatorias necesarias 
Sum_x = np.sum(xn)
Sum_y = np.sum(yn)
Sum_xx = np.sum(xn**2)
Sum_xy = np.sum(xn*yn)
Sum_xSumy = np.sum(xn)*np.sum(yn)
Delta = n*np.sum(xn**2) - (np.sum(xn))**2
print(n,',', Sum_x, ',',Sum_y,',', Sum_xx,',', Sum_xy,',', Sum_xSumy, ',',Delta)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
10 , 4.6 , 34.43 , 3.04 , 21.275, 158.378 , 9.240000000000002
}
\end{tcolorbox} 

Y finalmente calculamos $b$ y $m$
\begin{lstlisting}[language=Python] 
# Se escriben las ecuaciones para b y m 
b=(Sum_xx*Sum_y-Sum_xy*Sum_x)/(n*Sum_xx-Sum_x**2)
m=(n*Sum_xy-Sum_x*Sum_y)/(n*Sum_xx-Sum_x**2)
print('m=',m, ',', 'b=',b)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
m= 5.884415584415585 , b= 0.7361688311688325
}
\end{tcolorbox} 

Grafiquemos la recta y los datos
\begin{lstlisting}[language=Python] 
# La gráfica con los datos y la recta que mejor se ajusta 
y_pred=m_mc*xn + b_mc
plt.figure()
plt.scatter(xn, yn, color='b',marker='+', label='datos medidos')
plt.plot(xn, y_pred, 'r--',label='recta por mínimos cuadrados')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title(r'Velocidad vs tiempo', fontsize=18)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
plt.text(0.6, 1.0, '$y=(5.88) x + 0.736$', fontsize=12)
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.6in,width=3.0in]{figuras/fig13}  
\label{figdatos2}
\end{center}
\end{figure}

El siguiente paso es calcular $\Delta b$ y $\Delta m$, pero primero $S_y$

\begin{lstlisting}[language=Python] 
SSE= np.sum((yn -(b_mc + m_mc*xn))**2)
Sy= np.sqrt(SSE/(n-2))
print('n=',n,',','SSE=', SSE ,',', 'Sy=',Sy)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
n= 10 , SSE= 1.244265584415585 , Sy= 0.3943769745458628
}
\end{tcolorbox}

Los respectivos errores se obtienen de las ecuaciones (\ref{Delm}) y (\ref{Delb}) 

\begin{lstlisting}[language=Python] 
Delta_m = np.sqrt(n/(n*np.sum(xn ** 2) - np.sum(xn)**2))*Sy
Delta_b = np.sqrt(np.sum(xn**2)/(n*np.sum(xn**2)-np.sum(xn)**2))*Sy
print(f'm = {np.round(m_mc, 1)} \u00B1 {np.round(Delta_m, 1)}')
print(f'b = {np.round(b_mc, 1)} \u00B1 {np.round(Delta_b, 1)}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 \pm 0.4$

$b = 0.7 \pm 0.2$
}
\end{tcolorbox}

Por lo tanto:
$$
v= 5.9 t + 0.7 \,\, \mathrm{m/s} \,.
$$

Numpy tiene una función específica para calcular los parámetros $m$ y $b$, más información en \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html}

Veamos como funciona:

\begin{lstlisting}[language=Python] 
# Ajustar la recta por mínimos cuadrados usando linalg.lstsq
A = np.vstack([xn, np.ones(len(xn))]).T
m_c, b_c = np.linalg.lstsq(A, yn, rcond=None)[0]
print(f'm = {np.round(m_c, 1)}' )
print(f'b = {np.round(b_c, 1)}' )
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 $

$b = 0.7 $
}
\end{tcolorbox}


Actualmente, la función ``np.linalg.lstsq'' de Numpy no proporciona directamente los errores estándar de los coeficientes. Sin embargo, hay otras bibliotecas en Python que pueden hacer regresión lineal y proporcionar estos errores de manera más directa. Una de las bibliotecas más utilizadas para este propósito es ``statsmodels''. Para más información ver: \url{https://www.statsmodels.org/stable/index.html}

Veamos como funciona en nuestro ejemplo, primero cargamos la librería 
\begin{lstlisting}[language=Python] 
import statsmodels.api as sm
\end{lstlisting}

Luego ejecutamos las siguientes lineas de código para los cálculos

\begin{lstlisting}[language=Python] 
# Agregamos una constante (columna de unos) a xn
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
\end{lstlisting}

Con las siguientes lineas mostramos los resultados

\begin{lstlisting}[language=Python] 
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.8844 \pm 0.4103$

$b = 0.7362 \pm 0.2262$
}
\end{tcolorbox}

Y ahora la gráfica
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos originales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.6in,width=3.0in]{figuras/fig20}  
\label{figdatos2}
\end{center}
\end{figure}
\end{example}
\exampleline

\subsection{Propiedades del método de mínimos cuadrados}

Podemos notar de la ecuación (\ref{eme2})
$$
m  = \dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} =\sum_{i=1}^n c_i y_i \,,\quad \text{donde} \quad c_i = \frac{\left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2}
\label{eme3}
$$

Se dice que los estimadores $m$ y $b$ por mínimos cuadrados son estimadores no ambiguos porque
$$
\begin{aligned}
E\left(m\right)  =E\left(\sum_{i=1}^n c_i y_i\right)=\sum_{i=1}^n c_i E\left(y_i\right)  =\sum_{i=1}^n c_i\left(\beta_0+\beta_1 x_i\right)=\beta_0 \sum_{i=1}^n c_i+\beta_1 \sum_{i=1}^n c_i x_i
\end{aligned}
$$
ya que por suposición $E\left(\varepsilon_i\right)=0$. Se puede demostrar que $\sum_{i=1}^n c_i=0$ y $\sum_{i=1}^n c_i x_i=1$, por lo que
$$
E\left(m \right)=\beta_1
$$

Es decir, si suponemos que el modelo es correcto $\left[E\left(y_i\right)=\beta_0+\beta_1 x_i\right]$, entonces $m$ es un estimador no sesgado de $\beta_1$. Del mismo modo podemos demostrar que $b$ es un estimador no sesgado de $\beta_0$, o bien
$$
E\left(b\right)=\beta_0 \,.
$$


%example
\begin{example}
\label{ejemcizalla}
Una pega para piezas metálicas se prueba mediante una fuerza de cizallamiento entre las partes. La resistencia al cizallamiento de la unión entre los dos tipos de piezas es una característica crucial de calidad. Se observa que, con el transcurso del tiempo, la pega se debilita, lo cual es un problema que los ingenieros deben resolver. Los datos muestran esta relación con la edad de aplicación de la pega, medida en semanas. Se han recogido veinte observaciones sobre la resistencia al cizallamiento y la edad de la pega, como se muestra en la tabla \ref{datosciza}. Un diagrama de dispersión sugiere una fuerte relación estadística entre la resistencia al cizallamiento y la edad del pegamento. La hipótesis inicial de un modelo lineal, $y=\beta_0+\beta_1 x+\varepsilon$, parece razonable. 
\begin{figure}[h]
\begin{tabular}{ccc}
\hline Observación, $i$ & \begin{tabular}{c} 
Cizallamiento, $y_i ( \mathrm{psi} )$
\end{tabular} & \begin{tabular}{c} 
Edad de la pega, $x_i ($ semanas $)$
\end{tabular} \\
\hline 1 & 2158.70 & 15.50 \\
2 & 1678.15 & 23.75 \\
3 & 2316.00 & 8.00 \\
4 & 2061.30 & 17.00 \\
5 & 2207.50 & 5.50 \\
6 & 1708.30 & 19.00 \\
7 & 1784.70 & 24.00 \\
8 & 2575.00 & 2.50 \\
9 & 2357.90 & 7.50 \\
10 & 2256.70 & 11.00 \\
11 & 2165.20 & 13.00 \\
12 & 2399.55 & 3.75 \\
13 & 1779.80 & 25.00 \\
14 & 2336.75 & 9.75 \\
15 & 1765.30 & 22.00 \\
16 & 2053.50 & 18.00 \\
17 & 2414.40 & 6.00 \\
18 & 2200.50 & 12.50 \\
19 & 2654.20 & 2.00 \\
20 & 1753.70 & 21.50 \\
\hline
\end{tabular}
\label{datosciza}
\caption{Datos para el ejemplo \ref{ejemcizalla}.}
\end{figure}

A continuación haremos los cálculos directamente con Python y la librería ``statsmodels'' ya usada en el ejemplo \ref{ejemvelocidadtiempo}. Primero los datos de la tabla \ref{datosciza}
\begin{lstlisting}[language=Python] 
 # Datos de cizallamiento y edad de la pega
xn = np.array([15.50, 23.75, 8.00, 17.00, 5.50, 19.00, 24.00, 2.50,
                      7.50, 11.00, 13.00, 3.75, 25.00, 9.75, 22.00, 18.00,
                      6.00, 12.50, 2.00, 21.50])
yn = np.array([2158.70, 1678.15, 2316.00, 2061.30, 2207.50,
                          1708.30, 1784.70, 2575.00, 2357.90, 2256.70,
                          2165.20, 2399.55, 1779.80, 2336.75, 1765.30,
                          2053.50, 2414.40, 2200.50, 2654.20, 1753.70])
\end{lstlisting}

Luego hacemos el cálculo de los parámetros $m$ y $b$
\begin{lstlisting}[language=Python] 
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = -37.1536 \pm 2.8891$

$b = 2627.8224 \pm 44.1839$
}
\end{tcolorbox}

Y la gráfica con los datos y la recta obtenida
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.xlabel('Edad de la pega (semanas)')
plt.ylabel('Fuerza de cizallamiento (psi)')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.6in,width=3.0in]{figuras/fig21}  
\label{figdatos2}
\end{center}
\end{figure}

La recta tiene por ecuación
$$
\hat{y}=2627.82-37.15 x \,.
$$
Recordemos que la pendiente $-37.15$ se interpreta como la disminución media semanal de la resistencia al cizallamiento debida a la edad de la pega. Dado que el límite inferior de las $x$ está cerca del origen, el intercepto $ 2627.82$ representa la resistencia al corte en un lote de muestra inmediatamente después de la aplicación de la pega. 

Veamos los valores ajustados $\hat{y}$ obtenidos
\begin{lstlisting}[language=Python] 
y_pred = m * xn + b
y_pred
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([2051.94169936, 1745.42457406, 2330.59363144, 1996.21131294,
       2423.4776088 , 1921.90413105, 1736.13617632, 2534.93838164,
       2349.17042691, 2219.13285861, 2144.82567672, 2488.49639296,
       1698.98258538, 2265.57484729, 1810.44335821, 1959.05772199,
       2404.90081333, 2163.40247219, 2553.51517711, 1829.02015369])
}
\end{tcolorbox}
y también los residuos
\begin{lstlisting}[language=Python] 
e_i=yn-y_pred
e_i
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([ 106.75830064,  -67.27457406,  -14.59363144,   65.08868706,
       -215.9776088 , -213.60413105,   48.56382368,   40.06161836,
          8.72957309,   37.56714139,   20.37432328,  -88.94639296,
         80.81741462,   71.17515271,  -45.14335821,   94.44227801,
          9.49918667,   37.09752781,  100.68482289,  -75.32015369])
}
\end{tcolorbox}

Hay varias propiedades de los mínimos cuadrados que podemos probar:


1)  La suma de los valores observados $y_i$ es igual a la suma de los valores ajustados $\hat{y}_i$, o bien
$$
\sum_{i=1}^n y_i=\sum_{i=1}^n \hat{y}_i
$$
El lado izquierdo de la ecuación es:
\begin{lstlisting}[language=Python] 
Sum_y = np.sum(yn)
Sum_y 
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
42627.149999999994
}
\end{tcolorbox}
Y el lado derecho:
\begin{lstlisting}[language=Python] 
Sum_yp = np.sum(y_pred)
Sum_yp
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
42627.149999999965
}
\end{tcolorbox}

2) La suma de los residuos en cualquier modelo de regresión que contiene un intercepto $\beta_0$ es siempre cero, es decir,
$$
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n e_i=0
$$

\begin{lstlisting}[language=Python] 
np.sum(e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
3.069544618483633e-11
}
\end{tcolorbox}
Es decir, $\sum e_i=0.00$. 

3)  La recta de regresión por mínimos cuadrados siempre pasa por el centroide, el punto $(\bar{x}, \bar{y})$ de los datos.
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n  x_i \,, \quad  \bar{y} = \frac{1}{n}\sum_{i=1}^n  y_i= m\bar{x} +b \,. 
$$
\begin{lstlisting}[language=Python] 
# Los promedios de x y y 
xp =np.sum(xn)/len(xn) 
yp=np.sum(yn)/len(xn)
# Se imprimen los promedios y la recta evaluada en xp
print(xp,',', yp,',', m * xp + b)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
13.3625 , 2131.3574999999996 , 2131.3574999999983
}
\end{tcolorbox}

4) La suma de los residuos ponderada por el valor correspondiente de la variable regresora siempre es igual a cero, es decir,
$$
\sum_{i=1}^n x_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(xn*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
4.2746250983327627e-10
}
\end{tcolorbox}

5) La suma de los residuos ponderada por el valor ajustado correspondiente siempre
es igual a cero, es decir
$$
\sum_{i=1}^n \hat{y}_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(y_pred*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
6.478512659668922e-08
}
\end{tcolorbox}


Los programas estadísticos suelen ofrecer un resumen de todos los cálculos que realiza el programa. La salida en pantalla o consola de la figura \ref{figejemciza} presenta los resultados de la librería ``statsmodels''  para el ejemplo que estamos considerando. La parte superior de la tabla contiene el modelo de regresión ajustado. Observe que, antes del redondeo, los coeficientes de regresión coinciden con los que calculamos manualmente. La salida en pantalla mostrada el la figura \ref{figejemciza} también contiene otra información sobre el modelo de regresión que pueden ser consultadas en el manual de la librería. 

\begin{lstlisting}[language=Python] 
results = model
print(results.summary())
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=4.0in,width=6.4in]{figuras/fig22}  
\caption{Resumen de las estimaciones estadísticas por consola.}
\label{figejemciza}
\end{center}
\end{figure}

\end{example}
\exampleline


\subsubsection{La estimación de $\sigma^2$}

Además de estimar $\beta_0$ y $\beta_1$, se necesita una estimación de la varianza del error $\sigma^2$, que  proporciona información crucial sobre la variabilidad de los errores (residuos) y tiene varias aplicaciones en el análisis estadístico como  probar hipótesis y construir estimaciones de intervalo pertinentes para el modelo de regresión. Lo ideal sería que esta estimación no dependiera de la adecuación del modelo ajustado. Esto sólo es posible cuando hay varias observaciones de $y$ para al menos un valor de $x$  o cuando se dispone de información previa sobre $\sigma^2$. Cuando no se puede utilizar este enfoque, la estimación de $\sigma^2$ se obtiene a partir de la suma de cuadrados de los  errores:
\begin{equation}
\hat{\sigma}^2=\sum_{i=1}^n e_i^2=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 \,.
\end{equation}

Conocidos los valores estimados de $\beta_0$ y $\beta_1$, es decir: $
\hat{y}_i=b+m x_i $ y al sustituir en la ecuación anterior
\begin{equation}
\hat{\sigma}^2=\sum_{i=1}^n\left(y_i - b-m x_i \right)^2 = 
\sum_{i=1}^n\left(y_i - \bar{y}+m\bar{x} -m x_i \right)^2 = 
\sum_{i=1}^n\left(y_i - \bar{y}+m(\bar{x} - x_i) \right)^2 
\end{equation}

\begin{equation}
\sum_{i=1}^n y_i^2-n \bar{y}^2-\hat{\beta}_1 \sum_{i=1}^n y_i\left(x_i-\bar{x}\right)
\end{equation}

$$
=\sum_{i=1}^n y_i\left(x_i-\bar{x}\right)
$$

$b=\bar{y}-m\bar{x}$



