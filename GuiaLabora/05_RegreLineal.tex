\chapter{Regresión lineal}

\section{Introducción}

Este capítulo lo dedicamos al  análisis de regresión, que es una técnica estadística utilizada para investigar y modelar la relación entre variables. Los métodos de análisis de regresión tienen una gran cantidad de aplicaciones en ingeniería, ciencias físicas y químicas, economía, ciencias de la vida y las ciencias sociales. Otro campo de aplicación es la ciencia y análisis de datos, esto hace que el espectro de problemas donde se aplica el análisis de regresión sea muy amplio.

El análisis de regresión fue desarrollado por primera vez por Sir Francis Galton\footnote{Francis Galton 1822-1911. \url{https://en.wikipedia.org/wiki/Francis_Galton}} a finales del siglo XIX. Galton redescubrió de forma independiente el concepto de correlación y demostró su aplicación en el estudio de la herencia, la antropología y la psicología. Galton desarrolló una descripción matemática de la tendencia regresiva, precursora de los modelos de regresión actuales y el término regresión sigue utilizándose para describir las relaciones estadísticas entre variables.

\section{Regresión y la construcción de modelos}
El análisis de regresión es una técnica estadística que permite investigar y construir modelos que establecen relaciones entre variables.

Las relaciones estadísticas son diferentes de las  relaciones funcionales porque las relaciones estadísticas no son perfectas, es decir, las observaciones de una relación estadística no caen directamente sobre una curva de relación. 

Veamos el siguiente ejemplo de regresión poblacional, supongamos que en una universidad se han hecho diferentes estudios sobre la relación que puede haber entre el tiempo de estudio (en horas) y las calificaciones obtenidas por los estudiantes en el  examen final de un curso de matemáticas aplicadas. El curso en cuestión tiene 25 estudiantes y las 25 observaciones se representan en el gráfico de dispersión mostrado en el lado izquierdo de la figura \ref{regres1}. 
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.5in,width=2.5in]{figuras/fig18}  \quad 
\includegraphics[height=2.6in,width=2.6in]{figuras/fig19}  
\caption{Izquierda: gráfico de dispersión para las horas de estudio y las notas obtenidas. Derecha: la  generación de observaciones en la regresión lineal.}
\label{regres1}
\end{center}
\end{figure}

Esta representación sugiere  que existe una relación entre el tiempo de estudio y las notas obtenidas,   a medida que aumenta el número de horas aumentan las notas. Además, se puede ver que  los puntos de datos caen aproximadamente a lo largo de una línea recta, existe una dispersión de los puntos. Se puede pensar que existe una relación que nos indica la tendencia de la forma
\begin{equation}
y=\beta_0+\beta_1  x \,,
\label{reglin1}
\end{equation}
donde $y$ representa las notas obtenidas, $x$ la dedicación en estudio por número de horas,  $\beta_0$ es la intersección con el eje y $\beta_1$ es la pendiente de la recta. Como se puede ver, los puntos de datos no caen exactamente sobre una línea recta, esta dispersión de puntos en torno a la línea representa una variación en el número de cajas repartidas y el tiempo de entrega y es de naturaleza aleatoria. Veremos que estas  relaciones estadísticas pueden ser muy útiles, aunque no tengan la exactitud de una relación funcional. Por lo tanto, la ecuación (\ref{reglin1}) debe modificarse para tener esta característica en cuenta. 
 
Llamemos el error  $\varepsilon$ a la diferencia entre el valor observado de $y$ y la recta $\left(\beta_0+\beta_1 x\right)$. Es conveniente pensar en $\varepsilon$ como un error estadístico; es decir, es una variable aleatoria que explica el fracaso del modelo para ajustarse exactamente a los datos. El error puede estar compuesto por los efectos de otras variables al contabilizar las horas de estudio, errores de medición, etc. Así, un modelo más plausible para los datos de los tiempos de las entregas es
\begin{equation}
y=\beta_0+\beta_1 x+\varepsilon
\label{reglin2}
\end{equation}

A la ecuación (\ref{reglin2}) se le denomina Modelo de Regresión Lineal (MRL), donde normalmente a  $x$ se le denomina la variable independiente y a $y$ la variable dependiente. Esta denominación puede causar  confusión con el concepto de independencia estadística, por lo que suele denominarse a $x$ como la variable ``predictora'' o ``regresora'' y a $y$ como la variable de respuesta. Dado que la ecuación (\ref{reglin2}) sólo incluye una variable regresora, se denomina Modelo de Regresión Lineal Simple.

Podemos detenernos un momento para analizar el MRL, supongamos que fijamos el valor de la variable regresora $x$ y observamos el valor de la respuesta $y$. Al fijar $x$ la cantidad aleatoria $\varepsilon$ determinará las propiedades de $y$. Supongamos que la media y la varianza de $\varepsilon$ son $0$ y $\sigma^2$, respectivamente. 

La respuesta media a cualquier valor de la variable regresora se puede escribir de la siguiente manera:
\begin{equation}
\mu_{y \mid x} \equiv E(y \mid x)=E\left(\beta_0+\beta_1 x+\varepsilon\right)=\beta_0+\beta_1 x \,,
\end{equation}
que es la misma relación (\ref{reglin1}). La varianza de $y$ dado cualquier valor de $x$ es la siguiente:
\begin{equation}
\sigma_{y \mid x}^2 \equiv \operatorname{Var}(y \mid x)=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right)=\sigma^2 \,.
\end{equation}

Por lo tanto, el verdadero modelo de regresión $\mu_{y \mid x}=\beta_0+\beta_1 x$ es una línea de valores medios, es decir, la altura de la línea de regresión en cualquier valor de $x$ es sólo el valor esperado de $y$ para ese $x$. La pendiente, $\beta_1$ puede interpretarse como el cambio en la media de $y$ para un cambio unitario en $x$. Además, la variabilidad de $y$ en un valor concreto de $x$ viene determinada por la varianza del componente de error del modelo, $\sigma^2$. Esto implica que existe una distribución de los valores de $y$ en cada $x$ y que la varianza de esta distribución es la misma en cada $x$.

Por ejemplo, si suponemos que el verdadero modelo de regresión que relaciona el tiempo de estudio con las notas es $\mu_{y \mid x}=3.5+2.0 x $, y si suponemos también que la varianza es $\sigma^2=2$ entonces el resultado de estas suposiciones se puede ver en la parte derecha de la  figura \ref{regres1}. Aquí hemos utilizado una distribución normal para describir la variación aleatoria de $\varepsilon$. Puesto que $y$ es la suma de una constante $\beta_0+\beta_1 x$ (la media) y una variable aleatoria $\varepsilon$ de distribución normal, $y$ es una variable aleatoria de distribución normal. Para el caso en que  $x=10$ horas, la nota obtenida $y$ tiene una distribución normal con media $3.5+2(10)=23.5$ minutos y varianza 2. La varianza $\sigma^2$ determina la cantidad de variabilidad o ruido en las observaciones $y$ sobre el tiempo de estudio. Cuando $\sigma^2$ es pequeño, los valores observados de las notas obtenidas caerán cerca de la recta, y cuando $\sigma^2$ es grande, los valores observados de las notas observadas  pueden desviarse considerablemente de la recta.

Notemos que pudimos haber hecho un estudio similar pero con ocho estudiantes del curso,  que elegiríamos de manera aleatoria, para entonces proceder a recopilar los datos sobre el tiempo de estudio y las calificaciones de estos estudiantes. En este caso estaríamos  haciendo lo que se llama un estudio de regresión muestral. Si aplicáramos un  método de regresión, como mínimos cuadrados, podríamos obtener un resultado del tipo  $\hat{y}=3.7+1.8 x$, donde los estimadores obtenidos de la muestra (estimadores muestrales) los llamaremos $m=1.8$ y $b=3.7$.  Para evaluar la precisión de estos  estimadores muestrales, podríamos calcular los errores estándar de $m$ y $b$. Esto nos dará una idea de cuánto pueden variar nuestros estimadores muestrales con respecto a los verdaderos parámetros poblacionales. Por ejemplo, si calculamos el error estándar de $m$ y obtenemos $\Delta m=0.1$, podemos reportar el estimador muestral con su error estándar:
$$
m=1.8 \pm 0.1 \,.
$$
Esto significa que el verdadero valor de $m$ probablemente se encuentra en el intervalo $[1.7,1.9]$.

De alguna manera, los modelos de regresión pueden considerarse como modelos empíricos. Si la relación entre las variables es muy compleja se puede utilizar regresiones lineales a trozos para aproximar la relación verdadera entre las variables $y$ y $x$. En este caso las ecuaciones de regresión sólo son válidas en la región de las variables regresoras contenidas en los datos observados. 

De manera general, la variable de respuesta $y$ puede estar relacionada con un número $k$ regresores, $x_1, x_2, . . . , x_k$, de forma que
\begin{equation}
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k+\varepsilon
\label{reglin3}
\end{equation}

La ecuación (\ref{reglin3}) se denomina Modelo de Regresión Lineal Múltiple, y el adjetivo lineal se emplea para indicar que el modelo es lineal en los parámetros $\beta_0, \beta_1, \ldots, \beta_k$, no porque $y$ sea una función lineal de las $x$. Esto significa que muchos modelos en los que $y$ se relaciona con las $x$ de forma no lineal pueden seguir tratándose como modelos de regresión lineal siempre que la ecuación sea lineal en los $\beta$ 's.

Recordemos que el fin principal en el análisis de regresión es estimar los parámetros desconocidos del modelo de regresión. Este proceso también se denomina ajuste del modelo a los datos. Existen varias técnicas de estimación de parámetros y una de estas técnicas es el método de los mínimos cuadrados. En el ejemplo que hemos mencionado  de las horas de estudio y las notas obtenidas por los estudiantes nos llevaría a una ecuación como la que se muestra a continuación
$$
\hat{y}=3.687+ 1.821 x \,,
$$
donde $\hat{y}$  es el valor ajustado o estimado de las notas correspondiente a un valor del número de horas $x$. Esta ecuación ajustada es la que se  representa en la parte izquierda de la figura \ref{regres1}. Note que un estudiante que estudia cero horas $x=0$ obtendría una nota de $3.7$ de 100. 


Una fase importante de un análisis de regresión es la comprobación de la adecuación del modelo. En esta etapa, se evalúa la idoneidad del modelo y la calidad del ajuste. Este análisis determina la utilidad del modelo de regresión y puede indicar si el modelo es razonable o si necesita ser modificado. Por lo tanto, el análisis de regresión es un proceso iterativo en el que los datos guían la construcción del modelo, se ajusta el modelo a los datos y se evalúa la calidad del ajuste para decidir si se adopta o se modifica.

Es importante señalar que un modelo de regresión no implica una relación causa-efecto entre las variables. Una fuerte relación empírica entre dos o más variables no prueba que exista una relación causal entre los regresores y la respuesta. Para establecer causalidad, la relación debe basarse en algo más allá de los datos de la muestra, como consideraciones teóricas. El análisis de regresión puede ayudar a confirmar una relación causa-efecto, pero no debe ser la única base para tal afirmación.

Finalmente, el análisis de regresión es parte de un enfoque más amplio de análisis de datos para la resolución de problemas. La ecuación de regresión en sí puede no ser el objetivo principal del estudio; a menudo, es más importante entender el sistema que genera los datos.

En el trabajo de laboratorio es importante considerar la etapa que tiene que ver con la recolección de los datos,  todo análisis de regresión es tan bueno como los datos en los que se basa. 

Los problemas de estimación de parámetros pueden resolverse mediante métodos de regresión con el objetivo de predecir la variable de respuesta. Sin embargo, al extrapolar utilizando un modelo de regresión para predecir eventos futuros, se corren riesgos significativos debido a los errores inherentes del modelo. Aun cuando la forma del modelo sea correcta, una estimación incorrecta de los parámetros puede llevar a predicciones deficientes o incorrectas. Es esencial tener en cuenta tanto la precisión del modelo como la exactitud en la estimación de los parámetros para minimizar estos riesgos y mejorar la fiabilidad de las predicciones.

Los modelos de regresión pueden utilizarse con fines de control, donde es crucial que las variables estén relacionadas de forma causal. Sin embargo, para predicción, no es estrictamente necesario que exista una relación causa-efecto; basta con que las relaciones presentes en los datos originales utilizados para construir la ecuación de regresión sigan siendo válidas. Por ejemplo, el consumo diario de electricidad durante el mes de agosto en una ciudad puede ser un buen predictor de la temperatura máxima diaria en agosto. No obstante, cualquier intento de reducir la temperatura máxima disminuyendo el consumo de electricidad estaría claramente condenado al fracaso, ya que la relación entre estas variables no es causal.

\section{Los algoritmos en los modelos de regresión}

Desde el punto de vista computacional, la construcción de un modelo de regresión es un proceso iterativo. Los datos de entrada incluyen tanto los conocimientos teóricos del proceso que se está estudiando como los datos disponibles para especificar un modelo de regresión inicial. Las representaciones gráficas de los datos son muy útiles para esta especificación inicial. A continuación, se estiman los parámetros del modelo, normalmente utilizando el método de mínimos cuadrados. Luego, debe evaluarse la adecuación del modelo, lo que implica identificar posibles errores de especificación en la forma del modelo, la omisión de variables importantes, la inclusión de variables innecesarias o la presencia de datos inusuales o inadecuados. Si el modelo resulta ser inadecuado, es necesario realizar ajustes y volver a estimar los parámetros. Este proceso puede repetirse varias veces hasta obtener un modelo adecuado. Finalmente, se debe validar el modelo para asegurarse de que producirá resultados aceptables para la aplicación final.

El análisis de regresión requiere un uso inteligente y hábil del ordenador. Un buen programa de cálculo de regresión es una herramienta esencial en el proceso de creación de modelos. Sin embargo, la aplicación rutinaria de programas de regresión estándar no suele conducir a resultados satisfactorios. El ordenador no sustituye al pensamiento creativo sobre el problema. Debemos aprender a interpretar lo que nos dice el ordenador y a incorporar esa información en modelos posteriores. Entre la gran variedad de software estadístico disponible, se puede mencionar el SAS (Statistical Analysis System), un paquete de software comercial para análisis de datos que ofrece una amplia gama de procedimientos estadísticos, desde pruebas de hipótesis básicas hasta análisis multivariantes complejos. También está R, un lenguaje de código abierto y gratuito desarrollado por una comunidad global de colaboradores, conocido por su flexibilidad, extensibilidad e innovación.


\section{Regresión lineal simple}

Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
\begin{equation}
y=\beta_0+\beta_1 x+\varepsilon \,.
\label{regrelineal}
\end{equation}

La intersección $\beta_0$  y la pendiente $\beta_1$  son constantes desconocidas, mientras que $\varepsilon$ es un componente de error aleatorio. Suponemos que los errores tienen media cero y varianza desconocida $\sigma^2$. También asumimos que los errores no están correlacionados, lo que significa que el valor de un error no depende del valor de ningún otro error. Es importante considerar el regresor $x$
como controlado por el analista de datos o experimentador y medido con un error despreciable, mientras que la respuesta $y$ es una variable aleatoria. Esto implica que existe una distribución de probabilidad para $y$ 
en cada valor posible de $x$.

Recordemos que la media de esta distribución es:
\begin{equation}
E(y \mid x)=\beta_0+\beta_1 x\,.
\label{medireglin}
\end{equation}

Mientras que la varianza es
\begin{equation}
\operatorname{Var}(y \mid x)=\sigma^2=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right) \,.
\label{varireglin}
\end{equation}

La media de $y$ es una función lineal de $x$, aunque la varianza de $y$ no depende del valor de $x$. Además, como los errores no están correlacionados, las respuestas tampoco lo están. Los coeficientes de regresión $\beta_0$ y $\beta_1$ tienen una interpretación sencilla y a menudo útil. La pendiente $\beta_1$ indica el cambio en la media de la distribución de $y$ producido por un cambio unitario en $x$. Si el rango de datos de $x$ incluye $x = 0$, entonces  $\beta_0$ representa la media de la distribución de la respuesta $y$ cuando $x = 0$. Si el rango de $x$ no incluye cero, entonces $\beta_0$ carece de una interpretación práctica.





\subsection{Método de los mínimos cuadrados}

El método de los mínimos cuadrados es uno de los métodos estadísticos más usados para determinar la recta que mejor represente la tendencia de un conjunto de puntos experimentales. Como mencionamos anteriormente, los parámetros $\beta_0$ y $\beta_1$ son desconocidos y deben estimarse utilizando datos muestrales. Supongamos que tenemos $n$ pares de datos, digamos $\left(y_1, x_1\right),\left(y_2, x_2\right), \ldots,\left(y_n, x_n\right)$. Estos datos pueden proceder de un experimento controlado diseñado específicamente para recopilar los datos, de un estudio observacional o de registros históricos existentes (un estudio retrospectivo).

Para estimar $\beta_0$ y $\beta_1$ utilizaremos el método de los mínimos cuadrados, esto es, estimamos $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. A partir de (\ref{regrelineal}) podemos escribir
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{regrelineal}) puede verse como un modelo de regresión poblacional, mientras que la ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. Así, el criterio de mínimos cuadrados es
\begin{equation}
S\left(\beta_0, \beta_1\right)=\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 x_i\right)^2
\end{equation}

Si la dispersión de los puntos experimentales es debida solo a los errores casuales en las mediciones, la mejor recta será aquella para la cual la suma de los cuadrados de las distancias $\left(y_i-y_0 \right)$ sea un mínimo. Es por esto que, a este método se le llama método de los mínimos cuadrados.


A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador. Como se ha indicado anteriormente, los valores de esas magnitudes tendrán sus correspondientes errores que determinaremos más adelante. 

La desviación de un valor cualquiera $y_i$ determinado experimentalmente con respecto a su valor $y_0$ en la recta, será:
\begin{equation}
\Delta y_i=y_i-y_0=y_i-\left(b+m x_i\right)
\label{deltay}
\end{equation}

Ahora se puede enunciar el principio básico de este método, el cual dice que la mejor recta que puede ser trazada entre esos puntos, es aquella para la cual la suma de los cuadrados de las desviaciones $\Delta y_i$ de los datos experimentales, con respecto a esa recta, es mínima.
$$
\sum_{i=1}^n\left(\Delta y_i\right)^2=\sum_{i=1}^n \left[y_i-b-m x_i \right]^2
$$
donde $n$ es el número de pares de valores de $y$ y $x$.

Ya que la condición exigida es la de minimizar la suma anterior, entonces los parámetros $m$ y $b$ deben ajustarse para cumplir con esta condición. Ello se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.
$$
\begin{aligned}
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial b}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial b} = 
\sum_{i=1}^n -2(y_i-b-m x_i)=0  \\
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial m}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial m} =
\sum_{i=1}^n -2x_i(y_i-b-m x_i)=0
&
\end{aligned}
$$

Por lo tanto, se debe resolver el sistema 
$$
\begin{aligned}
&  \sum_{i=1}^n y_i - \sum_{i=1}^nb - \sum_{i=1}^n m x_i = 0  &\,\, \Rightarrow \,\,&
 nb + m \sum_{i=1}^n  x_i = \sum_{i=1}^n y_i  \\
&  \sum_{i=1}^n y_ix_i - \sum_{i=1}^n bx_i -\sum_{i=1}^n  m x_i^2 =0 & \,\, \Rightarrow \,\,&
b \sum_{i=1}^n x_i  + m \sum_{i=1}^n   x_i^2 = \sum_{i=1}^n y_ix_i
\end{aligned}
$$
para $m$ y $b$. 

Utilizando la regla de Cramer
$$
\begin{aligned}
\Delta= 
{\left|\begin{array}{cc}
n & \sum x_i \\
\sum x_i & \sum x_i^2
\end{array}\right|} = n \sum x_i^2-\left(\sum x_i\right)^2 \,,
\end{aligned}
$$
resulta:
$$
m=\frac{\left|\begin{array}{cc}
n & \sum y_i \\
\sum x_i & \sum x_i y_i
\end{array}\right|}{\Delta}=\frac{n\sum x_i y_i-\sum x_i \sum y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} \quad \text{y} \quad
 b=\frac{\left|\begin{array}{cc}
\sum y_i & \sum x_i \\
\sum x_i y_i & \sum x_i^2 
\end{array}\right|}{\Delta}=\frac{\sum x_i^2 \sum y_i-\sum x_i \sum x_i y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} 
$$

Por lo tanto:
\begin{eqnarray}
m &=&\dfrac{ n\sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{n \sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^n x_i\right)^2} = 
\dfrac{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2}
\label{eme2} \\
b &=&\dfrac{\sum\limits_{i=1}^n x_i^2 \sum\limits_{i=1}^n y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n x_i y_i}{n\sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^n x_i\right)^2} =  \bar{y}-m \bar{x} \,,
\label{b2}
\end{eqnarray}
donde $\bar{y} \equiv \frac{1}{n} \sum_{i=1}^n y_i$ y $\bar{x} \equiv \frac{1}{n} \sum_{i=1}^n x_i$ son las medias simples.

De esta manera obtenemos la recta $\hat{y}=mx+b$ que mejor de aproxima a los puntos. Nótese que los términos $\sum x_i^2$ y $\left(\sum x_i\right)^2$ no son lo mismo. Idéntica observación se debe hacer para los términos $\sum x_i y_i $ y $\sum x_i \sum y_i$.

Podríamos construir una tabla como la mostrada en \ref{tablamincua}, para así ordenar la información y facilitar los cálculos. Las cuatro sumas en la última línea, son los valores necesarios para calcular $m$ y $b$. Los valores de $m$ y $b$ que se obtengan por el método de los mínimos cuadrados, deberían ser muy próximos a los obtenidos directamente utilizando el método gráfico. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline $y_i [\,\,]$ & $x_i [\,\,]$ & $x_i^2 [\,\,]$& $x_i y_i [\,\,]$ \\
\hline \hline $y_1$ & $x_1$ & $x_1^2$ & $x_1 y_1$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline $y_n$ & $x_n$ & $x_n^2$ & $x_n y_n$ \\ \hline
\hline $\sum y_i$ &$\sum x_i $& $\sum x_i^2$ & $\sum x_i y_i$ \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar el uso del  método de mínimos cuadrados}
\label{tablamincua}
\end{table}

Pero en la actualidad es casi de rutina utilizar alguna herramienta computacional  que permite hacer los cálculos necesarios y los gráficos para el ajuste de la recta. Aunque el uso de este método no nos obliga a hacer el gráfico de la recta, por razones pedagógicas, es conveniente hacerlo para así observar más claramente las desviaciones de los puntos experimentales con respecto a la recta calculada. 

Una vez obtenido los valores de $m$ y ${b}$, es necesario calcular sus errores correspondientes $\Delta m$ y $\Delta b$. Esto lo podemos hacer calculando las desviaciones estándar  de la pendiente y la ordenada al origen, calculadas a partir de la distribución de diferencias $\Delta y_i$, ecuación (\ref{deltay}),  respecto de la mejor línea de ajuste.

Sea $\hat{y}_i=b+m x_i$ la predicción de $y$ basada en el valor $i$ de $x$. Entonces $e_i=y_i-\hat{y}_i$ representa el $i$ residuo - la diferencia entre el $i$ valor de respuesta observado y el $i$ valor de respuesta predicho por nuestro modelo lineal. Definimos la suma residual de cuadrados (RSS) como
$$
\mathrm{RSS}=e_1^2+e_2^2+\cdots+e_n^2
$$
o equivalentemente como
$$
\mathrm{RSS}=\left(y_1-b-m x_1\right)^2+\left(y_2-b-m x_2\right)^2+\cdots+\left(y_n-b-m x_n\right)^2 .
$$
\begin{table}[!t]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline$x_i [\,\,]$ & $y_i [\,\,]$ & $y_i-m x_i-b$ & $(y_i-m x_i-b)^2$ \\
\hline \hline$x_1$ & $y_1$ & $y_1-m x_1-b$ & $\left(y_1-m x_1-b\right)^2$ \\
\hline$x_2$ & $y_2$ & $y_2-m x_2-b$ & $\left(y_2-m x_2-b\right)^2$ \\
\hline$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\hline$x_n$ & $y_n$ & $y_n-m x_n-b$ & $\left(y_n-m x_n-b\right)^2$ \\ \hline
\hline & & & $\sum\limits_{i=1}^n \qquad \qquad \quad$ \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar los cálculos de $S_y$.}
\label{tablamincua2}
\end{table}

La diferencia entre el valor observado $y_i$ y el correspondiente valor ajustado $\hat{y}_i$ es un residuo. Matemáticamente, el residuo $i$ es
$$
e_i=y_i-\hat{y}_i=y_i-\left(b+m x_i\right), \quad i=1,2, \ldots, n
$$

Los residuos desempeñan un papel importante en la investigación de la adecuación del modelo y en la detección de desviaciones de los supuestos subyacentes. 

Sea SSE (Sum of Squared Errors) la suma de los cuadrados de los residuos.
$$
\mathrm{SSE} = \sum\limits_{i=1}^n e_i^2 = \sum\limits_{i=1}^n (y_i-\left(b+m x_i\right) )^2
$$

En el contexto del análisis de regresión por mínimos cuadrados, se define la cantidad $S_y$ como la desviación estándar de los errores, también conocida como el error estándar residual. Esta cantidad representa la raíz cuadrada de la suma de los cuadrados de los errores dividida por $n-2$,  matemáticamente es:
$$
S_y = \sqrt{\frac{\mathrm{SSE}}{n - 2}} 
$$
$S_y$ es una medida de la dispersión de los datos alrededor de la recta de regresión ajustada y se utilizada para calcular los errores estándar de la pendiente $\Delta m$
 y el término independiente $\Delta b$ de la regresión lineal.

\begin{eqnarray}
\Delta m &= \sqrt{\dfrac{n}{n \sum\limits_{i=1}^n x_i^2 - \left(\sum\limits_{i=1}^n x_i\right)^2}} \cdot S_y 
\label{Delm}\\ \nonumber  \\
\Delta b &= \sqrt{\dfrac{\sum\limits_{i=1}^n x_i^2}{n \sum\limits_{i=1}^n x_i^2 - \left(\sum\limits_{i=1}^n x_i\right)^2}} \cdot S_y
\label{Delb}
\end{eqnarray}


Finalmente para calcular $S_y$ se puede utilizar como ayuda la tabla \ref{tablamincua2}


\paragraph{Ejemplo 7.}

En un experimento sobre cinemática un grupo de estudiantes mide los tiempos con los que se desplaza  un móvil en un riel de aire. Los tiempos son medidos en segundos usando un cronómetro de sensibilidad $\Delta t=0,01$ s. Otro  instrumento  mide las velocidades con las que se desplaza el móvil, este instrumento tiene una sensibilidad de $\Delta v= 0,01$ m/s.  La tabla de datos y la respectiva gráfica se muestran la figura \ref{datosvt}.

\begin{figure}[h]
\centering
\begin{minipage}{0.49\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline $t_i \pm 0,01 (\mathrm{s})$ & $v_i \pm 0,01 (\mathrm{m/s})$  \\\hline \hline 
    \hline $0,00$ & 1.00 \\
    \hline $0,10$ & 1.64 \\
    \hline $0,20$ & 1.51 \\
    \hline $0,30$ & 2.03 \\
    \hline $0,40$ & 2.75 \\
    \hline $0,50$ & 3.59 \\
    \hline $0,60$ & 4.87 \\
    \hline $0,70$ & 5.23 \\
    \hline $0,80$ & 5.44 \\
    \hline $1,00$ & 6.37 \\
    \hline
    \end{tabular}
    \caption{Mediciones de la velocidad de un cuerpo en función del tiempo}
    \label{datosvt}
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[height=2.5in,width=2.5in]{figuras/fig12}
    \label{figdatos1}
\end{minipage}
\end{figure}

Como queremos  efectuar los cálculos manualmente es recomendable calcular las siguientes sumas:
$$
\begin{array}{c|c|c|c|c}
n&\sum_{i=1}^n t_i & \sum_{i=1}^n v_i   & \sum_{i=1}^n t_i^2  & \sum_{i=1}^n t_i v_i \\ \hline\hline 
10 & 4.60 & 34.43 & 3.04 & 21.275
\end{array}
$$
$$
\Delta=n\sum\limits_{i=1}^n t_i^2-\left(\sum\limits_{i=1}^n t_i\right)^2= 
10(3.04) - \left(4.60 \right)^2= 9.24 \,\, \mathrm{s}^2
$$
$$
b=\dfrac{\sum\limits_{i=1}^n t_i^2 \sum\limits_{i=1}^n v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n t_i v_i}{\Delta} =
\frac{(3.04)  (34.43) -(4.60) (21.275)}{9.24} = 0.7362 \,\, \mathrm{m/s}
$$
$$
m =\dfrac{n \sum\limits_{i=1}^n t_i v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n v_i}{\Delta} =\frac{10(21.275)  -(4.60) ( 34.43)}{9.24}= 5.8844\,\, \mathrm{m/s^2}
$$

Procedemos a calcular la suma de los cuadrados de los residuos
$$
\begin{array}{c|c}
n& \sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2  \\ \hline\hline 
10 & 1.244 
\end{array}
$$
La desviación estándar de $y$:
$$
S_y=\left[\frac{\sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2}{n-2}\right]^{\frac{1}{2}}=
\left(\frac{1.244}{8}\right)^{\frac{1}{2}}= 0.394 \,.
$$
y los errores 
$$
\begin{aligned}
\Delta m & =\left[\frac{n}{\Delta}\right]^{\frac{1}{2}} S_y = \left[\frac{10}{9.24}\right]^{\frac{1}{2}} ( 0.394 )=0.401  \,, \\
\Delta b & =\left[\frac{\sum\limits_{i=1}^n\left(t_i\right)^2}{\Delta}\right]^{\frac{1}{2}} S_y= 
\left[\frac{3.04}{9.24}\right]^{\frac{1}{2}} ( 0.394 )= 0.226 \,. 
\end{aligned}
$$

Por lo tanto nuestro resultado será, por los redondeos hechos en los cálculos
$$
v=mt + b= 5.91 t + 0.724 \,\, \mathrm{m/s}
$$
donde lo correcto sería escribir: $m=5.9 \pm 0.4$ y $b=0.7 \pm 0.2$.


En Python podemos hacer todos los casos anteriores, incluida la figura \ref{figdatos1}, pero primero debemos ingresar los datos
\begin{lstlisting}[language=Python]    
import numpy as np
import matplotlib.pyplot as plt
\end{lstlisting}
Los datos:
\begin{lstlisting}[language=Python]    
yn=  array([1.000, 1.64, 1.51, 2.03, 2.75, 3.59, 4.87, 5.23, 5.44, 6.37])
xn = array([0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 1.00])
\end{lstlisting}

La gráfica de la figura \ref{figdatos1} se obtiene a partir de las siguientes lineas de código:
\begin{lstlisting}[language=Python] 
plt.scatter(xn, yn, marker='*')
plt.title(r'Velocidad vs tiempo', fontsize=20)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
\end{lstlisting}


Para usar las ecuaciones (\ref{eme2})-(\ref{b2}) hagamos primero los siguientes cálculos intermedios
\begin{lstlisting}[language=Python] 
#Se obtiene el valor de n (numero de datos)
n=len(xn)
#Las sumatorias necesarias 
Sum_x = np.sum(xn)
Sum_y = np.sum(yn)
Sum_xx = np.sum(xn**2)
Sum_xy = np.sum(xn*yn)
Sum_xSumy = np.sum(xn)*np.sum(yn)
Delta = n*np.sum(xn**2) - (np.sum(xn))**2
print(n,',', Sum_x, ',',Sum_y,',', Sum_xx,',', Sum_xy,',', Sum_xSumy, ',',Delta)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
10 , 4.6 , 34.43 , 3.04 , 21.275, 158.378 , 9.240000000000002
}
\end{tcolorbox} 

Y finalmente calculamos $b$ y $m$
\begin{lstlisting}[language=Python] 
# Se escriben las ecuaciones para b y m 
b=(Sum_xx*Sum_y-Sum_xy*Sum_x)/(n*Sum_xx-Sum_x**2)
m=(n*Sum_xy-Sum_x*Sum_y)/(n*Sum_xx-Sum_x**2)
print('m=',m, ',', 'b=',b)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
m= 5.884415584415585 , b= 0.7361688311688325
}
\end{tcolorbox} 

Grafiquemos la recta y los datos
\begin{lstlisting}[language=Python] 
# La gráfica con los datos y la recta que mejor se ajusta 
y_pred=m_mc*xn + b_mc
plt.figure()
plt.scatter(xn, yn, color='b',marker='+', label='datos medidos')
plt.plot(xn, y_pred, 'r--',label='recta por mínimos cuadrados')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title(r'Velocidad vs tiempo', fontsize=18)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
plt.text(0.6, 1.0, '$y=(5.88) x + 0.736$', fontsize=12)
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.6in,width=3.0in]{figuras/fig13}  
\label{figdatos2}
\end{center}
\end{figure}

El siguiente paso es calcular $\Delta b$ y $\Delta m$, pero primero $S_y$

\begin{lstlisting}[language=Python] 
SSE= np.sum((yn -(b_mc + m_mc*xn))**2)
Sy= np.sqrt(SSE/(n-2))
print('n=',n,',','SSE=', SSE ,',', 'Sy=',Sy)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
n= 10 , SSE= 1.244265584415585 , Sy= 0.3943769745458628
}
\end{tcolorbox}

Los respectivos errores se obtienen de las ecuaciones (\ref{Delm}) y (\ref{Delb}) 

\begin{lstlisting}[language=Python] 
Delta_m = np.sqrt(n/(n*np.sum(xn ** 2) - np.sum(xn)**2))*Sy
Delta_b = np.sqrt(np.sum(xn**2)/(n*np.sum(xn**2)-np.sum(xn)**2))*Sy
print(f'm = {np.round(m_mc, 1)} \u00B1 {np.round(Delta_m, 1)}')
print(f'b = {np.round(b_mc, 1)} \u00B1 {np.round(Delta_b, 1)}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 \pm 0.4$

$b = 0.7 \pm 0.2$
}
\end{tcolorbox}

Por lo tanto:
$$
v= 5.9 t + 0.7 \,\, \mathrm{m/s} \,.
$$

Numpy tiene una función específica para calcular los parámetros $m$ y $b$, más información en \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html}

Veamos como funciona:

\begin{lstlisting}[language=Python] 
# Ajustar la recta por mínimos cuadrados usando linalg.lstsq
A = np.vstack([xn, np.ones(len(xn))]).T
m_c, b_c = np.linalg.lstsq(A, yn, rcond=None)[0]
print(f'm = {np.round(m_c, 1)}' )
print(f'b = {np.round(b_c, 1)}' )
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 $

$b = 0.7 $
}
\end{tcolorbox}


Actualmente, la función ``np.linalg.lstsq'' de Numpy no proporciona directamente los errores estándar de los coeficientes. Sin embargo, hay otras bibliotecas en Python que pueden hacer regresión lineal y proporcionar estos errores de manera más directa. Una de las bibliotecas más utilizadas para este propósito es ``statsmodels''. Para más información ver: \url{https://www.statsmodels.org/stable/index.html}

Veamos como funciona en nuestro ejemplo, primero cargamos la librería 
\begin{lstlisting}[language=Python] 
import statsmodels.api as sm
\end{lstlisting}

Luego ejecutamos las siguientes lineas de código para los cálculos

\begin{lstlisting}[language=Python] 
# Agregamos una constante (columna de unos) a xn
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
\end{lstlisting}

Con las siguientes lineas mostramos los resultados

\begin{lstlisting}[language=Python] 
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.8844 \pm 0.4103$

$b = 0.7362 \pm 0.2262$
}
\end{tcolorbox}

Y ahora la gráfica
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos originales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=2.6in,width=3.0in]{figuras/fig20}  
\label{figdatos2}
\end{center}
\end{figure}

