\chapter{Regresión lineal}

\section{Introducción}

Este capítulo lo dedicamos al  análisis de regresión, que es una técnica estadística utilizada para investigar y modelar la relación entre variables. Los métodos de análisis de regresión tienen una gran cantidad de aplicaciones en ingeniería, ciencias físicas y químicas, economía, ciencias de la vida y las ciencias sociales. Otro campo de aplicación es la ciencia y análisis de datos, esto hace que el espectro de problemas donde se aplica el análisis de regresión sea muy amplio.

El análisis de regresión fue desarrollado por primera vez por Sir Francis Galton\footnote{Francis Galton 1822-1911. \url{https://en.wikipedia.org/wiki/Francis_Galton}} a finales del siglo XIX. Galton redescubrió de forma independiente el concepto de correlación y demostró su aplicación en el estudio de la herencia, la antropología y la psicología. Galton desarrolló una descripción matemática de la tendencia regresiva, precursora de los modelos de regresión actuales y el término regresión sigue utilizándose para describir las relaciones estadísticas entre variables.

\section{Regresión y la construcción de modelos}
El análisis de regresión es una técnica estadística que permite investigar y construir modelos que establecen relaciones entre variables.

Las relaciones estadísticas son diferentes de las  relaciones funcionales porque las relaciones estadísticas no son perfectas, es decir, las observaciones de una relación estadística no caen directamente sobre una curva de relación. 

Supongamos que observamos una respuesta cuantitativa $y$ para  $k$ ``predictores'' diferentes: $x_1, x_2, \ldots, x_k$. Podemos suponer también que existe alguna relación entre $y$ y $x=\left(x_1, x_2, \ldots, x_k\right)$, que puede escribirse de la forma muy general
\begin{equation}
y=f(x)+\varepsilon \,.
\label{funcion}
\end{equation}
donde $f$ es alguna función fija pero desconocida de $x_1, \ldots, x_k$, y $\varepsilon$ es un término de error aleatorio, que es independiente de $x$ y tiene media cero. En esta formulación, $f$ representa la información sistemática que $x$ proporciona sobre $y$.

Existen dos razones principales para querer estimar $f$: la predicción y la inferencia. 

\begin{itemize}
\item Predicción:  Este proceso utiliza un modelo estadístico o de aprendizaje automático para predecir valores futuros basados en datos existentes. El objetivo principal es obtener valores precisos de la variable de interés, y la precisión de las predicciones es crucial. En este contexto, es común tener entradas $x$ disponibles, pero no la salida $y$. Podemos predecir $y$ usando
$$
\hat{y} = \hat{f}(x)\,,
$$
donde $\hat{f}$ es nuestra estimación de $f$, y $\hat{y}$ es la predicción para $y$. Consideramos $\hat{f}$ como una caja negra, no importando su forma exacta siempre que las predicciones sean precisas. Generalmente, $\hat{f}$ no será perfecta, introduciendo error que puede reducirse con mejores técnicas de aprendizaje estadístico. Incluso con una predicción perfecta $f$, $\hat{y} = f(x)$ aún tendría error debido a $\varepsilon$, de carácter aleatorio, conocido como error irreducible.

Métodos comunes para predicción incluyen regresión lineal, árboles de decisión, y redes neuronales, aplicables en situaciones como la predicción de precios en la bolsa o el valor de un producto bajo ciertas condiciones.

\item Inferencia:  El objetivo no es necesariamente predecir $y$ al estimar $f$, sino conocer $f$ de manera exacta. Se trata de usar un modelo estadístico para entender la relación entre variables y sacar conclusiones sobre la población de la cual se extrajo la muestra. El objetivo principal es realizar afirmaciones sobre los parámetros del modelo y la naturaleza de las relaciones entre las variables. Podríamos preguntarnos si existe una relación entre $y$ y cada predictor en forma de una ecuación lineal, o si es más compleja. Históricamente, la mayoría de los métodos para estimar $f$ han adoptado una forma lineal, lo cual es a veces razonable o deseable. Sin embargo, a menudo la relación verdadera es más complicada, y un modelo lineal puede no representar con precisión la relación entre las variables.

Métodos comunes en inferencia incluyen pruebas de hipótesis, estimación de intervalos de confianza, análisis de varianza (ANOVA), y regresión lineal múltiple. Estas técnicas pueden usarse para inferir si existe una relación entre el consumo de café y el riesgo de padecer diabetes.

\end{itemize}


Es muy probable encontrar situaciones que se encuadren en el ámbito de la predicción, en el de la inferencia, o en una combinación de ambos. Por ejemplo, los modelos lineales permiten una inferencia relativamente sencilla e interpretable, pero pueden no producir predicciones tan precisas como otros enfoques. Por el contrario, algunos de los enfoques no lineales pueden proporcionar predicciones bastante precisas para $y$, pero a costa de un modelo menos interpretable, haciendo la inferencia más difícil.


Uno de los métodos más utilizados para estimar $f$ es el ``método paramétrico'', que comienza con una suposición sobre la forma funcional de $f$. Por ejemplo, una suposición simple es que $f$ es lineal en $x$:
\begin{equation}
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,, 
\label{modlineal}
\end{equation}
Esto simplifica enormemente el problema de estimar $f$, ya que solo hay que estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_k$. Una vez seleccionado un modelo, necesitamos un procedimiento para ajustar o entrenar el modelo con los datos. En el caso del modelo lineal (\ref{modlineal}), queremos estimar los parámetros \( \beta_0, \beta_1, \ldots, \beta_k \), encontrando valores de estos parámetros tales que
$$
y \approx \beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,.
$$
El enfoque más común para ajustar el modelo lineal (\ref{modlineal}) es el de los mínimos cuadrados ordinarios, aunque existen otros métodos para ajustar el modelo.


De alguna manera, los modelos de regresión pueden considerarse como modelos empíricos. Si la relación entre las variables es muy compleja se puede utilizar regresiones lineales a trozos para aproximar la relación verdadera entre las variables $y$ y $x$. En este caso las ecuaciones de regresión sólo son válidas en la región de las variables regresoras contenidas en los datos observados. 

De manera general, la variable de respuesta $y$ puede estar relacionada con un número $k$ regresores, $x_1, x_2, . . . , x_k$, de forma que
\begin{equation}
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k+\varepsilon
\label{reglin3}
\end{equation}

La ecuación (\ref{reglin3}) se denomina Modelo de Regresión Lineal Múltiple, y el adjetivo lineal se emplea para indicar que el modelo es lineal en los parámetros $\beta_0, \beta_1, \ldots, \beta_k$, no porque $y$ sea una función lineal de las $x$. Esto significa que muchos modelos en los que $y$ se relaciona con las $x$ de forma no lineal pueden seguir tratándose como modelos de regresión lineal siempre que la ecuación sea lineal en los $\beta$ 's.

Una fase crucial del análisis de regresión es comprobar la adecuación del modelo, evaluando su idoneidad y calidad del ajuste. Este análisis determina la utilidad del modelo y puede indicar si necesita ser modificado, haciendo del análisis de regresión un proceso iterativo guiado por los datos.

Es importante destacar que un modelo de regresión no implica causalidad entre variables. Una fuerte relación empírica no prueba una relación causal entre regresores y respuesta. Para establecer causalidad, se necesita más que los datos de la muestra, como consideraciones teóricas. El análisis de regresión puede confirmar una relación causal, pero no debe ser la única base para tal afirmación.

El análisis de regresión es parte de un enfoque más amplio para resolver problemas. La ecuación de regresión puede no ser el objetivo principal del estudio; a menudo es más importante entender el sistema que genera los datos.

En el trabajo de laboratorio, la recolección de datos es crucial. Un análisis de regresión es tan bueno como los datos en que se basa. Los problemas de estimación de parámetros se resuelven mediante regresión para predecir la variable de respuesta, pero al extrapolar, se corren riesgos significativos debido a errores del modelo. Una estimación incorrecta de parámetros puede llevar a predicciones deficientes. Es esencial tener en cuenta la precisión del modelo y la exactitud en la estimación de parámetros.

Los modelos de regresión pueden usarse con fines de control, donde las variables deben estar causalmente relacionadas. Sin embargo, para predicción, no es estrictamente necesario que exista una relación causal. Basta con que las relaciones en los datos originales sigan siendo válidas. Por ejemplo, el consumo diario de electricidad en agosto puede predecir la temperatura máxima diaria, pero intentar reducir la temperatura disminuyendo el consumo de electricidad fracasaría, ya que la relación no es causal.



\section{Los algoritmos en los modelos de regresión}

Desde el punto de vista computacional, construir un modelo de regresión es un proceso iterativo. Los datos de entrada incluyen conocimientos teóricos y datos disponibles para especificar un modelo inicial. Las representaciones gráficas son útiles para esta especificación. A continuación, se estiman los parámetros del modelo, generalmente con el método de mínimos cuadrados. Luego, se evalúa la adecuación del modelo, identificando posibles errores de especificación, omisión de variables importantes, inclusión de variables innecesarias o presencia de datos inusuales. Si el modelo es inadecuado, se ajusta y se vuelven a estimar los parámetros. Este proceso se repite hasta obtener un modelo adecuado, que finalmente debe ser validado para asegurar resultados aceptables.

El análisis de regresión requiere un uso hábil del computador. Un buen programa de cálculo de regresión es esencial, pero la aplicación rutinaria de programas estándar no siempre da resultados satisfactorios. El computador no sustituye el pensamiento creativo sobre el problema. Debemos aprender a interpretar la información que proporciona y usarla en modelos posteriores. Entre el software estadístico disponible, se destaca el SAS (Statistical Analysis System), un paquete comercial para análisis de datos con una amplia gama de procedimientos estadísticos, y R, un lenguaje de código abierto y gratuito, conocido por su flexibilidad e innovación.


\section{Regresión lineal simple}

Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i \,.
\label{regrelineal}
\end{equation}

La intersección $\beta_0$  y la pendiente $\beta_1$  son constantes desconocidas, mientras que $\varepsilon$ es un componente de error aleatorio. Suponemos que los errores tienen media cero y varianza desconocida $\sigma^2$. También asumimos que los errores no están correlacionados, lo que significa que el valor de un error no depende del valor de ningún otro error. Es importante considerar el regresor $x$
como controlado por el analista de datos o experimentador y medido con un error despreciable, mientras que la respuesta $y$ es una variable aleatoria. Esto implica que existe una distribución de probabilidad para $y$  en cada valor posible de $x$.

\paragraph{Características importantes de este modelo}
\begin{enumerate}
\item La respuesta $y_i$  es la suma de dos componentes: (1) el término constante $\beta_0+\beta_1 x_i$ y (2) el término aleatorio $\varepsilon_i$. Por lo tanto, $y_i$ es una variable aleatoria.

\item Como $E\left\{\varepsilon_i\right\}=0$, se puede demostrar que la media de esta distribución es:
$$
E(y \mid x)=E\left\{y_i\right\}=E\left\{\beta_0+\beta_1x_i+\varepsilon_i\right\}=\beta_0+\beta_1 x_i+E\left\{\varepsilon_i\right\}=\beta_0+\beta_1 x_i
$$
Así, la respuesta $y_i$ para  $x_i$, procede de una distribución de probabilidad cuya media es:
$$
E\left\{y_i\right\}=\beta_0+\beta_1 x_i
$$
Por tanto, sabemos que la función de regresión para el modelo (\ref{regrelineal}) es:
$$
E\{y\}=\beta_0+\beta_1 x
$$
ya que la función de regresión relaciona las medias de las distribuciones de probabilidad de $y$ para $x$.

\item La respuesta $y_i$ supera o no alcanza el valor de la función de regresión por la cantidad del término de error $\varepsilon_i$.

\item  Se supone que los términos de error $\varepsilon_i$ tienen una varianza constante $\sigma^2$. Por lo tanto, se deduce que las respuestas $y_i$ tienen la misma varianza constante:
$$
\sigma^2\left\{y_i\right\}=\sigma^2
$$
Se puede demostrar también que:
$$
\sigma^2\left\{\beta_0+\beta_1 x_i+\varepsilon_i\right\}=\sigma^2\left\{\varepsilon_i\right\}=\sigma^2
$$
Así, el modelo de regresión (\ref{regrelineal}) asume que las distribuciones de probabilidad de $y$ tienen la misma varianza $\sigma^2$, independientemente del nivel de la variable predictora $x$.

\item  Se supone que los términos de error no están correlacionados. Dado que los términos de error $\varepsilon_i$ y $\varepsilon_j$ no están correlacionados, tampoco lo están las respuestas $y_i$ y $y_j$.


\end{enumerate}

\paragraph{Significado de los parámetros de regresión}

Los parámetros $\beta_0$ y $\beta_1$ del modelo de regresión (\ref{regrelineal}) se denominan coeficientes de regresión. $\beta_1$ es la pendiente de la recta de regresión. Indica el cambio en la media de la distribución de probabilidad de $y$ por unidad de aumento en $x$. El parámetro $\beta_0$ es la intercepción $y$ de la recta de regresión. Cuando el ámbito del modelo incluye $x=0$, $\beta_0$ da la media de la distribución de probabilidad de $y$ en $x=0$. Cuando el ámbito del modelo no incluye $x=0, \beta_0$ no tiene ningún significado particular como término separado en el modelo de regresión.

Supongamos que en una universidad se estudió la relación entre el tiempo de estudio (en horas) y las calificaciones de 25 estudiantes en el examen final de matemáticas aplicadas. El gráfico de dispersión en la figura \ref{regres1} muestra que a mayor tiempo de estudio, las notas tienden a aumentar, sugiriendo una relación lineal aproximada entre las variables. 
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.0in]{figuras/fig18}  \quad 
\includegraphics[height=3.0in,width=3.0in]{figuras/fig19}  
\caption{Izquierda: gráfico de dispersión para las horas de estudio y las notas obtenidas. Derecha: la  generación de observaciones en la regresión lineal.}
\label{regres1}
\end{center}
\end{figure}

Podemos suponer una relación de la forma
\begin{equation}
y = \beta_0 + \beta_1 x \,,
\label{reglin1}
\end{equation}
donde $y$ son las notas, $x$ es el tiempo de estudio, $\beta_0$ es la intersección y $\beta_1$ es la pendiente de la recta.

Dado que los puntos no caen exactamente sobre la línea recta, la ecuación (\ref{reglin1}) se ajusta añadiendo un término de error:
\begin{equation}
y = \beta_0 + \beta_1 x + \varepsilon \,,
\label{reglin2}
\end{equation}
donde $\varepsilon$ representa la variación aleatoria en las notas. Este modelo más realista considera factores no medidos y errores, denominándose Modelo de Regresión Lineal (MRL). Aquí, $x$ es la variable ``predictora'' o ``regresora'' y $y$ la variable de respuesta. Al incluir solo una variable regresora, este modelo se llama Modelo de Regresión Lineal Simple (MRLS).


Podemos analizar el MRL fijando el valor de $x$ y observando $y$. Si la media y varianza de $\varepsilon$ son 0 y $\sigma^2$, respectivamente, la respuesta media para cualquier valor de $x$ es:
\begin{equation}
\mu_{y \mid x} \equiv E(y \mid x)=E\left(\beta_0+\beta_1 x+\varepsilon\right)=\beta_0+\beta_1 x \,,
\end{equation}
igual a la relación (\ref{reglin1}). La varianza de $y$ dado $x$ es:
\begin{equation}
\sigma_{y \mid x}^2 \equiv \operatorname{Var}(y \mid x)=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right)=\sigma^2 \,.
\end{equation}

El verdadero modelo de regresión $\mu_{y \mid x}=\beta_0+\beta_1 x$ es una línea de valores medios, donde $\beta_1$ indica el cambio en la media de $y$ por un cambio unitario en $x$. La variabilidad de $y$ en cada $x$ está dada por $\sigma^2$, implicando que los valores de $y$ tienen la misma varianza en cada $x$.

Si el modelo de regresión es $\mu_{y \mid x}=3.5+2.0 x$ y $\sigma^2=2$ el resultado se ve en la figura derecha de  \ref{regres1}. Utilizando una distribución normal para $\varepsilon$, $y$ también es normal. Para $x=10$ horas, $y$ tiene como media $3.5+2(10)=23.5$ y varianza 2. $\sigma^2$ determina la variabilidad en $y$ sobre el tiempo de estudio: un $\sigma^2$ pequeño indica valores de $y$ cerca de la recta, mientras que un $\sigma^2$ grande indica mayores desviaciones.

Notemos que pudimos haber hecho un estudio similar pero con ocho estudiantes del curso,  que elegiríamos de manera aleatoria, para entonces proceder a recopilar los datos sobre el tiempo de estudio y las calificaciones de estos estudiantes. En este caso estaríamos  haciendo lo que se llama un estudio de regresión muestral. 


Recordemos que el fin principal en el análisis de regresión es estimar los parámetros desconocidos del modelo de regresión. Este proceso también se denomina ajuste del modelo a los datos. Existen varias técnicas de estimación de parámetros y una de estas técnicas es el método de los mínimos cuadrados. En el ejemplo que hemos mencionado  de las horas de estudio y las notas obtenidas por los estudiantes nos llevaría a una ecuación como la que se muestra a continuación
$$
\hat{y}=3.687+ 1.821 x \,,
$$
donde $\hat{y}$  es el valor ajustado o estimado de las notas correspondiente a un valor del número de horas $x$. Esta ecuación ajustada es la que se  representa en la parte izquierda de la figura \ref{regres1}. Note que un estudiante que estudia cero horas $x=0$ obtendría una nota de $3.7$ de 100. 


\subsection{Método de los mínimos cuadrados}

El método de los mínimos cuadrados es uno de los métodos estadísticos más usados para determinar la recta que mejor represente la tendencia de un conjunto de puntos experimentales. Como mencionamos anteriormente, los parámetros $\beta_0$ y $\beta_1$ son desconocidos y deben estimarse utilizando datos muestrales. Supongamos que tenemos $n$ pares de datos, digamos $\left(y_1, x_1\right),\left(y_2, x_2\right), \ldots,\left(y_n, x_n\right)$. Estos datos pueden proceder de un experimento controlado diseñado específicamente para recopilar los datos, de un estudio observacional o de registros históricos existentes (un estudio retrospectivo).

Para estimar $\beta_0$ y $\beta_1$ utilizaremos el método de los mínimos cuadrados, esto es, estimamos $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. A partir de (\ref{regrelineal}) podemos escribir
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{regrelineal}) puede verse como un modelo de regresión poblacional, mientras que la ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. Así, el criterio de mínimos cuadrados es
\begin{equation}
S\left(\beta_0, \beta_1\right)=\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 x_i\right)^2
\end{equation}

Si la dispersión de los puntos experimentales es debida solo a los errores casuales en las mediciones, la mejor recta será aquella para la cual la suma de los cuadrados de las distancias $\left(y_i-y_0 \right)$ sea un mínimo. Es por esto que, a este método se le llama método de los mínimos cuadrados.


A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador. Como se ha indicado anteriormente, los valores de esas magnitudes tendrán sus correspondientes errores que determinaremos más adelante. 

La desviación de un valor cualquiera $y_i$ determinado experimentalmente con respecto a su valor $y_0$ en la recta, será:
\begin{equation}
\Delta y_i=y_i-y_0=y_i-\left(b+m x_i\right)
\label{deltay}
\end{equation}

Ahora se puede enunciar el principio básico de este método, el cual dice que la mejor recta que puede ser trazada entre esos puntos, es aquella para la cual la suma de los cuadrados de las desviaciones $\Delta y_i$ de los datos experimentales, con respecto a esa recta, es mínima.
$$
\sum_{i=1}^n\left(\Delta y_i\right)^2=\sum_{i=1}^n \left[y_i-b-m x_i \right]^2
$$
donde $n$ es el número de pares de valores de $y$ y $x$.

Ya que la condición exigida es la de minimizar la suma anterior, entonces los parámetros $m$ y $b$ deben ajustarse para cumplir con esta condición. Ello se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.
$$
\begin{aligned}
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial b}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial b} = 
\sum_{i=1}^n -2(y_i-b-m x_i)=0  \\
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial m}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial m} =
\sum_{i=1}^n -2x_i(y_i-b-m x_i)=0
&
\end{aligned}
$$

Por lo tanto, se debe resolver el sistema 
$$
\begin{aligned}
&  \sum_{i=1}^n y_i - \sum_{i=1}^nb - \sum_{i=1}^n m x_i = 0  &\,\, \Rightarrow \,\,&
 nb + m \sum_{i=1}^n  x_i = \sum_{i=1}^n y_i  \\
&  \sum_{i=1}^n y_ix_i - \sum_{i=1}^n bx_i -\sum_{i=1}^n  m x_i^2 =0 & \,\, \Rightarrow \,\,&
b \sum_{i=1}^n x_i  + m \sum_{i=1}^n   x_i^2 = \sum_{i=1}^n y_ix_i
\end{aligned}
$$
para $m$ y $b$. 

Utilizando la regla de Cramer y definiendo $\Delta$ como se muestra a continuación:
$$
\begin{aligned}
\Delta= 
{\left|\begin{array}{cc}
n & \sum x_i \\
\sum x_i & \sum x_i^2
\end{array}\right|} = n \sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^nx_i\right)^2 \,,
\end{aligned}
$$
resulta:
$$
m=\frac{\left|\begin{array}{cc}
n & \sum y_i \\
\sum x_i & \sum x_i y_i
\end{array}\right|}{\Delta}=\frac{n\sum x_i y_i-\sum x_i \sum y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} \quad \text{y} \quad
 b=\frac{\left|\begin{array}{cc}
\sum y_i & \sum x_i \\
\sum x_i y_i & \sum x_i^2 
\end{array}\right|}{\Delta}=\frac{\sum x_i^2 \sum y_i-\sum x_i \sum x_i y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} 
$$

Por lo tanto:
\begin{eqnarray}
m &=&\dfrac{ n\sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{\Delta} = 
\dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} 
\label{eme2} \\
b &=&\dfrac{\sum\limits_{i=1}^n x_i^2 \sum\limits_{i=1}^n y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n x_i y_i}{\Delta} =  \bar{y}-m \bar{x} \,,
\label{b2}
\end{eqnarray}

Se puede  escribir (\ref{eme2}) como
\begin{equation}
m=\frac{S_{xy}}{S_x^2} \,.
\end{equation}

En donde $\bar{x}$ e $\bar{y}$ denotan las medias muestrales de $x$ y $y$ (respectivamente), 
$$
\bar{x}=\frac{\sum_{i=1}^n x_i}{n}, \, \quad \bar{y}=\frac{\sum_{i=1}^n y_i}{n}\,,
$$

$S_x^2$ es la varianza muestral de $x$ y $S_{xy}$ es la covarianza muestral entre $x$ y $y$. Estos parámetros se calculan como:
$$
S_x^2=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}{n}, \, S_y^2=\frac{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}{n}, \, S_{xy}=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{n} .
$$

La cantidad $m$ se denomina coeficiente de regresión de $y$ sobre $x$, lo denotamos por $m_{y/x}$.




En el trabajo de laboratorio podríamos construir una tabla como la mostrada en \ref{tablamincua}, para así ordenar la información y facilitar los cálculos. Las cuatro sumas en la última línea, son los valores necesarios para calcular $m$ y $b$. Los valores de $m$ y $b$ que se obtengan por el método de los mínimos cuadrados, deberían ser muy próximos a los obtenidos directamente utilizando el método gráfico. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline $y_i [\,\,]$ & $x_i [\,\,]$ & $x_i^2 [\,\,]$& $x_i y_i [\,\,]$ & $y_i-m x_i-b$ & $(y_i-m x_i-b)^2$  \\
\hline \hline $y_1$ & $x_1$ & $x_1^2$ & $x_1 y_1$ & $y_1-m x_1-b$ & $\left(y_1-m x_1-b\right)^2$\\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &  $\vdots$ & $\vdots$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &  $\vdots$ & $\vdots$ \\
\hline $y_n$ & $x_n$ & $x_n^2$ & $x_n y_n$ & $y_n-m x_n-b$ &$\left(y_n-m x_n-b\right)^2$ \\ \hline
\hline $\sum y_i$ &$\sum x_i $& $\sum x_i^2$ & $\sum x_i y_i$ & & $\sum\limits_{i=1}^n (y_i-m x_i-b)^2$  \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar el uso del  método de mínimos cuadrados}
\label{tablamincua}
\end{table}

Pero en la actualidad es casi de rutina utilizar alguna herramienta computacional  que permite hacer los cálculos necesarios y los gráficos para el ajuste de la recta. Aunque el uso de este método no nos obliga a hacer el gráfico de la recta, por razones pedagógicas, es conveniente hacerlo para así observar más claramente las desviaciones de los puntos experimentales con respecto a la recta calculada. 

Una vez obtenido los valores de $m$ y ${b}$, es necesario calcular sus errores correspondientes $\Delta m$ y $\Delta b$. Esto lo podemos hacer calculando las desviaciones estándar  de la pendiente y la ordenada al origen, calculadas a partir de la distribución de diferencias $\Delta y_i$, ecuación (\ref{deltay}),  respecto de la mejor línea de ajuste.

Sea $\hat{y}_i=b+m x_i$ la predicción de $y$ basada en el valor $i$ de $x$. Entonces $e_i=y_i-\hat{y}_i$ representa el $i$ residuo - la diferencia entre el $i$ valor de respuesta observado y el $i$ valor de respuesta predicho por nuestro modelo lineal. Definimos la suma residual de cuadrados (RSS) como
$$
\mathrm{RSS}=e_1^2+e_2^2+\cdots+e_n^2
$$
o equivalentemente como
$$
\mathrm{RSS}=\left(y_1-b-m x_1\right)^2+\left(y_2-b-m x_2\right)^2+\cdots+\left(y_n-b-m x_n\right)^2 .
$$


La diferencia entre el valor observado $y_i$ y el correspondiente valor ajustado $\hat{y}_i$ es un residuo. Matemáticamente, el residuo $i$ es
$$
e_i=y_i-\hat{y}_i=y_i-\left(b+m x_i\right), \quad i=1,2, \ldots, n
$$

Los residuos desempeñan un papel importante en la investigación de la adecuación del modelo y en la detección de desviaciones de los supuestos subyacentes. 

Sea SSE (Sum of Squared Errors) la suma de los cuadrados de los residuos.
$$
\mathrm{SSE} = \sum\limits_{i=1}^n e_i^2 = \sum\limits_{i=1}^n (y_i-\left(b+m x_i\right) )^2
$$
y representa la parte de la variabilidad de $y$  que no se puede explicar por el modelo de regresión.

También se suelen definir las siguientes cantidades: la variabilidad Explicada (Sum of Squares for Regression, SSR) que representa la parte de la variabilidad de $y$ que se explica por el modelo de regresión.
$$
\mathrm{SSR}=\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 \,.
$$

Y la variabilidad Total (Total Sum of Squares, SST)que  representa la variabilidad total de la variable dependiente $y$ respecto a su media.
$$
\mathrm{SST}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 \,.
$$

La variabilidad total se puede descomponer en la variabilidad explicada por el modelo y la variabilidad residual
$$
\mathrm{SST}= \mathrm{SSR} + \mathrm{SSE} \,\, \Rightarrow \,\,
\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 =
\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 + 
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
$$


Podemos calcular  el error estándar residual y  los errores estándar de la pendiente $\Delta m$ y el término independiente $\Delta b$ de la regresión lineal de la siguiente manera:
\begin{eqnarray}
\mathrm{SE}\left(m\right)^2 &=& \Delta m=\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \,, 
\label{Delm}  \\
\mathrm{SE}\left(b\right)^2 &= &\Delta b=\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right]  \,,
\label{Delb}
\end{eqnarray}
donde 
$$
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2} 
$$




%example
\begin{example}
\label{ejemvelocidadtiempo}
En un experimento sobre cinemática un grupo de estudiantes mide los tiempos con los que se desplaza  un móvil en un riel de aire. Los tiempos son medidos en segundos usando un cronómetro de sensibilidad $\Delta t=0,01$ s. Otro  instrumento  mide las velocidades con las que se desplaza el móvil, este instrumento tiene una sensibilidad de $\Delta v= 0,01$ m/s.  La tabla de datos y la respectiva gráfica se muestran la figura \ref{datosvt}.
\begin{figure}[h]
\centering
\begin{minipage}{0.54\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline $t_i \pm 0,01 (\mathrm{s})$ & $v_i \pm 0,01 (\mathrm{m/s})$  \\\hline \hline 
    \hline $0,00$ & 1.00 \\
    \hline $0,10$ & 1.64 \\
    \hline $0,20$ & 1.51 \\
    \hline $0,30$ & 2.03 \\
    \hline $0,40$ & 2.75 \\
    \hline $0,50$ & 3.59 \\
    \hline $0,60$ & 4.87 \\
    \hline $0,70$ & 5.23 \\
    \hline $0,80$ & 5.44 \\
    \hline $1,00$ & 6.37 \\
    \hline
    \end{tabular}
    \caption{Mediciones de la velocidad de un cuerpo en función del tiempo y la gráfica de dispersión respectiva.}
    \label{datosvt}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
    \centering
    \includegraphics[height=2.6in,width=2.6in]{figuras/fig12}
\end{minipage}
\end{figure}

Como queremos  efectuar los cálculos manualmente es recomendable calcular las siguientes sumas:
$$
\begin{array}{c|c|c|c|c}
n&\sum_{i=1}^n t_i & \sum_{i=1}^n v_i   & \sum_{i=1}^n t_i^2  & \sum_{i=1}^n t_i v_i \\ \hline\hline 
10 & 4.60 & 34.43 & 3.04 & 21.275
\end{array}
$$
$$
\Delta=n\sum\limits_{i=1}^n t_i^2-\left(\sum\limits_{i=1}^n t_i\right)^2= 
10(3.04) - \left(4.60 \right)^2= 9.24 \,\, \mathrm{s}^2
$$
$$
b=\dfrac{\sum\limits_{i=1}^n t_i^2 \sum\limits_{i=1}^n v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n t_i v_i}{\Delta} =
\frac{(3.04)  (34.43) -(4.60) (21.275)}{9.24} = 0.7362 \,\, \mathrm{m/s}
$$
$$
m =\dfrac{n \sum\limits_{i=1}^n t_i v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n v_i}{\Delta} =\frac{10(21.275)  -(4.60) ( 34.43)}{9.24}= 5.8844\,\, \mathrm{m/s^2}
$$

Procedemos a calcular la suma de los cuadrados de los residuos
$$
\begin{array}{c|c}
n& \sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2  \\ \hline\hline 
10 & 1.244  
\end{array} 
$$
Por lo tanto 
$$
\hat{\sigma}=\left[\frac{\mathrm{SSE}}{n-2}\right]^{\frac{1}{2}}=
\left(\frac{1.244}{8}\right)^{\frac{1}{2}}= 0.394 \,.
$$


y los errores a partir de las ecuaciones (\ref{Delm}) y (\ref{Delb})
$$
\begin{aligned}
\Delta m & =0.401  \,, \\
\Delta b & = 0.226 \,. 
\end{aligned}
$$

Por lo tanto nuestro resultado será, por los redondeos hechos en los cálculos
$$
v=mt + b= 5.91 t + 0.724 \,\, \mathrm{m/s}
$$
donde lo correcto sería escribir: $m=5.9 \pm 0.4$ y $b=0.7 \pm 0.2$.


Con Python podemos hacer todos los cálculos anteriores, incluida la figura  \ref{datosvt}. Aquí vamos a  transcribir la ecuaciones las fórmulas utilizadas con anterioridad a manera de entender el proceso de los mínimos cuadrados. Luego veremos que existen otras opciones más directas. 

Primero que todo, debemos llamar las librerías:

\begin{lstlisting}[language=Python]    
import numpy as np
import matplotlib.pyplot as plt
\end{lstlisting}
\vspace{5mm}

Los datos deben escribirse en formas de arreglos:

\begin{lstlisting}[language=Python]    
yn=  array([1.000, 1.64, 1.51, 2.03, 2.75, 3.59, 4.87, 5.23, 5.44, 6.37])
xn = array([0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 1.00])
\end{lstlisting}

La gráfica de la figura \ref{datosvt} se obtiene a partir de las siguientes lineas de código:
\begin{lstlisting}[language=Python] 
plt.scatter(xn, yn, marker='*')
plt.title(r'Velocidad vs tiempo', fontsize=20)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig12}  
\label{figdatos2}
\end{center}
\end{figure}

Para usar las ecuaciones (\ref{eme2})-(\ref{b2}) se necesita hacer una serie de  cálculos intermedios, como se muestra a continuación:
\begin{lstlisting}[language=Python] 
#Se obtiene el valor de n (numero de datos)
n=len(xn)
#Las sumatorias necesarias 
Sum_x = np.sum(xn)
Sum_y = np.sum(yn)
Sum_xx = np.sum(xn**2)
Sum_xy = np.sum(xn*yn)
Sum_xSumy = np.sum(xn)*np.sum(yn)
Delta = n*np.sum(xn**2) - (np.sum(xn))**2
print(n,',', Sum_x, ',',Sum_y,',', Sum_xx,',', Sum_xy,',', Sum_xSumy, ',',Delta)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
10 , 4.6 , 34.43 , 3.04 , 21.275, 158.378 , 9.240000000000002
}
\end{tcolorbox} 

Y finalmente calculamos $b$ y $m$

\begin{lstlisting}[language=Python] 
# Se escriben las ecuaciones para b y m 
m_mc = (n * Sum_xy - Sum_x * Sum_y) / Delta
b_mc = Sum_y /n - m_mc * Sum_x/n
print('m=',m_mc, ',', 'b=',b_mc)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
m= 5.884415584415585 , b= 0.7361688311688325
}
\end{tcolorbox} 

En este bloque de comandos lo que se quiere es hacer una gráfica que contenga la gráfica de dispersión y la recta que mejor ajusta los datos

\begin{lstlisting}[language=Python] 
# La gráfica con los datos y la recta que mejor se ajusta 
y_hat= m_mc*xn + b_mc
# 
plt.figure()
plt.scatter(xn, yn, color='b',marker='+', label='datos medidos')
plt.plot(xn, y_hat, 'r--',label='recta por mínimos cuadrados')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title(r'Velocidad vs tiempo', fontsize=18)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
plt.text(0.6, 1.0, '$y=(5.88) x + 0.736$', fontsize=12)
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig13}  
\label{figdatos2}
\end{center}
\end{figure}

El siguiente paso es calcular $\Delta b$ y $\Delta m$, pero primero $\hat{\sigma}$

\begin{lstlisting}[language=Python] 
SSE= np.sum((yn - y_hat)**2)
s_hat= np.sqrt(SSE/(n-2))
print('n=',n,',','SSE=', SSE ,',', 'sigma=',s_hat)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
n= 10 , SSE= 1.244265584415585 ,  $\sigma$= 0.3943769745458628
}
\end{tcolorbox}

Los respectivos errores se obtienen de las ecuaciones (\ref{Delm}) y (\ref{Delb}) 

\begin{lstlisting}[language=Python] 
# Los promedios de x y y 
xp= np.sum(xn)/n
yp= np.sum(yn)/n
Delta_m = np.sqrt(s_hat**2 / np.sum((xn - xp)**2))
Delta_b= np.sqrt(s_hat**2 * (1/n + xp**2 / np.sum((xn - xp)**2)))
print(f'm = {np.round(m_mc, 1)} \u00B1 {np.round(Delta_m, 1)}')
print(f'b = {np.round(b_mc, 1)} \u00B1 {np.round(Delta_b, 1)}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 \pm 0.4$

$b = 0.7 \pm 0.2$
}
\end{tcolorbox}

Por lo tanto:
$$
v= 5.9 t + 0.7 \,\, \mathrm{m/s} \,.
$$

Numpy tiene una función específica para calcular los parámetros $m$ y $b$, que se llama ''linalg.lstsq". Para mayor información se puede consultar: 

 \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html}

La librería ''linalg.lstsq" resuelve para  el vector x la ecuación matricial $a.x = b$  utilizando el método de los mínimos cuadrados. Esta función se utiliza comúnmente en una variedad de aplicaciones, como el análisis de regresión, el ajuste de curvas y otras tareas de aprendizaje automático.

Veamos como funciona:

\begin{lstlisting}[language=Python] 
# Ajustar la recta por mínimos cuadrados usando linalg.lstsq
A = np.vstack([xn, np.ones(len(xn))]).T
m_c, b_c = np.linalg.lstsq(A, yn, rcond=None)[0]
#
print(f'm = {np.round(m_c, 1)}' )
print(f'b = {np.round(b_c, 1)}' )
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 $

$b = 0.7 $
}
\end{tcolorbox}


Actualmente, la función ``np.linalg.lstsq'' de Numpy no proporciona directamente los errores estándar de los coeficientes. Sin embargo, hay otras bibliotecas en Python que pueden hacer regresión lineal y proporcionar estos errores de manera más directa. 

Una de las bibliotecas más utilizadas para este propósito es ``statsmodels''. Para más información ver: 

\url{https://www.statsmodels.org/stable/index.html}

Para estudiar como funciona en el ejemplo que estamos considerando lo primero que hay que hacer es  cargar la librería 

\begin{lstlisting}[language=Python] 
# Importamos la librería 
import statsmodels.api as sm
\end{lstlisting}

Luego ejecutamos las siguientes lineas de código para los cálculos

\begin{lstlisting}[language=Python] 
# Agregamos una constante (columna de unos) a xn
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
\end{lstlisting}

Con las siguientes lineas mostramos los resultados

\begin{lstlisting}[language=Python] 
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.8844 \pm 0.4103$

$b = 0.7362 \pm 0.2262$
}
\end{tcolorbox}

Y ahora la gráfica
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos originales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.xlabel('$t$ [s]')
plt.ylabel('$v$ [m/s]')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig20}  
\label{figdatos2}
\end{center}
\end{figure}


Se puede pedir que se genere en consola un resumen del modelo 

\begin{lstlisting}[language=Python] 
results = model
print(results.summary
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=4.6in]{figuras/fig20a}  
\label{figdatos2}
\end{center}
\end{figure}

Se pueden identificar fácilmente los valores de los parámetros y sus errores estándar que relatan la precisión de las estimaciones obtenidas. De ``R-squared: 0.963'' hablaremos más adelante, pero este valor indica qué tan bien los datos se ajustan al modelo de regresión (0 a 1, donde valores más cercanos a 1 indican un mejor ajuste).
\end{example}
\exampleline

\subsection{Propiedades del método de mínimos cuadrados}

Podemos notar de la ecuación (\ref{eme2})
$$
m  = \dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} =\sum_{i=1}^n c_i y_i \,,\quad \text{donde} \quad c_i = \frac{\left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2}
\label{eme3}
$$

Se dice que los estimadores $m$ y $b$ por mínimos cuadrados son estimadores no ambiguos porque
$$
\begin{aligned}
E\left(m\right)  =E\left(\sum_{i=1}^n c_i y_i\right)=\sum_{i=1}^n c_i E\left(y_i\right)  =\sum_{i=1}^n c_i\left(\beta_0+\beta_1 x_i\right)=\beta_0 \sum_{i=1}^n c_i+\beta_1 \sum_{i=1}^n c_i x_i
\end{aligned}
$$
ya que por suposición $E\left(\varepsilon_i\right)=0$. Se puede demostrar que $\sum_{i=1}^n c_i=0$ y $\sum_{i=1}^n c_i x_i=1$, por lo que
$$
E\left(m \right)=\beta_1
$$

Es decir, si suponemos que el modelo es correcto $\left[E\left(y_i\right)=\beta_0+\beta_1 x_i\right]$, entonces $m$ es un estimador no sesgado de $\beta_1$. Del mismo modo podemos demostrar que $b$ es un estimador no sesgado de $\beta_0$, o bien
$$
E\left(b\right)=\beta_0 \,.
$$


%example
\begin{example}{Ley de enfriamiento de Steinhart-Hart. }
\label{ejemcizalla}
\begin{figure}[!t]
\begin{tabular}{ccc}
\hline Observación, $i$ & \begin{tabular}{c} 
Resistencia, $y_i \ ( \Omega )$
\end{tabular} & \begin{tabular}{c} 
Temperatura, $x_i \ ($ºC $)$
\end{tabular} \\
\hline 1 & 2158.70 & 15.50 \\
2 & 1678.15 & 23.75 \\
3 & 2316.00 & 8.00 \\
4 & 2061.30 & 17.00 \\
5 & 2207.50 & 5.50 \\
6 & 1708.30 & 19.00 \\
7 & 1784.70 & 24.00 \\
8 & 2575.00 & 2.50 \\
9 & 2357.90 & 7.50 \\
10 & 2256.70 & 11.00 \\
11 & 2165.20 & 13.00 \\
12 & 2399.55 & 3.75 \\
13 & 1779.80 & 25.00 \\
14 & 2336.75 & 9.75 \\
15 & 1765.30 & 22.00 \\
16 & 2053.50 & 18.00 \\
17 & 2414.40 & 6.00 \\
18 & 2200.50 & 12.50 \\
19 & 2654.20 & 2.00 \\
20 & 1753.70 & 21.50 \\
21 & 2665.86 & 0.00 \\
\hline
\end{tabular}
\label{datosciza}
\caption{Datos para el ejemplo \ref{ejemcizalla}.}
\end{figure}

Para describir con precisión la relación entre la resistencia $R$  de un termistor y su temperatura $T$ se utiliza la ecuación de Steinhart-Hart:
$$
\frac{1}{T}=A+B \ln (R)+C[\ln (R)]^3
$$
donde:

- $T$ es la temperatura.

- $R$ es la resistencia en ohmios.

- $A, B, C$ son constantes específicas del termistor.

En un rango pequeño de temperaturas, la relación entre $R$ y $T$ puede aproximarse a:
$$
R \approx R_0+\alpha\left(T-T_0\right)
$$        
donde: $R_0$ es la resistencia a una temperatura de referencia $T_0$ y $\alpha$ es el coeficiente de temperatura, que indica la tasa de cambio de la resistencia con la temperatura.

{\bf Experimento:}  Medición de la resistencia de un termistor NTC

El objetivo del experimento es determinar la relación lineal aproximada entre la resistencia y la temperatura de un termistor NTC en un rango limitado de temperaturas, donde las variables son: $y$, resistencia del material y $x$ la temperatura.  


Procedimiento:  
\begin{itemize}
\item Calentar el termistor: coloca el termistor en un baño de agua caliente para llevarlo a una temperatura significativamente más alta que la ambiente.
\item  Medir la resistencia y temperatura: retirar el termistor del agua caliente y medir su resistencia a intervalos regulares mientras se enfría. Simultáneamente, medir la temperatura del termistor.
\item Registrar los Datos: anotar las mediciones de resistencia $R$ y temperatura $T$
\end{itemize}


Un diagrama de dispersión con los datos recolectados sugiere una fuerte relación estadística entre la resistencia  y la temperatura. La hipótesis inicial de un modelo lineal, $y=\beta_0+\beta_1 x+\varepsilon$, parece razonable. 

A continuación haremos los cálculos directamente con Python y la librería ``statsmodels'' ya usada en el ejemplo \ref{ejemvelocidadtiempo}. 
\begin{lstlisting}[language=Python] 
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
\end{lstlisting}

Ahora es necesario introducir los datos de la tabla \ref{datosciza}
\begin{lstlisting}[language=Python] 
# Datos de resistencias y de temperaturas
yn = np.array([2158.70, 1678.15, 2316.00, 2061.30, 2207.50,
               1708.30, 1784.70, 2575.00, 2357.90, 2256.70,
               2165.20, 2399.55, 1779.80, 2336.75, 1765.30,
               2053.50, 2414.40, 2200.50, 2654.20, 1753.70,2665.86])
xn = np.array([150.50, 230.75, 80.00, 170.00, 50.50, 190.00, 240.00, 20.50,
               70.50, 110.00, 130.00, 30.75, 250.00, 90.75, 220.00, 180.00,
               60.00, 120.50, 20.00, 210.50,0.00])
\end{lstlisting}

Luego hacemos el cálculo de los parámetros $m$ y $b$
\begin{lstlisting}[language=Python] 
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = -3.7234 \pm 0.2695$

$b = 2622.2834 \pm 39.7582$
}
\end{tcolorbox}

Y la gráfica con los datos y la recta obtenida
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos experimentales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title('Linealidad')
plt.xlabel('Temperatura (°C)')
plt.ylabel('Resistencia ($Omega$)')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig21}  
\label{figdatos2}
\end{center}
\end{figure}

La recta tiene por ecuación
$$
\hat{y}=2622.2834 - 3.7234 x \,.
$$
Recordemos que la pendiente $-3.7234$ se interpreta como la disminución de la resistencia  debida a al cambio de temperatura. Dado que el límite inferior de las $x$ está cerca del origen, el intercepto $2622.2834$ representa la resistencia para la temperatura inicial. 

Veamos los valores ajustados $\hat{y}$ obtenidos
\begin{lstlisting}[language=Python] 
y_pred = m * xn + b
y_pred
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([2061.90678577, 1763.10128967, 2324.40881038, 1989.29984279,
       2434.25008309, 1914.83118333, 1728.65953467, 2545.95307228,
       2359.78142362, 2212.70582118, 2138.23716172, 2507.78788431,
       1691.42520494, 2284.38190591, 1803.12819413, 1952.06551306,
       2398.87746984, 2173.60977496, 2547.81478877, 1838.50080738,
       2622.28344823])
}
\end{tcolorbox}
y también los residuos
\begin{lstlisting}[language=Python] 
e_i=yn-y_pred
e_i
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([  96.79321423,  -84.95128967,   -8.40881038,   72.00015721,
       -226.75008309, -206.53118333,   56.04046533,   29.04692772,
         -1.88142362,   43.99417882,   26.96283828, -108.23788431,
         88.37479506,   52.36809409,  -37.82819413,  101.43448694,
         15.52253016,   26.89022504,  106.38521123,  -84.80080738,
         43.57655177])
}
\end{tcolorbox}

Hay varias propiedades de los mínimos cuadrados que podemos probar:


1)  La suma de los valores observados $y_i$ es igual a la suma de los valores ajustados $\hat{y}_i$, o bien
$$
\sum_{i=1}^n y_i=\sum_{i=1}^n \hat{y}_i
$$
El lado izquierdo de la ecuación es:
\begin{lstlisting}[language=Python] 
Sum_y = np.sum(yn)
Sum_y 
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
45293.009999999995
}
\end{tcolorbox}
Y el lado derecho:
\begin{lstlisting}[language=Python] 
Sum_yp = np.sum(y_pred)
Sum_yp
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
45293.00999999999
}
\end{tcolorbox}

2) La suma de los residuos en cualquier modelo de regresión que contiene un intercepto $\beta_0$ es siempre cero, es decir,
$$
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n e_i=0
$$

\begin{lstlisting}[language=Python] 
np.sum(e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
1.1823431123048067e-11
}
\end{tcolorbox}
Es decir, $\sum e_i=0.00$. 

3)  La recta de regresión por mínimos cuadrados siempre pasa por el centroide, el punto $(\bar{x}, \bar{y})$ de los datos.
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n  x_i \,, \quad  \bar{y} = \frac{1}{n}\sum_{i=1}^n  y_i= m\bar{x} +b \,. 
$$
\begin{lstlisting}[language=Python] 
# Los promedios de x y y 
n=len(xn)
xp= np.sum(xn)/n
yp= np.sum(yn)/n
print(xp,',', yp,',', m * xp + b)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
125.01190476190476 , 2156.81 , 2156.8099999999995
}
\end{tcolorbox}

4) La suma de los residuos ponderada por el valor correspondiente de la variable regresora siempre es igual a cero, es decir,
$$
\sum_{i=1}^n x_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(xn*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
1.2842065189033747e-09
}
\end{tcolorbox}

5) La suma de los residuos ponderada por el valor ajustado  siempre es cero
$$
\sum_{i=1}^n \hat{y}_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(y_pred*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
2.6237103156745434e-08
}
\end{tcolorbox}

Como ya lo señalamos, los programas estadísticos suelen ofrecer un resumen de todos los cálculos que realiza el programa. La salida en pantalla o consola de la figura \ref{figejemciza} presenta los resultados de la librería ``statsmodels''  para el ejemplo que estamos considerando. La parte superior de la tabla contiene el modelo de regresión ajustado. Observe que, antes del redondeo, los coeficientes de regresión coinciden con los que calculamos manualmente. La salida en pantalla también contiene otra información sobre el modelo de regresión que pueden ser consultadas en el manual de la librería. 
\begin{lstlisting}[language=Python] 
results = model
print(results.summary())
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=4.6in]{figuras/fig22}  
\caption{Resumen de las estimaciones estadísticas por consola.}
\label{figejemciza}
\end{center}
\end{figure}

\end{example}
\exampleline


\subsubsection{La estimación de $\sigma^2$}

Además de estimar $\beta_0$ y $\beta_1$, se necesita una estimación de la varianza del error $\sigma^2$, que  proporciona información crucial sobre la variabilidad de los errores (residuos) y tiene varias aplicaciones en el análisis estadístico, como  probar hipótesis y construir estimaciones de intervalo pertinentes para el modelo de regresión. Lo ideal sería que esta estimación no dependiera de la adecuación del modelo ajustado. Esto sólo es posible cuando hay varias observaciones de $y$ para al menos un valor de $x$  o cuando se dispone de información previa sobre $\sigma^2$. Cuando no se puede utilizar este enfoque, la estimación de $\sigma^2$ se obtiene a partir de la suma de cuadrados de los  errores:
\begin{equation}
\hat{\sigma}^2=\sum_{i=1}^n e_i^2=\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2 \,.
\label{sigma2}
\end{equation}
En inglés $\sigma^2=$ Sum of Squares for Error ($\mathrm{SSE}$)

Conocidos los valores estimados de $\beta_0$ y $\beta_1$, es decir: $
\hat{y}_i=b+m x_i $ y al sustituir en la ecuación anterior se llega al siguiente resultado
\begin{equation}
\hat{\sigma}^2=\sum_{i=1}^n y_i^2-n \bar{y}^2-m \sum_{i=1}^n y_i\left(x_i-\bar{x}\right) \,.
\end{equation}
Como
\begin{equation}
\sum_{i=1}^n y_i^2-n \bar{y}^2=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 \equiv {\mathrm{SST}} \,,
\end{equation}
donde $\mathrm{SST}=$ Total Sum of Squares. 

Esto no es más que la suma de cuadrados corregida de las observaciones de respuesta, por lo que
\begin{equation}
\hat{\sigma}^2= {\mathrm{SSE}}= {\mathrm{SST}}-m S_{x y}
\label{sigma2b}
\end{equation}

La suma residual de cuadrados tiene $n-2$ grados de libertad, porque dos grados de libertad están asociados con las estimaciones $b$ y $m$ implicadas en la obtención de $\hat{y}_i$. Se puede mostrar que el valor esperado de $S S_{\text {Res}}$ es $E\left(S S_{\text {Res }}\right)=(n-2) \sigma^2$, por lo que un estimador no sesgado de $\sigma^2$ es
\begin{equation}
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2}\,.
\label{sigma2c}
\end{equation}

La raíz cuadrada de $\hat{\sigma}^2$ se llama a veces el error estándar de regresión, y tiene las mismas unidades que la variable de respuesta $y$.

Dado que $\hat{\sigma}^2$ depende de la suma residual de cuadrados, cualquier violación de los supuestos sobre los errores del modelo o cualquier especificación errónea de la forma del modelo puede dañar seriamente la utilidad de $\hat{\sigma}^2$ como estimación de $\sigma^2$. Dado que $\hat{\sigma}^2$ se calcula a partir de los residuos del modelo de regresión, decimos que es una estimación dependiente del modelo de $\sigma^2$.

A partir de  $\hat{\sigma}^2$ los errores estándar de $m$ y $b$ se pueden obtener a partir de las siguientes ecuaciones
\begin{equation}
\mathrm{SE}\left(b\right)^2=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right], \quad 
\mathrm{SE}\left(m\right)^2=\frac{\sigma^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2},
\label{SEmyb}
\end{equation}


Si volvemos al ejemplo \ref{ejemcizalla} se pueden hacer los siguientes cálculos

$$
\begin{aligned}
S S_{\mathrm{T}} & =\sum_{i=1}^n y_i^2-n \bar{y}^2=\sum_{i=1}^n y_i^2-\frac{\left(\sum_{i=1}^n y_i\right)^2}{n} \\
& =92547433.45-\frac{(42,627.15)^2}{20}=1693737.60
\end{aligned}
$$

De la ecuación  (\ref{sigma2b}) la suma residual de cuadrados es
$$
\begin{aligned}
S S_{\text {Res }} & =S S_{\mathrm{T}}-\hat{\beta}_1 S_{x y} \\
& =1693737.60-(-37.15)(-41112.65)=166402.65
\end{aligned}
$$

El valor estimado de $\sigma^2$ se obtiene a partir de la ecuación (\ref{sigma2c}) 
$$
\hat{\sigma}^2=\frac{S S_{\mathrm{Res}}}{n-2}=\frac{166402.65}{18}=9244.59
$$

Para los errores usamos las ecuaciones (\ref{SEmyb})
$$
\Delta b=\sqrt{(9244.59)\left[\frac{1}{20}+\frac{178.56}{1106.56}\right]}=44.184 \,,\quad 
\Delta m=\sqrt{\frac{9244.59}{1106.56}}=2.889 \,,
$$

\subsubsection{Prueba de hipótesis sobe $m$ y $b$}


Los errores estándar también pueden utilizarse para realizar pruebas de hipótesis sobre los coeficientes. La prueba de hipótesis más común consiste en probar la hipótesis nula:
$$
H_0 \text {: No hay relación entre } x \text{ y } y
$$
frente a la hipótesis alternativa 
$$
H_a  \text {:  Existe alguna relación entre } x \text{ y }  y
$$

Matemáticamente, esto corresponde a probar
$$
H_0: \beta_1=0 \text { frente a } H_a: \beta_1 \neq 0 \,,
$$
ya que si $\beta_1=0$ entonces el modelo (\ref{regrelineal}) se reduce a $y=\beta_0+\varepsilon$, y por lo tanto, $x$ no está relacionado con $y$. Para probar la hipótesis nula, tenemos que determinar si $m$, nuestra estimación de $\beta_1$, está lo suficientemente lejos de cero para que podemos estar seguros de que $\beta_1$ es distinto de cero ¿Qué distancia es suficiente? Esto, por supuesto, depende de la exactitud de $m$, es decir, depende de $\operatorname{SE}\left(m\right)$. Si $\operatorname{SE}\left(m\right)$ es pequeño, entonces incluso valores relativamente pequeños de $m$ pueden proporcionar una fuerte evidencia de que $\beta_1 \neq 0$, y por lo tanto que existe una relación entre $x$ y $y$. Por el contrario, si $\mathrm{SE}\left(m\right)$ es grande, entonces $m$ debe ser grande en valor absoluto para que podamos rechazar la hipótesis nula. En la práctica, se suele calcular un ``estadístico'' $t$ o valor $t$, dado por
\begin{equation}
t=\frac{m-0}{\mathrm{SE}\left(m\right)} \,,
\label{valort}
\end{equation}
que mide el número de desviaciones típicas que $\beta_1$ se aleja de 0. Si realmente no hay relación entre $x$ y $y$, entonces esperamos que (\ref{valort}) tenga una distribución $t$ con $n - 2$ grados de libertad. La distribución $t$ tiene forma de campana y para valores de $n$ superiores a 30 aproximadamente es bastante similar a la distribución normal estándar. En consecuencia, resulta sencillo calcular la probabilidad de observar cualquier número igual a $|t|$ o mayor en valor absoluto, suponiendo que $\beta_1=0$. 

Llamamos a esta probabilidad el valor $p$. A grandes rasgos, interpretamos el valor $p$ de la siguiente manera: un valor $p$ pequeño indica que es poco probable observar una asociación tan sustancial entre el predictor y la respuesta debido al azar, en ausencia de cualquier asociación real entre el predictor y la respuesta. Por lo tanto, si vemos un valor $p$ pequeño, podemos inferir que existe una asociación entre el predictor y la respuesta. Rechazamos la hipótesis nula, es decir, declaramos que existe una relación entre $x$ y $y$, si el valor $p$ es lo suficientemente pequeño. Los límites típicos del valor $p$ para rechazar la hipótesis nula son el 5\% o el 1\%. Cuando $n = 30$, estos valores corresponden a estadísticos $t$ (\ref{valort}) de alrededor de 2 y 2.75, respectivamente.








