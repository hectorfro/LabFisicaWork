\chapter{Regresión lineal}

\section{Introducción}

Este capítulo lo dedicamos al  análisis de regresión, que es una técnica estadística utilizada para investigar y modelar la relación entre variables. Los métodos de análisis de regresión tienen una gran cantidad de aplicaciones en ingeniería, ciencias físicas y químicas, economía, ciencias de la vida y las ciencias sociales. Otro campo de aplicación es la ciencia y análisis de datos, esto hace que el espectro de problemas donde se aplica el análisis de regresión sea muy amplio.

El análisis de regresión fue desarrollado por primera vez por Sir Francis Galton\footnote{Francis Galton 1822-1911. \url{https://en.wikipedia.org/wiki/Francis_Galton}} a finales del siglo XIX. Galton redescubrió de forma independiente el concepto de correlación y demostró su aplicación en el estudio de la herencia, la antropología y la psicología. Galton desarrolló una descripción matemática de la tendencia regresiva, precursora de los modelos de regresión actuales y el término regresión sigue utilizándose para describir las relaciones estadísticas entre variables.

\section{Regresión y la construcción de modelos}
El análisis de regresión es una técnica estadística que permite investigar y construir modelos que establecen relaciones entre variables.

Las relaciones estadísticas son diferentes de las  relaciones funcionales porque las relaciones estadísticas no son perfectas, es decir, las observaciones de una relación estadística no caen directamente sobre una curva de relación. 

Supongamos que observamos una respuesta cuantitativa $y$ para  $k$ ``predictores'' diferentes: $x_1, x_2, \ldots, x_k$. Podemos suponer también que existe alguna relación entre $y$ y $x=\left(x_1, x_2, \ldots, x_k\right)$, que puede escribirse de la forma muy general
\begin{equation}
y=f(x)+\varepsilon \,.
\label{funcion}
\end{equation}
donde $f$ es alguna función fija pero desconocida de $x_1, \ldots, x_k$, y $\varepsilon$ es un término de error aleatorio, que es independiente de $x$ y tiene media cero. En esta formulación, $f$ representa la información sistemática que $x$ proporciona sobre $y$.

Existen dos razones principales para querer estimar $f$: la predicción y la inferencia. 

\begin{itemize}
\item Predicción:  Este proceso utiliza un modelo estadístico o de aprendizaje automático para predecir valores futuros basados en datos existentes. El objetivo principal es obtener valores precisos de la variable de interés, y la precisión de las predicciones es crucial. En este contexto, es común tener entradas $x$ disponibles, pero no la salida $y$. Podemos predecir $y$ usando
$$
\hat{y} = \hat{f}(x)\,,
$$
donde $\hat{f}$ es nuestra estimación de $f$, y $\hat{y}$ es la predicción para $y$. Consideramos $\hat{f}$ como una caja negra, no importando su forma exacta siempre que las predicciones sean precisas. Generalmente, $\hat{f}$ no será perfecta, introduciendo error que puede reducirse con mejores técnicas de aprendizaje estadístico. Incluso con una predicción perfecta $f$, $\hat{y} = f(x)$ aún tendría error debido a $\varepsilon$, de carácter aleatorio, conocido como error irreducible.

Métodos comunes para predicción incluyen regresión lineal, árboles de decisión, y redes neuronales, aplicables en situaciones como la predicción de precios en la bolsa o el valor de un producto bajo ciertas condiciones.

\item Inferencia:  El objetivo no es necesariamente predecir $y$ al estimar $f$, sino conocer $f$ de manera exacta. Se trata de usar un modelo estadístico para entender la relación entre variables y sacar conclusiones sobre la población de la cual se extrajo la muestra. El objetivo principal es realizar afirmaciones sobre los parámetros del modelo y la naturaleza de las relaciones entre las variables. Podríamos preguntarnos si existe una relación entre $y$ y cada predictor en forma de una ecuación lineal, o si es más compleja. Históricamente, la mayoría de los métodos para estimar $f$ han adoptado una forma lineal, lo cual es a veces razonable o deseable. Sin embargo, a menudo la relación verdadera es más complicada, y un modelo lineal puede no representar con precisión la relación entre las variables.

Métodos comunes en inferencia incluyen pruebas de hipótesis, estimación de intervalos de confianza, análisis de varianza (ANOVA), y regresión lineal múltiple. Estas técnicas pueden usarse para inferir si existe una relación entre el consumo de café y el riesgo de padecer diabetes.

\end{itemize}


Es muy probable encontrar situaciones que se encuadren en el ámbito de la predicción, en el de la inferencia, o en una combinación de ambos. Por ejemplo, los modelos lineales permiten una inferencia relativamente sencilla e interpretable, pero pueden no producir predicciones tan precisas como otros enfoques. Por el contrario, algunos de los enfoques no lineales pueden proporcionar predicciones bastante precisas para $y$, pero a costa de un modelo menos interpretable, haciendo la inferencia más difícil.


Uno de los métodos más utilizados para estimar $f$ es el ``método paramétrico'', que comienza con una suposición sobre la forma funcional de $f$. Por ejemplo, una suposición simple es que $f$ es lineal en $x$:
\begin{equation}
f(x)=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,, 
\label{modlineal}
\end{equation}
Esto simplifica enormemente el problema de estimar $f$, ya que solo hay que estimar los coeficientes $\beta_0, \beta_1, \ldots, \beta_k$. Una vez seleccionado un modelo, necesitamos un procedimiento para ajustar o entrenar el modelo con los datos. En el caso del modelo lineal (\ref{modlineal}), queremos estimar los parámetros \( \beta_0, \beta_1, \ldots, \beta_k \), encontrando valores de estos parámetros tales que
$$
y \approx \beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k \,.
$$
El enfoque más común para ajustar el modelo lineal (\ref{modlineal}) es el de los mínimos cuadrados ordinarios, aunque existen otros métodos para ajustar el modelo.


De alguna manera, los modelos de regresión pueden considerarse como modelos empíricos. Si la relación entre las variables es muy compleja se puede utilizar regresiones lineales a trozos para aproximar la relación verdadera entre las variables $y$ y $x$. En este caso las ecuaciones de regresión sólo son válidas en la región de las variables regresoras contenidas en los datos observados. 

De manera general, la variable de respuesta $y$ puede estar relacionada con un número $k$ regresores, $x_1, x_2, . . . , x_k$, de forma que
\begin{equation}
y=\beta_0+\beta_1 x_1+\beta_2 x_2+\cdots+\beta_k x_k+\varepsilon
\label{reglin3}
\end{equation}

La ecuación (\ref{reglin3}) se denomina Modelo de Regresión Lineal Múltiple, y el adjetivo lineal se emplea para indicar que el modelo es lineal en los parámetros $\beta_0, \beta_1, \ldots, \beta_k$, no porque $y$ sea una función lineal de las $x$. Esto significa que muchos modelos en los que $y$ se relaciona con las $x$ de forma no lineal pueden seguir tratándose como modelos de regresión lineal siempre que la ecuación sea lineal en los $\beta$ 's.

Una fase crucial del análisis de regresión es comprobar la adecuación del modelo, evaluando su idoneidad y calidad del ajuste. Este análisis determina la utilidad del modelo y puede indicar si necesita ser modificado, haciendo del análisis de regresión un proceso iterativo guiado por los datos.

Es importante destacar que un modelo de regresión no implica causalidad entre variables. Una fuerte relación empírica no prueba una relación causal entre regresores y respuesta. Para establecer causalidad, se necesita más que los datos de la muestra, como consideraciones teóricas. El análisis de regresión puede confirmar una relación causal, pero no debe ser la única base para tal afirmación.

El análisis de regresión es parte de un enfoque más amplio para resolver problemas. La ecuación de regresión puede no ser el objetivo principal del estudio; a menudo es más importante entender el sistema que genera los datos.

En el trabajo de laboratorio, la recolección de datos es crucial. Un análisis de regresión es tan bueno como los datos en que se basa. Los problemas de estimación de parámetros se resuelven mediante regresión para predecir la variable de respuesta, pero al extrapolar, se corren riesgos significativos debido a errores del modelo. Una estimación incorrecta de parámetros puede llevar a predicciones deficientes. Es esencial tener en cuenta la precisión del modelo y la exactitud en la estimación de parámetros.

Los modelos de regresión pueden usarse con fines de control, donde las variables deben estar causalmente relacionadas. Sin embargo, para predicción, no es estrictamente necesario que exista una relación causal. Basta con que las relaciones en los datos originales sigan siendo válidas. Por ejemplo, el consumo diario de electricidad en agosto puede predecir la temperatura máxima diaria, pero intentar reducir la temperatura disminuyendo el consumo de electricidad fracasaría, ya que la relación no es causal.



\section{Los algoritmos en los modelos de regresión}

Desde el punto de vista computacional, construir un modelo de regresión es un proceso iterativo. Los datos de entrada incluyen conocimientos teóricos y datos disponibles para especificar un modelo inicial. Las representaciones gráficas son útiles para esta especificación. A continuación, se estiman los parámetros del modelo, generalmente con el método de mínimos cuadrados. Luego, se evalúa la adecuación del modelo, identificando posibles errores de especificación, omisión de variables importantes, inclusión de variables innecesarias o presencia de datos inusuales. Si el modelo es inadecuado, se ajusta y se vuelven a estimar los parámetros. Este proceso se repite hasta obtener un modelo adecuado, que finalmente debe ser validado para asegurar resultados aceptables.

El análisis de regresión requiere un uso hábil del computador. Un buen programa de cálculo de regresión es esencial, pero la aplicación rutinaria de programas estándar no siempre da resultados satisfactorios. El computador no sustituye el pensamiento creativo sobre el problema. Debemos aprender a interpretar la información que proporciona y usarla en modelos posteriores. Entre el software estadístico disponible, se destaca el SAS (Statistical Analysis System), un paquete comercial para análisis de datos con una amplia gama de procedimientos estadísticos, y R, un lenguaje de código abierto y gratuito, conocido por su flexibilidad e innovación.


\section{Regresión lineal simple}

Un modelo con un único regresor $x$ y que tiene una relación con la respuesta $y$ en la forma de una línea recta es lo que se denomina un modelo de regresión lineal simple:
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i \,.
\label{regrelineal}
\end{equation}

La intersección $\beta_0$  y la pendiente $\beta_1$  son constantes desconocidas, mientras que $\varepsilon$ es un componente de error aleatorio. Suponemos que los errores tienen media cero y varianza desconocida $\sigma^2$. También asumimos que los errores no están correlacionados, lo que significa que el valor de un error no depende del valor de ningún otro error. Es importante considerar el regresor $x$
como controlado por el analista de datos o experimentador y medido con un error despreciable, mientras que la respuesta $y$ es una variable aleatoria. Esto implica que existe una distribución de probabilidad para $y$  en cada valor posible de $x$.

\paragraph{Características importantes de este modelo}
\begin{enumerate}
\item La respuesta $y_i$  es la suma de dos componentes: (1) el término constante $\beta_0+\beta_1 x_i$ y (2) el término aleatorio $\varepsilon_i$. Por lo tanto, $y_i$ es una variable aleatoria.

\item Como $E\left\{\varepsilon_i\right\}=0$, se puede demostrar que la media de esta distribución es:
$$
E(y \mid x)=E\left\{y_i\right\}=E\left\{\beta_0+\beta_1x_i+\varepsilon_i\right\}=\beta_0+\beta_1 x_i+E\left\{\varepsilon_i\right\}=\beta_0+\beta_1 x_i
$$
Así, la respuesta $y_i$ para  $x_i$, procede de una distribución de probabilidad cuya media es:
$$
E\left\{y_i\right\}=\beta_0+\beta_1 x_i
$$
Por tanto, sabemos que la función de regresión para el modelo (\ref{regrelineal}) es:
$$
E\{y\}=\beta_0+\beta_1 x
$$
ya que la función de regresión relaciona las medias de las distribuciones de probabilidad de $y$ para $x$.

\item La respuesta $y_i$ supera o no alcanza el valor de la función de regresión por la cantidad del término de error $\varepsilon_i$.

\item  Se supone que los términos de error $\varepsilon_i$ tienen una varianza constante $\sigma^2$. Por lo tanto, se deduce que las respuestas $y_i$ tienen la misma varianza constante:
$$
\sigma^2\left\{y_i\right\}=\sigma^2
$$
Se puede demostrar también que:
$$
\sigma^2\left\{\beta_0+\beta_1 x_i+\varepsilon_i\right\}=\sigma^2\left\{\varepsilon_i\right\}=\sigma^2
$$
Así, el modelo de regresión (\ref{regrelineal}) asume que las distribuciones de probabilidad de $y$ tienen la misma varianza $\sigma^2$, independientemente del nivel de la variable predictora $x$.

\item  Se supone que los términos de error no están correlacionados. Dado que los términos de error $\varepsilon_i$ y $\varepsilon_j$ no están correlacionados, tampoco lo están las respuestas $y_i$ y $y_j$.


\end{enumerate}

\paragraph{Significado de los parámetros de regresión}

Los parámetros $\beta_0$ y $\beta_1$ del modelo de regresión (\ref{regrelineal}) se denominan coeficientes de regresión. $\beta_1$ es la pendiente de la recta de regresión. Indica el cambio en la media de la distribución de probabilidad de $y$ por unidad de aumento en $x$. El parámetro $\beta_0$ es la intercepción $y$ de la recta de regresión. Cuando el ámbito del modelo incluye $x=0$, $\beta_0$ da la media de la distribución de probabilidad de $y$ en $x=0$. Cuando el ámbito del modelo no incluye $x=0, \beta_0$ no tiene ningún significado particular como término separado en el modelo de regresión.

Supongamos que en una universidad se estudió la relación entre el tiempo de estudio (en horas) y las calificaciones de 25 estudiantes en el examen final de matemáticas aplicadas. El gráfico de dispersión en la figura \ref{regres1} muestra que a mayor tiempo de estudio, las notas tienden a aumentar, sugiriendo una relación lineal aproximada entre las variables. 
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.0in]{figuras/fig18}  \quad 
\includegraphics[height=3.0in,width=3.0in]{figuras/fig19}  
\caption{Izquierda: gráfico de dispersión para las horas de estudio y las notas obtenidas. Derecha: la  generación de observaciones en la regresión lineal.}
\label{regres1}
\end{center}
\end{figure}

Podemos suponer una relación de la forma
\begin{equation}
y = \beta_0 + \beta_1 x \,,
\label{reglin1}
\end{equation}
donde $y$ son las notas, $x$ es el tiempo de estudio, $\beta_0$ es la intersección y $\beta_1$ es la pendiente de la recta.

Dado que los puntos no caen exactamente sobre la línea recta, la ecuación (\ref{reglin1}) se ajusta añadiendo un término de error:
\begin{equation}
y = \beta_0 + \beta_1 x + \varepsilon \,,
\label{reglin2}
\end{equation}
donde $\varepsilon$ representa la variación aleatoria en las notas. Este modelo más realista considera factores no medidos y errores, denominándose Modelo de Regresión Lineal (MRL). Aquí, $x$ es la variable ``predictora'' o ``regresora'' y $y$ la variable de respuesta. Al incluir solo una variable regresora, este modelo se llama Modelo de Regresión Lineal Simple (MRLS).


Podemos analizar el MRL fijando el valor de $x$ y observando $y$. Si la media y varianza de $\varepsilon$ son 0 y $\sigma^2$, respectivamente, la respuesta media para cualquier valor de $x$ es:
\begin{equation}
\mu_{y \mid x} \equiv E(y \mid x)=E\left(\beta_0+\beta_1 x+\varepsilon\right)=\beta_0+\beta_1 x \,,
\end{equation}
igual a la relación (\ref{reglin1}). La varianza de $y$ dado $x$ es:
\begin{equation}
\sigma_{y \mid x}^2 \equiv \operatorname{Var}(y \mid x)=\operatorname{Var}\left(\beta_0+\beta_1 x+\varepsilon\right)=\sigma^2 \,.
\end{equation}

El verdadero modelo de regresión $\mu_{y \mid x}=\beta_0+\beta_1 x$ es una línea de valores medios, donde $\beta_1$ indica el cambio en la media de $y$ por un cambio unitario en $x$. La variabilidad de $y$ en cada $x$ está dada por $\sigma^2$, implicando que los valores de $y$ tienen la misma varianza en cada $x$.

Si el modelo de regresión es $\mu_{y \mid x}=3.5+2.0 x$ y $\sigma^2=2$ el resultado se ve en la figura derecha de  \ref{regres1}. Utilizando una distribución normal para $\varepsilon$, $y$ también es normal. Para $x=10$ horas, $y$ tiene como media $3.5+2(10)=23.5$ y varianza 2. $\sigma^2$ determina la variabilidad en $y$ sobre el tiempo de estudio: un $\sigma^2$ pequeño indica valores de $y$ cerca de la recta, mientras que un $\sigma^2$ grande indica mayores desviaciones.

Notemos que pudimos haber hecho un estudio similar pero con ocho estudiantes del curso,  que elegiríamos de manera aleatoria, para entonces proceder a recopilar los datos sobre el tiempo de estudio y las calificaciones de estos estudiantes. En este caso estaríamos  haciendo lo que se llama un estudio de regresión muestral. 


Recordemos que el fin principal en el análisis de regresión es estimar los parámetros desconocidos del modelo de regresión. Este proceso también se denomina ajuste del modelo a los datos. Existen varias técnicas de estimación de parámetros y una de estas técnicas es el método de los mínimos cuadrados. En el ejemplo que hemos mencionado  de las horas de estudio y las notas obtenidas por los estudiantes nos llevaría a una ecuación como la que se muestra a continuación
$$
\hat{y}=3.687+ 1.821 x \,,
$$
donde $\hat{y}$  es el valor ajustado o estimado de las notas correspondiente a un valor del número de horas $x$. Esta ecuación ajustada es la que se  representa en la parte izquierda de la figura \ref{regres1}. Note que un estudiante que estudia cero horas $x=0$ obtendría una nota de $3.7$ de 100. 


\subsection{Método de los mínimos cuadrados}

El método de los mínimos cuadrados es uno de los métodos estadísticos más usados para determinar la recta que mejor represente la tendencia de un conjunto de puntos experimentales. Como mencionamos anteriormente, los parámetros $\beta_0$ y $\beta_1$ son desconocidos y deben estimarse utilizando datos muestrales. Supongamos que tenemos $n$ pares de datos, digamos $\left(y_1, x_1\right),\left(y_2, x_2\right), \ldots,\left(y_n, x_n\right)$. Estos datos pueden proceder de un experimento controlado diseñado específicamente para recopilar los datos, de un estudio observacional o de registros históricos existentes (un estudio retrospectivo).

Para estimar $\beta_0$ y $\beta_1$ utilizaremos el método de los mínimos cuadrados, esto es, estimamos $\beta_0$ y $\beta_1$ de forma que la suma de los cuadrados de las diferencias entre las observaciones $y_i$ y la recta sea un mínimo. A partir de (\ref{regrelineal}) podemos escribir
\begin{equation}
y_i=\beta_0+\beta_1 x_i+\varepsilon_i, \quad i=1,2, \ldots, n
\label{diferencia}
\end{equation}

La ecuación (\ref{regrelineal}) puede verse como un modelo de regresión poblacional, mientras que la ecuación (\ref{diferencia}) es un modelo de regresión muestral, escrito en términos de los $n$ pares de datos $\left(y_i, x_i\right)$, con $i=1,2, \ldots, n$. Así, el criterio de mínimos cuadrados es
\begin{equation}
S\left(\beta_0, \beta_1\right)=\sum_{i=1}^n\left(y_i-\beta_0-\beta_1 x_i\right)^2
\end{equation}

Si la dispersión de los puntos experimentales es debida solo a los errores casuales en las mediciones, la mejor recta será aquella para la cual la suma de los cuadrados de las distancias $\left(y_i-y_0 \right)$ sea un mínimo. Es por esto que, a este método se le llama método de los mínimos cuadrados.


A los estimadores obtenidos por mínimos cuadrados $\beta_0$ y $\beta_1$, los llamaremos $b$ y $m$, respectivamente, y deben satisfacer una relación lineal de la forma:
$$
\hat{y}=m x+b \,,
$$
donde $\hat{y}$ es la variable dependiente y $x$ es la variable independiente, en nuestro caso la magnitud controlada por el experimentador. Como se ha indicado anteriormente, los valores de esas magnitudes tendrán sus correspondientes errores que determinaremos más adelante. 

La desviación de un valor cualquiera $y_i$ determinado experimentalmente con respecto a su valor $y_0$ en la recta, será:
\begin{equation}
\Delta y_i=y_i-y_0=y_i-\left(b+m x_i\right)
\label{deltay}
\end{equation}

Ahora se puede enunciar el principio básico de este método, el cual dice que la mejor recta que puede ser trazada entre esos puntos, es aquella para la cual la suma de los cuadrados de las desviaciones $\Delta y_i$ de los datos experimentales, con respecto a esa recta, es mínima.
$$
\sum_{i=1}^n\left(\Delta y_i\right)^2=\sum_{i=1}^n \left[y_i-b-m x_i \right]^2
$$
donde $n$ es el número de pares de valores de $y$ y $x$.

Ya que la condición exigida es la de minimizar la suma anterior, entonces los parámetros $m$ y $b$ deben ajustarse para cumplir con esta condición. Ello se logra calculando las derivadas parciales de la suma con respecto a $m$ y con respecto a $b$, e igualándolas a cero.
$$
\begin{aligned}
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial b}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial b} = 
\sum_{i=1}^n -2(y_i-b-m x_i)=0  \\
& \frac{\partial\left[\sum\left(\Delta y_i\right)^2\right]}{\partial m}=
\sum_{i=1}^n\frac{\partial\left(y_i-b-m x_i \right)^2}{\partial m} =
\sum_{i=1}^n -2x_i(y_i-b-m x_i)=0
&
\end{aligned}
$$

Por lo tanto, se debe resolver el sistema 
$$
\begin{aligned}
&  \sum_{i=1}^n y_i - \sum_{i=1}^nb - \sum_{i=1}^n m x_i = 0  &\,\, \Rightarrow \,\,&
 nb + m \sum_{i=1}^n  x_i = \sum_{i=1}^n y_i  \\
&  \sum_{i=1}^n y_ix_i - \sum_{i=1}^n bx_i -\sum_{i=1}^n  m x_i^2 =0 & \,\, \Rightarrow \,\,&
b \sum_{i=1}^n x_i  + m \sum_{i=1}^n   x_i^2 = \sum_{i=1}^n y_ix_i
\end{aligned}
$$
para $m$ y $b$. 

Utilizando la regla de Cramer y definiendo $\Delta$ como se muestra a continuación:
$$
\begin{aligned}
\Delta= 
{\left|\begin{array}{cc}
n & \sum x_i \\
\sum x_i & \sum x_i^2
\end{array}\right|} = n \sum\limits_{i=1}^n x_i^2-\left(\sum\limits_{i=1}^nx_i\right)^2 \,,
\end{aligned}
$$
resulta:
$$
m=\frac{\left|\begin{array}{cc}
n & \sum y_i \\
\sum x_i & \sum x_i y_i
\end{array}\right|}{\Delta}=\frac{n\sum x_i y_i-\sum x_i \sum y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} \quad \text{y} \quad
 b=\frac{\left|\begin{array}{cc}
\sum y_i & \sum x_i \\
\sum x_i y_i & \sum x_i^2 
\end{array}\right|}{\Delta}=\frac{\sum x_i^2 \sum y_i-\sum x_i \sum x_i y_i}{n \sum x_i^2-\left(\sum x_i\right)^2} 
$$

Por lo tanto:
\begin{eqnarray}
m &=&\dfrac{ n\sum\limits_{i=1}^n x_i y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n y_i}{\Delta} = 
\dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} 
\label{eme2} \\
b &=&\dfrac{\sum\limits_{i=1}^n x_i^2 \sum\limits_{i=1}^n y_i-\sum\limits_{i=1}^n x_i \sum\limits_{i=1}^n x_i y_i}{\Delta} =  \bar{y}-m \bar{x} \,,
\label{b2}
\end{eqnarray}

Se puede  escribir (\ref{eme2}) como
\begin{equation}
m=\frac{S_{xy}}{S_x^2} \,.
\end{equation}

En donde $\bar{x}$ e $\bar{y}$ denotan las medias muestrales de $x$ y $y$ (respectivamente), 
$$
\bar{x}=\frac{\sum_{i=1}^n x_i}{n}, \, \quad \bar{y}=\frac{\sum_{i=1}^n y_i}{n}\,,
$$

$S_x^2$ es la varianza muestral de $x$ y $S_{xy}$ es la covarianza muestral entre $x$ y $y$. Estos parámetros se calculan como:
$$
S_x^2=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}{n}, \, S_y^2=\frac{\sum_{i=1}^n\left(y_i-\bar{y}\right)^2}{n}, \, S_{xy}=\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{n} .
$$

La cantidad $m$ se denomina coeficiente de regresión de $y$ sobre $x$, lo denotamos por $m_{y/x}$.

En el trabajo de laboratorio podríamos construir una tabla como la mostrada en \ref{tablamincua}, para así ordenar la información y facilitar los cálculos. Las cuatro sumas en la última línea, son los valores necesarios para calcular $m$ y $b$. Los valores de $m$ y $b$ que se obtengan por el método de los mínimos cuadrados, deberían ser muy próximos a los obtenidos directamente utilizando el método gráfico. 
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline $y_i [\,\,]$ & $x_i [\,\,]$ & $x_i^2 [\,\,]$& $x_i y_i [\,\,]$ & $y_i-m x_i-b$ & $(y_i-m x_i-b)^2$  \\
\hline \hline $y_1$ & $x_1$ & $x_1^2$ & $x_1 y_1$ & $y_1-m x_1-b$ & $\left(y_1-m x_1-b\right)^2$\\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &  $\vdots$ & $\vdots$ \\
\hline $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ &  $\vdots$ & $\vdots$ \\
\hline $y_n$ & $x_n$ & $x_n^2$ & $x_n y_n$ & $y_n-m x_n-b$ &$\left(y_n-m x_n-b\right)^2$ \\ \hline
\hline $\sum y_i$ &$\sum x_i $& $\sum x_i^2$ & $\sum x_i y_i$ & & $\sum\limits_{i=1}^n (y_i-m x_i-b)^2$  \\
\hline
\end{tabular}
\end{center}
\caption{Tabla para facilitar el uso del  método de mínimos cuadrados}
\label{tablamincua}
\end{table}

Pero en la actualidad es casi de rutina utilizar alguna herramienta computacional  que permite hacer los cálculos necesarios y los gráficos para el ajuste de la recta. Aunque el uso de este método no nos obliga a hacer el gráfico de la recta, por razones pedagógicas, es conveniente hacerlo para así observar más claramente las desviaciones de los puntos experimentales con respecto a la recta calculada. 

Una vez obtenido los valores de $m$ y ${b}$, es necesario calcular sus errores correspondientes $\Delta m$ y $\Delta b$. Esto lo podemos hacer calculando las desviaciones estándar  de la pendiente y la ordenada al origen, calculadas a partir de la distribución de diferencias $\Delta y_i$, ecuación (\ref{deltay}),  respecto de la mejor línea de ajuste.

Sea $\hat{y}_i=b+m x_i$ la predicción de $y$ basada en el valor $i$ de $x$. Entonces $e_i=y_i-\hat{y}_i$ representa el $i$ residuo - la diferencia entre el $i$ valor de respuesta observado y el $i$ valor de respuesta predicho por nuestro modelo lineal. Definimos la suma residual de cuadrados (RSS) como
$$
\mathrm{RSS}=e_1^2+e_2^2+\cdots+e_n^2
$$
o equivalentemente como
$$
\mathrm{RSS}=\left(y_1-b-m x_1\right)^2+\left(y_2-b-m x_2\right)^2+\cdots+\left(y_n-b-m x_n\right)^2 .
$$


La diferencia entre el valor observado $y_i$ y el correspondiente valor ajustado $\hat{y}_i$ es un residuo. Matemáticamente, el residuo $i$ es
$$
e_i=y_i-\hat{y}_i=y_i-\left(b+m x_i\right), \quad i=1,2, \ldots, n
$$

Los residuos desempeñan un papel importante en la investigación de la adecuación del modelo y en la detección de desviaciones de los supuestos subyacentes. 

Sea SSE (Sum of Squared Errors) la suma de los cuadrados de los residuos:
\begin{equation}
\mathrm{SSE} = \sum\limits_{i=1}^n e_i^2 = \sum\limits_{i=1}^n (y_i-\left(b+m x_i\right) )^2\,,
\label{sse}
\end{equation}
esta cantidad representa la parte de la variabilidad de $y$  que no se puede explicar por el modelo de regresión.

También se suelen definir las siguientes cantidades: la variabilidad explicada (Sum of Squares for Regression, SSR) que representa la parte de la variabilidad de $y$ que se explica por el modelo de regresión.
\begin{equation}
\mathrm{SSR}=\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 \,.
\label{ssr}
\end{equation}

Y la variabilidad total (Total Sum of Squares, SST) que representa la variabilidad total de la variable dependiente $y$ respecto a su media.
\begin{equation}
\mathrm{SST}=\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 \,.
\label{sst}
\end{equation}

La variabilidad total se puede descomponer en la variabilidad explicada por el modelo y la variabilidad residual
\begin{equation}
\mathrm{SST}= \mathrm{SSR} + \mathrm{SSE} \,\, \Rightarrow \,\,
\sum_{i=1}^n\left(y_i-\bar{y}\right)^2 =
\sum_{i=1}^n\left(\hat{y}_i-\bar{y}\right)^2 + 
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)^2
\label{sst2}
\end{equation}


Podemos calcular  el error estándar residual y  los errores estándar de la pendiente $\Delta m$ y el término independiente $\Delta b$ de la regresión lineal de la siguiente manera:
\begin{eqnarray}
\mathrm{SE}\left(m\right)^2 = \Delta m &=&\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \,, 
\label{Delm}  \\
\mathrm{SE}\left(b\right)^2 = \Delta b &=& \hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right]  \,,
\label{Delb}
\end{eqnarray}
donde 
\begin{equation}
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2}  \,.
\label{sigma2}
\end{equation}


%example%%%%%%%%
\begin{example}
\label{ejemvelocidadtiempo}
En un experimento sobre cinemática un grupo de estudiantes mide los tiempos con los que se desplaza  un móvil en un riel de aire. Los tiempos son medidos en segundos usando un cronómetro de sensibilidad $\Delta t=0,01$ s. Otro  instrumento  mide las velocidades con las que se desplaza el móvil, este instrumento tiene una sensibilidad de $\Delta v= 0,01$ m/s.  La tabla de datos y la respectiva gráfica se muestran la figura \ref{datosvt}.
\begin{figure}[h]
\centering
\begin{minipage}{0.54\textwidth}
    \centering
    \begin{tabular}{|c|c|}
    \hline $t_i \pm 0,01 (\mathrm{s})$ & $v_i \pm 0,01 (\mathrm{m/s})$  \\\hline \hline 
    \hline $0,00$ & 1.00 \\
    \hline $0,10$ & 1.64 \\
    \hline $0,20$ & 1.51 \\
    \hline $0,30$ & 2.03 \\
    \hline $0,40$ & 2.75 \\
    \hline $0,50$ & 3.59 \\
    \hline $0,60$ & 4.87 \\
    \hline $0,70$ & 5.23 \\
    \hline $0,80$ & 5.44 \\
    \hline $1,00$ & 6.37 \\
    \hline
    \end{tabular}
    \caption{Mediciones de la velocidad de un cuerpo en función del tiempo y la gráfica de dispersión respectiva.}
    \label{datosvt}
\end{minipage}
\hfill
\begin{minipage}{0.44\textwidth}
    \centering
    \includegraphics[height=2.6in,width=2.6in]{figuras/fig12}
\end{minipage}
\end{figure}

Como queremos  efectuar los cálculos manualmente es recomendable calcular las siguientes sumas:
$$
\begin{array}{c|c|c|c|c}
n&\sum_{i=1}^n t_i & \sum_{i=1}^n v_i   & \sum_{i=1}^n t_i^2  & \sum_{i=1}^n t_i v_i \\ \hline\hline 
10 & 4.60 & 34.43 & 3.04 & 21.275
\end{array}
$$
$$
\Delta=n\sum\limits_{i=1}^n t_i^2-\left(\sum\limits_{i=1}^n t_i\right)^2= 
10(3.04) - \left(4.60 \right)^2= 9.24 \,\, \mathrm{s}^2
$$
$$
b=\dfrac{\sum\limits_{i=1}^n t_i^2 \sum\limits_{i=1}^n v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n t_i v_i}{\Delta} =
\frac{(3.04)  (34.43) -(4.60) (21.275)}{9.24} = 0.7362 \,\, \mathrm{m/s}
$$
$$
m =\dfrac{n \sum\limits_{i=1}^n t_i v_i-\sum\limits_{i=1}^n t_i \sum\limits_{i=1}^n v_i}{\Delta} =\frac{10(21.275)  -(4.60) ( 34.43)}{9.24}= 5.8844\,\, \mathrm{m/s^2}
$$

Procedemos a calcular la suma de los cuadrados de los residuos
$$
\begin{array}{c|c}
n& \sum\limits_{i=1}^n \left(v_i-b-m t_i\right)^2  \\ \hline\hline 
10 & 1.244  
\end{array} 
$$
Por lo tanto 
$$
\hat{\sigma}=\left[\frac{\mathrm{SSE}}{n-2}\right]^{\frac{1}{2}}=
\left(\frac{1.244}{8}\right)^{\frac{1}{2}}= 0.394 \,.
$$


y los errores a partir de las ecuaciones (\ref{Delm}) y (\ref{Delb})
$$
\begin{aligned}
\Delta m & =0.401  \,, \\
\Delta b & = 0.226 \,. 
\end{aligned}
$$

Por lo tanto nuestro resultado será, por los redondeos hechos en los cálculos
$$
v=mt + b= 5.91 t + 0.724 \,\, \mathrm{m/s}
$$
donde lo correcto sería escribir: $m=5.9 \pm 0.4$ y $b=0.7 \pm 0.2$.


Con Python podemos hacer todos los cálculos anteriores, incluida la figura  \ref{datosvt}. Aquí vamos a  transcribir la ecuaciones las fórmulas utilizadas con anterioridad a manera de entender el proceso de los mínimos cuadrados. Luego veremos que existen otras opciones más directas. 

Primero que todo, debemos llamar las librerías:

\begin{lstlisting}[language=Python]    
import numpy as np
import matplotlib.pyplot as plt
\end{lstlisting}
\vspace{5mm}

Los datos deben escribirse en formas de arreglos:

\begin{lstlisting}[language=Python]    
yn=  array([1.000, 1.64, 1.51, 2.03, 2.75, 3.59, 4.87, 5.23, 5.44, 6.37])
xn = array([0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 1.00])
\end{lstlisting}

La gráfica de la figura \ref{datosvt} se obtiene a partir de las siguientes lineas de código:
\begin{lstlisting}[language=Python] 
plt.scatter(xn, yn, marker='*')
plt.title(r'Velocidad vs tiempo', fontsize=20)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig12}  
\label{figdatos2}
\end{center}
\end{figure}

Para usar las ecuaciones (\ref{eme2})-(\ref{b2}) se necesita hacer una serie de  cálculos intermedios, como se muestra a continuación:
\begin{lstlisting}[language=Python] 
#Se obtiene el valor de n (numero de datos)
n=len(xn)
#Las sumatorias necesarias 
Sum_x = np.sum(xn)
Sum_y = np.sum(yn)
Sum_xx = np.sum(xn**2)
Sum_xy = np.sum(xn*yn)
Sum_xSumy = np.sum(xn)*np.sum(yn)
Delta = n*np.sum(xn**2) - (np.sum(xn))**2
print(n,',', Sum_x, ',',Sum_y,',', Sum_xx,',', Sum_xy,',', Sum_xSumy, ',',Delta)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
10 , 4.6 , 34.43 , 3.04 , 21.275, 158.378 , 9.240000000000002
}
\end{tcolorbox} 

Y finalmente calculamos $b$ y $m$

\begin{lstlisting}[language=Python] 
# Se escriben las ecuaciones para b y m 
m_mc = (n * Sum_xy - Sum_x * Sum_y) / Delta
b_mc = Sum_y /n - m_mc * Sum_x/n
print('m=',m_mc, ',', 'b=',b_mc)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
m= 5.884415584415585 , b= 0.7361688311688325
}
\end{tcolorbox} 

En este bloque de comandos lo que se quiere es hacer una gráfica que contenga la gráfica de dispersión y la recta que mejor ajusta los datos

\begin{lstlisting}[language=Python] 
# La gráfica con los datos y la recta que mejor se ajusta 
y_hat= m_mc*xn + b_mc
# 
plt.figure()
plt.scatter(xn, yn, color='b',marker='+', label='datos medidos')
plt.plot(xn, y_hat, 'r--',label='recta por mínimos cuadrados')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title(r'Velocidad vs tiempo', fontsize=18)
plt.xlabel(r'$t$ [s]', fontsize=16)
plt.ylabel(r'$v$ [m/s]', fontsize=16)
plt.text(0.6, 1.0, '$y=(5.88) x + 0.736$', fontsize=12)
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig13}  
\label{figdatos2}
\end{center}
\end{figure}

El siguiente paso es calcular $\Delta b$ y $\Delta m$, pero primero $\hat{\sigma}$

\begin{lstlisting}[language=Python] 
SSE= np.sum((yn - y_hat)**2)
s_hat= np.sqrt(SSE/(n-2))
print('n=',n,',','SSE=', SSE ,',', 'sigma=',s_hat)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
n= 10 , SSE= 1.244265584415585 ,  $\sigma$= 0.3943769745458628
}
\end{tcolorbox}

Los respectivos errores se obtienen de las ecuaciones (\ref{Delm}) y (\ref{Delb}) 

\begin{lstlisting}[language=Python] 
# Los promedios de x y y 
xp= np.sum(xn)/n
yp= np.sum(yn)/n
Delta_m = np.sqrt(s_hat**2 / np.sum((xn - xp)**2))
Delta_b= np.sqrt(s_hat**2 * (1/n + xp**2 / np.sum((xn - xp)**2)))
print(f'm = {np.round(m_mc, 1)} \u00B1 {np.round(Delta_m, 1)}')
print(f'b = {np.round(b_mc, 1)} \u00B1 {np.round(Delta_b, 1)}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 \pm 0.4$

$b = 0.7 \pm 0.2$
}
\end{tcolorbox}

Por lo tanto:
$$
v= 5.9 t + 0.7 \,\, \mathrm{m/s} \,.
$$

Numpy tiene una función específica para calcular los parámetros $m$ y $b$, que se llama ``linalg.lstsq''. Para mayor información se puede consultar: 

 \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html}

La librería ``linalg.lstsq'' resuelve para  el vector $x$ la ecuación matricial $a.x = b$  utilizando el método de los mínimos cuadrados. Esta función se utiliza comúnmente en una variedad de aplicaciones, como el análisis de regresión, el ajuste de curvas y otras tareas de aprendizaje automático.

Veamos como funciona:

\begin{lstlisting}[language=Python] 
# Ajustar la recta por mínimos cuadrados usando linalg.lstsq
A = np.vstack([xn, np.ones(len(xn))]).T
m_c, b_c = np.linalg.lstsq(A, yn, rcond=None)[0]
#
print(f'm = {np.round(m_c, 1)}' )
print(f'b = {np.round(b_c, 1)}' )
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.9 $

$b = 0.7 $
}
\end{tcolorbox}


Actualmente, la función ``np.linalg.lstsq'' de Numpy no proporciona directamente los errores estándar de los coeficientes. Sin embargo, hay otras bibliotecas en Python que pueden hacer regresión lineal y proporcionar estos errores de manera más directa. 

Una de las bibliotecas más utilizadas para este propósito es ``statsmodels''. Para más información ver:  \url{https://www.statsmodels.org/stable/index.html}

Para estudiar como funciona en el ejemplo que estamos considerando lo primero que hay que hacer es  cargar la librería 

\begin{lstlisting}[language=Python] 
# Importamos la librería 
import statsmodels.api as sm
\end{lstlisting}

Luego ejecutamos las siguientes lineas de código para los cálculos

\begin{lstlisting}[language=Python] 
# Agregamos una constante (columna de unos) a xn
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
\end{lstlisting}

Con las siguientes lineas mostramos los resultados

\begin{lstlisting}[language=Python] 
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = 5.8844 \pm 0.4103$

$b = 0.7362 \pm 0.2262$
}
\end{tcolorbox}

Y ahora la gráfica
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos originales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.xlabel('$t$ [s]')
plt.ylabel('$v$ [m/s]')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig20}  
\label{figdatos2}
\end{center}
\end{figure}


Se puede pedir que se genere en consola un resumen del modelo 

\begin{lstlisting}[language=Python] 
results = model
print(results.summary)
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=4.6in]{figuras/fig20a}  
\caption{Salida en consola de la función ``.summary'' para el ejemplo \ref{ejemvelocidadtiempo}.}
\label{resuejem1}
\end{center}
\end{figure}

En la figura \ref{resuejem1} se pueden identificar fácilmente los valores de los parámetros y sus errores estándar que relatan la precisión de las estimaciones obtenidas. De ``R-squared: 0.963'' , y otros parámetros hablaremos más adelante, pero este valor indica qué tan bien los datos se ajustan al modelo de regresión (toma valores entre 0 y 1, donde valores más cercanos a 1 indican un mejor ajuste).
\end{example}
\exampleline

\subsection{Propiedades del método de mínimos cuadrados}

Podemos notar de la ecuación (\ref{eme2}) que 
$$
m  = \dfrac{\sum\limits_{i=1}^n y_i \left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} =\sum_{i=1}^n c_i y_i \,,\quad \text{donde} \quad c_i = \frac{\left(x_i-\bar{x}\right)}{\sum\limits_{i=1}^n \left(x_i-\bar{x}\right)^2} \,.
\label{eme3}
$$

Se dice que los estimadores $m$ y $b$ por mínimos cuadrados son estimadores no ambiguos porque
$$
\begin{aligned}
E\left(m\right)  =E\left(\sum_{i=1}^n c_i y_i\right)=\sum_{i=1}^n c_i E\left(y_i\right)  =\sum_{i=1}^n c_i\left(\beta_0+\beta_1 x_i\right)=\beta_0 \sum_{i=1}^n c_i+\beta_1 \sum_{i=1}^n c_i x_i \,,
\end{aligned}
$$
ya que por suposición $E\left(\varepsilon_i\right)=0$. Se puede demostrar que $\sum_{i=1}^n c_i=0$ y $\sum_{i=1}^n c_i x_i=1$, por lo que
$$
E\left(m \right)=\beta_1 \,,
$$
es decir, si suponemos que el modelo es correcto $\left[E\left(y_i\right)=\beta_0+\beta_1 x_i\right]$, entonces $m$ es un estimador no sesgado de $\beta_1$. Del mismo modo podemos demostrar que $b$ es un estimador no sesgado de $\beta_0$, o bien
$$
E\left(b\right)=\beta_0 \,.
$$

%example%%%%%%%
\begin{example}{Ley de enfriamiento de Steinhart-Hart. }
\label{ejemcizalla}


\begin{table}[!t]
  \centering
  \caption{Datos para el ejemplo \ref{ejemcizalla}.}  
  \vspace{0.3cm}
\begin{tabular}{ccc}
\hline Observación, $i$ & Resistencia, $y_i(\Omega)$ & Temperatura, $x_i\left({ }^{\circ} \mathrm{C}\right)$ \\
\hline 1 & 2158.70 & 15.50 \\
2 & 1678.15 & 23.75 \\
3 & 2316.00 & 8.00 \\
4 & 2061.30 & 17.00 \\
5 & 2207.50 & 5.50 \\
6 & 1708.30 & 19.00 \\
7 & 1784.70 & 24.00 \\
8 & 2575.00 & 2.50 \\
9 & 2357.90 & 7.50 \\
10 & 2256.70 & 11.00 \\
11 & 2165.20 & 13.00 \\
12 & 2399.55 & 3.75 \\
13 & 1779.80 & 25.00 \\
14 & 2336.75 & 9.75 \\
15 & 1765.30 & 22.00 \\
16 & 2053.50 & 18.00 \\
17 & 2414.40 & 6.00 \\
18 & 2200.50 & 12.50 \\
19 & 2654.20 & 2.00 \\
20 & 1753.70 & 21.50 \\
21 & 2665.86 & 0.00 \\
\hline
\end{tabular}
  \label{datosciza}
\end{table}


Para describir con precisión la relación entre la resistencia $R$  de un termistor y su temperatura $T$ se utiliza la ecuación de Steinhart-Hart:
$$
\frac{1}{T}=A+B \ln (R)+C[\ln (R)]^3
$$
donde:
\begin{itemize}
\item  $T$ es la temperatura.
\item  $R$ es la resistencia en ohmios.
\item  $A, B, C$ son constantes específicas del termistor.
\end{itemize}

En un rango pequeño de temperaturas, la relación entre $R$ y $T$ puede aproximarse a:
$$
R \approx R_0+\alpha\left(T-T_0\right)
$$        
donde: $R_0$ es la resistencia a una temperatura de referencia $T_0$ y $\alpha$ es el coeficiente de temperatura, que indica la tasa de cambio de la resistencia con la temperatura.

{\bf Experimento:}  Medición de la resistencia de un termistor NTC

El objetivo del experimento es determinar la relación lineal aproximada entre la resistencia y la temperatura de un termistor NTC en un rango limitado de temperaturas, donde las variables son: $y$, resistencia del material y $x$ la temperatura.  La forma de llevar el experimento consiste colocar el termistor en un baño de agua caliente para llevarlo a una temperatura significativamente más alta que la ambiente. Luego se retira el termistor del agua caliente y se miden su resistencia y temperatura a intervalos regulares mientras se enfría. 

Un diagrama de dispersión con los datos recolectados sugiere una fuerte relación estadística entre la resistencia  y la temperatura. La hipótesis inicial de un modelo lineal, $y=\beta_0+\beta_1 x+\varepsilon$, parece razonable. 

A continuación haremos los cálculos directamente con Python y la librería ``statsmodels'' ya usada en el ejemplo \ref{ejemvelocidadtiempo}. 
\begin{lstlisting}[language=Python] 
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
\end{lstlisting}

Ahora es necesario introducir los datos de la tabla \ref{datosciza}
\begin{lstlisting}[language=Python] 
# Datos de resistencias y de temperaturas
yn = np.array([2158.70, 1678.15, 2316.00, 2061.30, 2207.50,
               1708.30, 1784.70, 2575.00, 2357.90, 2256.70,
               2165.20, 2399.55, 1779.80, 2336.75, 1765.30,
               2053.50, 2414.40, 2200.50, 2654.20, 1753.70,2665.86])
xn = np.array([150.50, 230.75, 80.00, 170.00, 50.50, 190.00, 240.00, 20.50,
               70.50, 110.00, 130.00, 30.75, 250.00, 90.75, 220.00, 180.00,
               60.00, 120.50, 20.00, 210.50,0.00])
\end{lstlisting}

Luego hacemos el cálculo de los parámetros $m$ y $b$ y los correspondientes  errores estándar.
\begin{lstlisting}[language=Python] 
X = sm.add_constant(xn)
# Ajustamos el modelo
model = sm.OLS(yn, X).fit()
# Obtenemos los coeficientes y los errores estándar
b, m = model.params
Delta_b, Delta_m = model.bse
# Imprimimos los coeficientes y sus errores estándar
print(f'm = {m:.4f} \u00B1 {Delta_m:.4f}')
print(f'b = {b:.4f} \u00B1 {Delta_b:.4f}')
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
$m = -3.7234 \pm 0.2695$

$b = 2622.2834 \pm 39.7582$
}
\end{tcolorbox}

Ahora la gráfica con los datos y la recta obtenida
\begin{lstlisting}[language=Python] 
# Generamos valores de y usando los coeficientes obtenidos
y_pred = m * xn + b
# Graficamos los datos originales y la línea ajustada
plt.scatter(xn, yn, color='blue', label='Datos experimentales')
plt.plot(xn, y_pred, color='red', label='Línea ajustada')
plt.grid(linestyle='dotted')
plt.legend(loc='best')
plt.title('Linealidad')
plt.xlabel('Temperatura (°C)')
plt.ylabel('Resistencia ($Omega$)')
plt.legend()
plt.show()
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=3.6in]{figuras/fig21}  
\label{figdatos2}
\end{center}
\end{figure}

Por lo tanto, la recta tiene por ecuación
$$
\hat{y}=2622.2834 - 3.7234 x \,.
$$
Recordemos que la pendiente $-3.7234$ se interpreta como la disminución de la resistencia  debida a al cambio de temperatura. Dado que el límite inferior de las $x$ está cerca del origen, el intercepto $2622.2834$ representa la resistencia para la temperatura de cero grados. 

Veamos los valores ajustados $\hat{y}$ obtenidos
\begin{lstlisting}[language=Python] 
y_pred = m * xn + b
y_pred
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([2061.90678577, 1763.10128967, 2324.40881038, 1989.29984279,
       2434.25008309, 1914.83118333, 1728.65953467, 2545.95307228,
       2359.78142362, 2212.70582118, 2138.23716172, 2507.78788431,
       1691.42520494, 2284.38190591, 1803.12819413, 1952.06551306,
       2398.87746984, 2173.60977496, 2547.81478877, 1838.50080738,
       2622.28344823])
}
\end{tcolorbox}
y también los residuos
\begin{lstlisting}[language=Python] 
e_i=yn-y_pred
e_i
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
array([  96.79321423,  -84.95128967,   -8.40881038,   72.00015721,
       -226.75008309, -206.53118333,   56.04046533,   29.04692772,
         -1.88142362,   43.99417882,   26.96283828, -108.23788431,
         88.37479506,   52.36809409,  -37.82819413,  101.43448694,
         15.52253016,   26.89022504,  106.38521123,  -84.80080738,
         43.57655177])
}
\end{tcolorbox}
Hay varias propiedades de los mínimos cuadrados que podemos probar con este ejemplo:

1)  La suma de los valores observados $y_i$ es igual a la suma de los valores ajustados $\hat{y}_i$, o bien
$$
\sum_{i=1}^n y_i=\sum_{i=1}^n \hat{y}_i
$$
El lado izquierdo de la ecuación es:
\begin{lstlisting}[language=Python] 
Sum_y = np.sum(yn)
Sum_y 
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
45293.009999999995
}
\end{tcolorbox}
Y el lado derecho:
\begin{lstlisting}[language=Python] 
Sum_yp = np.sum(y_pred)
Sum_yp
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
45293.00999999999
}
\end{tcolorbox}

2) La suma de los residuos en cualquier modelo de regresión que contiene un intercepto $\beta_0$ es siempre cero, es decir,
$$
\sum_{i=1}^n\left(y_i-\hat{y}_i\right)=\sum_{i=1}^n e_i=0
$$

\begin{lstlisting}[language=Python] 
np.sum(e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
1.1823431123048067e-11
}
\end{tcolorbox}
Es decir, $\sum e_i=0.00$. 

3)  La recta de regresión por mínimos cuadrados siempre pasa por el centroide, el punto $(\bar{x}, \bar{y})$ de los datos.
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n  x_i \,, \quad  \bar{y} = \frac{1}{n}\sum_{i=1}^n  y_i= m\bar{x} +b \,. 
$$
\begin{lstlisting}[language=Python] 
# Los promedios de x y y 
n=len(xn)
xp= np.sum(xn)/n
yp= np.sum(yn)/n
print(xp,',', yp,',', m * xp + b)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
125.01190476190476 , 2156.81 , 2156.8099999999995
}
\end{tcolorbox}

4) La suma de los residuos ponderada por el valor correspondiente de la variable regresora siempre es igual a cero, es decir,
$$
\sum_{i=1}^n x_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(xn*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
1.2842065189033747e-09
}
\end{tcolorbox}

5) La suma de los residuos ponderada por el valor ajustado  siempre es cero
$$
\sum_{i=1}^n \hat{y}_i e_i=0
$$
\begin{lstlisting}[language=Python] 
np.sum(y_pred*e_i)
\end{lstlisting}
\begin{tcolorbox}[width=\textwidth,colback={ghostwhite}]   
{\small 
2.6237103156745434e-08
}
\end{tcolorbox}

Como ya lo señalamos, los programas estadísticos suelen ofrecer un resumen de todos los cálculos que realiza el programa. La salida en pantalla o consola de la figura \ref{figejemciza} presenta los resultados de la librería ``statsmodels''  para el ejemplo que estamos considerando. La parte superior de la tabla contiene el modelo de regresión ajustado. Observe que, antes del redondeo, los coeficientes de regresión coinciden con los que calculamos manualmente. La salida en pantalla también contiene otra información sobre el modelo de regresión que pueden ser consultadas en el manual de la librería. 
\begin{lstlisting}[language=Python] 
results = model
print(results.summary())
\end{lstlisting}
\begin{figure}[!h]
\begin{center}
\includegraphics[height=3.0in,width=4.6in]{figuras/fig22}  
\caption{Resumen de las estimaciones estadísticas por consola para el ejemplo \ref{ejemcizalla}.}
\label{resumejem2}
\end{center}
\end{figure}

\end{example}
\exampleline


\subsection{Estadísticas de diagnóstico del modelo de regresión}


Además de estimar $\beta_0$ y $\beta_1$, como ya vimos, se necesita una estimación de la varianza del error $\sigma^2$, que  proporciona información crucial sobre la variabilidad de los errores (residuos) y tiene varias aplicaciones en el análisis estadístico, como  probar hipótesis y construir estimaciones de intervalo pertinentes para el modelo de regresión.  Lo ideal sería que esta estimación no dependiera de la adecuación del modelo ajustado. Esto sólo es posible cuando hay varias observaciones de $y$ para al menos un valor de $x$  o cuando se dispone de información previa sobre $\sigma^2$. 
Cuando no se puede utilizar este enfoque, la estimación de $\sigma^2$ se obtiene a partir de la ecuación (\ref{sigma2}).


Dado que $\hat{\sigma}^2$ depende de la suma residual de cuadrados, cualquier violación de los supuestos sobre los errores del modelo o cualquier especificación errónea de la forma del modelo puede dañar seriamente la utilidad de $\hat{\sigma}^2$ como estimación de $\sigma^2$. Dado que $\hat{\sigma}^2$ se calcula a partir de los residuos del modelo de regresión, decimos que es una estimación dependiente del modelo de $\sigma^2$.

Como ya vimos, a partir de  $\hat{\sigma}^2$ los errores estándar de $m$ y $b$ se pueden obtener a partir de las siguientes ecuaciones
\begin{equation}
\mathrm{SE}\left(b\right)^2=\sigma^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right], \quad 
\mathrm{SE}\left(m\right)^2=\frac{\sigma^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2},
\label{SEmyb}
\end{equation}

En nuestro ejemplo \ref{ejemcizalla} , los estadísticos descriptivos  para las ($n=21$) variables temperatura y tiempo del enfriamiento son los siguientes:

$$
\begin{aligned}
& \bar{x}=125.0119 , \quad \bar{y}=2156.81\\
& S_x^2=12896.0685, \quad S_y^2=196582.6099 \\
& S_{xy}=-48017.6465 \\
& m= -3.7234, \quad   b=  2622.2834
\end{aligned}
$$

Estos valores de $m$ y $b$ se pueden apreciar en la  la figura \ref{resumejem2} donde aparece la columna {\rm \bf{coef}}.

La recta de regresión ajustada es la siguiente:
$$
\hat{y}=2622.2834 -3.7234 x \,,
$$
donde $\hat{y}$ es la resistencia y $x$ la temperatura. 

\subsubsection{Coeficiente de correlación lineal}

El coeficiente de correlación lineal entre $x$ e $y$ viene dado por:
$$
r=\frac{S_{xy}}{S_x S_y}
$$
y es una medida estadística que cuantifica la intensidad de la relación lineal entre dos variables. Su cuadrado se denomina coeficiente de determinación $r^2$.

Propiedades del coeficiente de correlación:
\begin{enumerate}
\item  No tiene dimensión, y siempre toma valores en [-1,1].
\item Si las variables son independientes, entonces $r=0$, el inverso no tiene por qué ser cierto.
\item Si existe una relación lineal exacta entre $x$ e $y$, entonces $r=1$  (relación directa) ó $r=-1$ (relación inversa).
\item Si $r>0$,  indica una relación directa entre las variables (si aumenta $x$,  aumenta $y$ ).
\item Si $r<0$, indica una relación directa entre las variables (si aumenta $x$,  disminuye $y$).
\end{enumerate}

Para nuestro ejemplo el valor de $r$ es
$$
r=\frac{S_{xy}}{S_x S_y} = \frac{-48017.6465}{ \sqrt{12896.0685} \sqrt{196582.6099}}= -0.9537
$$
El menos indica que existe una relación inversa entre las variables. Además su valor es próximo a $1$ indicando una dependencia lineal muy fuerte.

La relación entre los coeficientes de regresión y de correlación:
$$
m_{y/ x}=r \frac{S_y}{S_x}=-3.7234\,,
$$
$$
m_{x / y}=r \frac{S_x}{S_y}=-0.2443 \,.
$$
Los dos coeficientes de regresión y el coeficiente de correlación tienen  el mismo signo: a medida que $x$ aumenta  $y$ tiende a disminuir.

El coeficiente de determinación $r^2$ es una medida que cuantifica la proporción de la variabilidad total de $y$ que es explicada por el modelo de regresión. Se calcula como:
$$
r^2=\frac{\sum\left(\hat{y}_i-\bar{y}\right)^2}{\sum\left(y_i-\bar{y}\right)^2}=\frac{\mathrm{SSR}}{\mathrm{SST}}
$$

Para el modelo de temperaturas y resistencias tenemos que 
$$
\mathrm{SSR}= 1787904.8827\,, \,\, \mathrm{SSE}= 177921.2163\,,
\mathrm{SST}= 1965826.099 \,,
$$
por lo tanto:
$$
r^2=\frac{\mathrm{SSR}}{\mathrm{SST}} = \frac{1787904.8827}{1965826.099}= 0.9095 \,.
$$
Este valor se puede apreciar en la figura \ref{resumejem2} como {\rm \bf{R-squared}}.


El error estándar residual y  los errores estándar de la pendiente $\Delta m$ y el término independiente $\Delta b$ de la regresión lineal son

$$
\mathrm{SE}\left(m\right)^2=\Delta m=\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \,, \,\, 
\mathrm{SE}\left(b\right)^2=\Delta b=\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right] 
$$
donde 
$$
\hat{\sigma}^2=\frac{{\mathrm{SSE}}}{n-2} = 9364.2745 
$$

Por lo tanto
$$
\Delta m = \sqrt{\frac{\hat{\sigma}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}}= 0.2695 \,, \quad \Delta b = \sqrt{\hat{\sigma}^2\left[\frac{1}{n}+\frac{\bar{x}^2}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}\right]  }   = 39.7582 \,.
$$

Ver la figura \ref{resumejem2} donde aparece la columna {\rm \bf{std err}}.



\subsubsection{Prueba de hipótesis sobe $m$ y $b$}

El test de hipótesis o prueba de significación es una herramienta estadística utilizada para determinar si hay suficiente evidencia en una muestra de datos para inferir que una hipótesis sobre una población es verdadera. En el contexto de la regresión lineal, esta prueba se utiliza para evaluar la significancia de los coeficientes del modelo. Como veremos, los errores estándar también pueden utilizarse para realizar pruebas de hipótesis sobre los coeficientes. 

La prueba de hipótesis más común consiste en probar la hipótesis nula:
$$
H_0 \text {: No hay relación entre } x \text{ y } y
$$
frente a la hipótesis alternativa 
$$
H_a  \text {:  Existe alguna relación entre } x \text{ y }  y
$$

Matemáticamente, esto corresponde a probar
$$
H_0: \beta_1=0 \text { frente a } H_a: \beta_1 \neq 0 \,,
$$
ya que si $\beta_1=0$ entonces el modelo (\ref{regrelineal}) se reduce a $y=\beta_0+\varepsilon$, y por lo tanto, $x$ no está relacionado con $y$. 

Para probar la hipótesis nula, tenemos que determinar si $m$ (o $b$), nuestra estimación de $\beta_1$ (o $\beta_0$), está lo suficientemente lejos de cero para que podemos estar seguros de que $\beta_1$ es distinto de cero. 

Ahora bien ¿Qué distancia es suficiente? Esto, por supuesto, depende de la exactitud de $m$, es decir, depende de $\operatorname{SE}\left(m\right)$ (o de $\operatorname{SE}\left(b\right)$). Si $\operatorname{SE}\left(m\right)$ es pequeño, entonces incluso valores relativamente pequeños de $m$ pueden proporcionar una fuerte evidencia de que $\beta_1 \neq 0$, y por lo tanto que existe una relación entre $x$ y $y$. Por el contrario, si $\mathrm{SE}\left(m\right)$ es grande, entonces $m$ debe ser grande en valor absoluto para que podamos rechazar la hipótesis nula. 

\paragraph{Cálculo del estadístico de prueba $t$-valor:} En la práctica, se suele calcular un ``estadístico'' $t$ o valor $t$, este  estadístico de prueba  es el valor $t$, que se calcula como el cociente entre el coeficiente estimado $(\hat{\beta})$ y su error estándar $\operatorname{SE}(\hat{\beta})$:
\begin{equation}
t_i=\frac{\hat{\beta_i}}{\operatorname{SE}(\hat{\beta_i})} \,.
\label{valort}
\end{equation}

Este valor $t$ sigue una distribución $t$ de Student con $n-k-1$ grados de libertad, donde $n$ es el número de observaciones y $k$ es el número de predictores en el modelo, en nuestro caso $k=2$. Mide el número de desviaciones típicas que $\beta_1$ se aleja de 0. Si realmente no hay relación entre $x$ y $y$, entonces esperamos que (\ref{valort}) tenga una distribución $t$ con $n - 2$ grados de libertad. La distribución $t$ tiene forma de campana y para valores de $n$ superiores a 30 aproximadamente es bastante similar a la distribución normal estándar. 

Para el ejemplo que estamos considerando, ejemplo \ref{ejemcizalla}, los $t$-valores son:
$$
t_m=\frac{m}{\Delta m}= -13.8177\,, \,\, t_b=\frac{b}{\Delta b}= 65.9557 \,.
$$
Compare estos valores con los de la figura \ref{resumejem2} donde aparece la columna {\rm \bf{t}}.

Un $t$-valor grande (en valor absoluto) indica que el coeficiente estimado está lejos de cero, lo que sugiere que es menos probable que el coeficiente estimado  sea igual a cero, mientras que un $t$-valor pequeño (en valor absoluto) sugiere que el coeficiente estimado está cerca de cero, lo que sugiere que es más probable que el coeficiente no sea significativamente diferente de cero.

\paragraph{Determinación del $p$-valor:} El $p$-valor es la probabilidad de observar un valor $t$ tan extremo o más extremo que el valor observado, bajo la suposición de que la hipótesis nula es verdadera.

Un $p$-valor pequeño (típicamente menor que un nivel de significancia $\alpha$ 
 de 0.05 o 0.01) indica que hay suficiente evidencia para rechazar la hipótesis nula. Visto de otra manera, rechazamos la hipótesis nula, es decir, declaramos que existe una relación entre $x$ y $y$, si el valor $p$ es lo suficientemente pequeño. Los límites típicos del valor $p$ para rechazar la hipótesis nula son el 5\% o el 1\%. Cuando $n = 30$, estos valores corresponden a estadísticos $t$ (\ref{valort}) de alrededor de 2 y 2,75, respectivamente.
 
 El p-valor se calcula a partir del $t$-valor en una distribución $t$ de Student:
\begin{equation}
p \text {-valor }=2 \times\left(1-\operatorname{CDF}_t(|t|, \text { dof })\right) \,,
\label{pvalor}
\end{equation}
donde: $\mathrm{CDF}_t$ es la función de distribución acumulativa de la distribución t de Student,  $|t|$ es el valor absoluto del $t$-valor y dof son los grados de libertad del modelo. Cuando $|t|$ es muy grande, $\operatorname{CDF}(|t|$, dof $)$ se acerca mucho a 1 , haciendo que $1-$ $\operatorname{CDF}(|t|$, dof ) sea muy pequeño $\mathrm{y}$, por ende, el $\mathrm{p}$-valor sea extremadamente pequeño.
 
Resumiendo, si el $p$-valor es menor que el nivel de significancia $\alpha$ (generalmente $< 0.05$), se rechaza la hipótesis nula y se concluye que el coeficiente es significativamente diferente de cero. Si el p-valor es mayor que $\alpha$ (generalmente $> 0.05$), no se rechaza la hipótesis nula y se concluye que no hay suficiente evidencia para decir que el coeficiente es diferente de cero. El $t$-valor indica cuántos errores estándar se aleja el coeficiente estimado de cero mientras que el $p$-valor proporciona una medida probabilística de si este $t$-valor es suficientemente extremo como para considerar que el coeficiente es significativamente diferente de cero.
 
Volviendo a nuestro ejemplo  \ref{ejemcizalla} resulta:
\begin{itemize}
\item Coeficiente de la Constante ( $\hat{\beta}_0=b$ )

- t-valor: 65.9557

- p-valor: 0.000

\item Coeficiente del Predictor ( $\hat{\beta}_1=m$ ) 

- t-valor: -13.8177

- p-valor: 0.000
\end{itemize}

Estos valores se pueden ver en la figura \ref{resumejem2}, columna $\mathrm{\mathbf{P>|t|}}$.


\paragraph{El nivel de confianza:}  Es la probabilidad de que un intervalo de confianza contenga el valor verdadero del parámetro que se está estimando,  es una medida de la fiabilidad de una estimación y se expresa como un porcentaje (por ejemplo, 95\%, 99\%). Un nivel de confianza del 95\% significa que si se repitiera el proceso de estimación muchas veces, aproximadamente el 95\% de los intervalos de confianza calculados de esa manera contendrían el verdadero valor del parámetro y no debe pensarse que hay un 95\% de probabilidad de que el parámetro verdadero esté dentro del intervalo calculado a partir de una sola muestra.

En nuestro caso de un modelo de regresión simple $y=\beta_0+\beta_1 x+\epsilon$, podemos calcular los intervalos de confianza para los coeficientes de la regresión ($\beta_0$ y $\beta_1$) ya que conocemos los coeficientes de la regresión  $\hat{\beta}_0$ y $\hat{\beta}_1$  y los errores estándar $\operatorname{SE}(\hat{\beta}_0)$ y $\mathrm{SE} (\hat{\beta}_1)$.

El nivel de confianza se construye a través de un  valor crítico $t_{\alpha / 2 , n-2}$ o $t_{\text {critical}}$ que es un punto específico en la distribución ($t$-Student). Un nivel de confianza del $95 \%$ corresponde a $\alpha=0.05$, porque $1-0.95=0.05$. Dado que el nivel de confianza es bilateral, se divide en dos colas de la distribución $t$, lo que significa que cada cola tiene un área de $\alpha / 2$. 

El intervalo de confianza se puede determinar por
$$
\hat{\beta}_j \pm t_{\alpha / 2, n-2} \cdot \operatorname{SE}(\hat{\beta}_j) \,.
$$
Aquí, $\hat{\beta}_j$ es el estimador del parámetro (intersección o pendiente) y $\operatorname{SE}(\hat{\beta}_j)$ es el error estándar del estimador.

En el contexto del ejemplo que estamos considerando  los grados de libertad son $n-2= 19$, y para tener un nivel de confianza del $95 \%$ entonces $\alpha=0.05$. El valor crítico $t_{\alpha / 2 \text {,dof }}$ con $\alpha/2=0.025$ y 19 grados de libertad puede considerarse  con un valor de 2.093 (prueba t de Student).
   
Los intervalos de confianza (IC)  se calculan como se muestra a continuación:
\begin{itemize}
\item  Para $\hat{\beta}_1=m=-3.7234$ y  $\mathrm{SE}(m)=0.2695$

$$
\begin{aligned}
& \mathrm{IC}= \hat{\beta}_1 \pm t_{\alpha / 2, n-2} \cdot \mathrm{SE}(\hat{\beta}_1) \\
& \mathrm{IC}= -3.7234 \pm 2.093 \cdot 0.2695 \\
& \mathrm{IC}= -3.7234 \pm 0.5640\\
& \mathrm{IC}= [-4.2874 \,\, , \,\, -3.1594]
\end{aligned}
$$


\item Para $\hat{\beta}_0=b=2622.2834$ y  $\mathrm{SE}\left(b\right)=39.7582$

$$
\begin{aligned}
& \mathrm{IC}=\hat{\beta}_0 \pm t_{\alpha / 2, n-2} \cdot \mathrm{SE}(\hat{\beta}_0) \\
& \mathrm{IC}= 2622.2834 \pm 2.093 \cdot 39.7582 \\
& \mathrm{IC}= 2622.2834\pm 83.2140 \\
& \mathrm{IC}= [2539.0694 \,\, , \,\, 2705.4975]
\end{aligned}
$$

\end{itemize}

Estos intervalos son los que aparecen en la figura \ref{resumejem2}, columna $\mathbf{[0.025 \quad  0.975]}$.


\paragraph{La significancia global del modelo de regresión:} 

Se utiliza para determinar si el modelo de regresión en su conjunto explica significativamente la variabilidad de la variable dependiente $y$.  Si la prueba F indica que el modelo es significativo, se puede concluir que hay una relación lineal entre las variables y que el modelo explica una proporción significativa de la variabilidad de $y$

Un valor estadístico de F alto junto con un valor $p$ bajo (menor que el nivel de significancia, usualmente 0.05) indica que el modelo es robusto y que la variable independiente contribuyen significativamente a la predicción de $y$.

Para el cálculo de estadístico F (F-statistic) usamos la siguiente ecuación
$$
\mathrm{F}=\frac{\mathrm{MSR}}{\mathrm{MSE}}= 
\dfrac{\dfrac{\mathrm{SSR}}{ k-1}}{\dfrac{\mathrm{SSE}}{n-k}} =  \dfrac{\dfrac{\mathrm{SSR}}{ k-1}}{\hat{\sigma}}
$$

Para el caso particular del ejemplo que estamos considerando es
$$
\mathrm{F}=\dfrac{\dfrac{\mathrm{SSR}}{ 2-1}}{\dfrac{\mathrm{SSE}}{ 21-2}}=\frac{1787904.8827}{9364.2745}= 190.9283
$$

Este valor indica que el modelo es estadísticamente significativo, lo que significa que la relación entre las variables independientes y la variable dependiente es poco probable que se deba al azar.


